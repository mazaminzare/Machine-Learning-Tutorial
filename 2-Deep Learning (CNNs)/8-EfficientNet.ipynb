{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive EfficientNet Tutorial with Detailed Mathematical Formulations\n",
        "\n",
        "EfficientNet is a convolutional neural network architecture introduced by Tan and Le, which optimizes performance through a compound scaling method. This tutorial provides a detailed mathematical breakdown of EfficientNet operations, including forward and backward passes for each layer.\n",
        "\n",
        "## EfficientNet Architecture Overview\n",
        "\n",
        "EfficientNet uses a systematic scaling method to scale the depth, width, and resolution of the network uniformly. Key components include:\n",
        "\n",
        "1. **Input Layer**: Processes the input image.\n",
        "2. **Stem Layer**: Initial convolution layer.\n",
        "3. **MBConv Blocks**: Mobile Inverted Bottleneck Convolutional Blocks with SE (Squeeze-and-Excitation).\n",
        "4. **Head Layers**: Final layers before classification.\n",
        "5. **Fully Connected Output Layer**: Produces the classification output.\n",
        "6. **Output Layer**: Applies a softmax function for classification.\n",
        "\n",
        "### Stem Layer\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $O = \\sigma(W \\ast X + b)$\n",
        "    - Where $\\ast$ denotes the convolution operation.\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial X} = W^T \\ast \\frac{\\partial L}{\\partial O} \\cdot \\sigma'(W \\ast X + b)$\n",
        "  - **Gradient w.r.t. weights**: $\\frac{\\partial L}{\\partial W} = X \\ast \\frac{\\partial L}{\\partial O} \\cdot \\sigma'(W \\ast X + b)$\n",
        "\n",
        "### MBConv Block\n",
        "An MBConv block consists of depthwise separable convolutions with a squeeze-and-excitation layer.\n",
        "\n",
        "- **Forward Pass**:\n",
        "  - **Expansion Phase**:\n",
        "    - **Formula**: $z = \\sigma(W_e \\ast x + b_e)$\n",
        "  - **Depthwise Convolution**:\n",
        "    - **Formula**: $d = \\sigma(W_d \\ast z + b_d)$\n",
        "  - **Squeeze-and-Excitation**:\n",
        "    - **Squeeze**: $s = \\text{avg\\_pool}(d)$\n",
        "    - **Excite**: $e = \\sigma(W_{se2} \\cdot \\sigma(W_{se1} \\cdot s + b_{se1}) + b_{se2}) \\cdot d$\n",
        "  - **Projection Phase**:\n",
        "    - **Formula**: $O = W_p \\ast e + b_p$\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. Projection Phase**:\n",
        "    - $\\frac{\\partial L}{\\partial e} = W_p^T \\ast \\frac{\\partial L}{\\partial O}$\n",
        "    - $\\frac{\\partial L}{\\partial W_p} = e \\ast \\frac{\\partial L}{\\partial O}$\n",
        "  - **Gradient w.r.t. Squeeze-and-Excitation**:\n",
        "    - **Excitation Phase**:\n",
        "      - $\\frac{\\partial L}{\\partial s} = W_{se1}^T \\cdot \\frac{\\partial L}{\\partial e} \\cdot \\sigma'(W_{se1} \\cdot s + b_{se1})$\n",
        "      - $\\frac{\\partial L}{\\partial W_{se1}} = s \\cdot \\frac{\\partial L}{\\partial e} \\cdot \\sigma'(W_{se1} \\cdot s + b_{se1})$\n",
        "    - **Squeeze Phase**:\n",
        "      - $\\frac{\\partial L}{\\partial d} = \\frac{\\partial L}{\\partial e} \\cdot \\sigma'(W_{se2} \\cdot \\sigma(W_{se1} \\cdot s + b_{se1}) + b_{se2})$\n",
        "  - **Gradient w.r.t. Depthwise Convolution**:\n",
        "    - $\\frac{\\partial L}{\\partial z} = W_d^T \\ast \\frac{\\partial L}{\\partial d}$\n",
        "    - $\\frac{\\partial L}{\\partial W_d} = z \\ast \\frac{\\partial L}{\\partial d}$\n",
        "  - **Gradient w.r.t. Expansion Phase**:\n",
        "    - $\\frac{\\partial L}{\\partial x} = W_e^T \\ast \\frac{\\partial L}{\\partial z}$\n",
        "    - $\\frac{\\partial L}{\\partial W_e} = x \\ast \\frac{\\partial L}{\\partial z}$\n",
        "\n",
        "### Head Layers\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $O = \\sigma(W \\ast x + b)$\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial x} = W^T \\ast \\frac{\\partial L}{\\partial O} \\cdot \\sigma'(W \\ast x + b)$\n",
        "  - **Gradient w.r.t. weights**: $\\frac{\\partial L}{\\partial W} = x \\ast \\frac{\\partial L}{\\partial O} \\cdot \\sigma'(W \\ast x + b)$\n",
        "\n",
        "### Global Average Pooling\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $O_k = \\frac{1}{H \\times W} \\sum_{i=1}^H \\sum_{j=1}^W x_{ijk}$\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial x_{ijk}} = \\frac{1}{H \\times W} \\frac{\\partial L}{\\partial O_k}$\n",
        "\n",
        "### Fully Connected Layers\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $O = W \\cdot x + b$\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial x} = W^T \\cdot \\frac{\\partial L}{\\partial O}$\n",
        "  - **Gradient w.r.t. weights**: $\\frac{\\partial L}{\\partial W} = x \\cdot \\frac{\\partial L}{\\partial O}$\n",
        "\n",
        "### Output Layer - Softmax\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $S_k = \\frac{e^{O_k}}{\\sum_i e^{O_i}}$\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. output of last fully connected layer**: $\\frac{\\partial L}{\\partial O_k} = S_k - y_k$, where $y_k$ is the target class.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9wJtyaTd7S8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EfficientNet Overview\n",
        "\n",
        "EfficientNet, developed by Mingxing Tan and Quoc V. Le, is a state-of-the-art convolutional neural network that sets new benchmarks for efficiency and accuracy. Introduced in 2019, EfficientNet leverages a novel compound scaling method that uniformly scales all dimensions of depth/width/resolution using a set of fixed scaling coefficients.\n",
        "\n",
        "### Innovations of EfficientNet\n",
        "\n",
        "EfficientNet introduced significant advancements in scaling deep learning models:\n",
        "\n",
        "1. **Compound Scaling**: Unlike conventional scaling methods that independently increase depth, width, or resolution, EfficientNet scales these factors in a more balanced manner based on a compound coefficient.\n",
        "2. **Efficient Blocks**: Uses mobile inverted bottleneck convolutions, which include lightweight depthwise separable convolutions, to enhance efficiency.\n",
        "3. **Squeeze-and-Excitation Optimization**: Each block incorporates squeeze-and-excitation layers that recalibrate channel-wise feature responses to further boost performance.\n",
        "4. **Fixed Scaling Coefficients**: Through a systematic study, the creators derived a set of fixed scaling coefficients that determine how network depth, width, and resolution are adjusted as the model scales up, optimizing both accuracy and efficiency.\n",
        "\n",
        "\n",
        "### Variants of EfficientNet\n",
        "\n",
        "EfficientNet comes in several variants, labeled B0 through B7, that offer a spectrum of capabilities catering to different computational and memory requirements:\n",
        "\n",
        "- **EfficientNet-B0**: The baseline model designed to provide a balance between efficiency and accuracy.\n",
        "- **EfficientNet-B1 to B7**: Each subsequent model increases in size and capacity, systematically scaled up using the compound coefficient method to achieve higher accuracy.\n",
        "### Detailed Architecture and Parameter Calculation\n",
        "\n",
        "Below is the general structure of the original EfficientNet-B0, which forms the basis for other scaled versions:\n",
        "\n",
        "| Layer Type                | Input Dimension              | Output Dimension             | Kernel Size/Stride/Pad | Parameters Formula                                            | Number of Parameters |\n",
        "|---------------------------|------------------------------|------------------------------|------------------------|---------------------------------------------------------------|----------------------|\n",
        "| **Input**                 | $224 \\times 224 \\times 3$    | N/A                          | N/A                    | N/A                                                           | 0                    |\n",
        "| **Conv1**                 | $224 \\times 224 \\times 3$    | $112 \\times 112 \\times 32$   | $3 \\times 3$, S=2, P=1 | $(3 \\times 3 \\times 3 + 1) \\times 32$                         | 896                  |\n",
        "| **MBConv1**               | $112 \\times 112 \\times 32$   | $112 \\times 112 \\times 16$   | $3 \\times 3$, S=1, P=1 | Depthwise: $(3 \\times 3 \\times 32) \\times 1$, Pointwise: $(32 + 1) \\times 16$ | 528                |\n",
        "| **MBConv6 x2**            | $112 \\times 112 \\times 16$   | $56 \\times 56 \\times 24$     | $3 \\times 3$, S=2, P=1 | Varies per layer                                              | Varies               |\n",
        "| **Global Avg Pooling**    | $7 \\times 7 \\times 320$      | $1 \\times 1 \\times 320$      | Global                 | 0                                                              | 0                    |\n",
        "| **Fully Connected**       | $320$                        | Number of classes            | N/A                    | $(320 + 1) \\times \\text{Number of classes}$                   | Varies               |\n",
        "\n",
        "### Calculation Formulas\n",
        "\n",
        "- **Parameter Formula for Conv Layers**: $(K \\times K \\times C_{\\text{in}} + 1) \\times C_{\\text{out}}$, where $K$ is the kernel size, $C_{\\text{in}}$ is the number of input channels, and $C_{\\text{out}}$ is the number of output channels for convolutions.\n",
        "- **Output Dimension for Conv Layers**: $\\left\\lfloor\\frac{W-K+2P}{S}+1\\right\\rfloor \\times \\left\\lfloor\\frac{H-K+2P}{S}+1\\right\\rfloor$, where $W$ and $H$ are the width and height of the input, $K$ is the kernel size, $P$ is the padding, and $S$ is the stride.\n",
        "\n",
        "### Advantages of EfficientNet\n",
        "\n",
        "- **Highly Efficient**: Achieves much higher accuracy with significantly fewer parameters and lower computation compared to other CNNs.\n",
        "- **Scalable Architecture**: Can be systematically scaled up to achieve a wider range of performance targets and resource constraints.\n",
        "- **State-of-the-Art Performance**: On benchmarks like ImageNet, EfficientNets often outperform models that are much larger and more computationally expensive.\n",
        "\n",
        "### Disadvantages of EfficientNet\n",
        "\n",
        "- **Complexity in Implementation**: The novel components such as squeeze-and-excitation and MBConv blocks can be complex to implement from scratch.\n",
        "- **Resource Intensity at Higher Scales**: While EfficientNet-B0 is quite efficient, larger versions like B7 require substantial computational resources.\n",
        "\n",
        "### Key Properties of EfficientNet\n",
        "\n",
        "- **Optimized Scaling**: The network efficiently utilizes resources by scaling width, depth, and resolution based on fixed coefficients.\n",
        "- **Robust Performance**: Demonstrates strong robustness and lower susceptibility to overfitting despite its depth and complexity.\n",
        "- **Innovative Design**: The integration of advanced techniques like squeeze-and-exitation and MBConv optimizes both the performance and efficiency of the network.\n",
        "\n",
        "EfficientNet continues to be a highly influential model in deep learning, showcasing the impact of balanced scaling on achieving high performance in convolutional neural networks.\n",
        "\n",
        "EfficientNet represents a significant step forward in the development of CNNs, demonstrating that it is possible to achieve both high efficiency and high accuracy through careful, principled scaling.\n"
      ],
      "metadata": {
        "id": "V0Kuq1KP7XXJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0LKbhCe7Sgb"
      },
      "outputs": [],
      "source": []
    }
  ]
}