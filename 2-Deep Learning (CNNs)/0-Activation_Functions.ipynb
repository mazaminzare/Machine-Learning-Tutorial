{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions\n",
        "\n",
        "Activation functions are crucial components in neural networks. They introduce non-linearity into the model, allowing it to learn and represent complex patterns. Without activation functions, a neural network would simply be a linear regression model, regardless of its depth.\n",
        "\n",
        "## Key Concepts of Activation Functions:\n",
        "\n",
        "1. **Definition**:\n",
        "    - An activation function determines whether a neuron should be activated or not.\n",
        "    - It maps the input signal to an output signal, introducing non-linear properties to the network.\n",
        "\n",
        "2. **Common Types of Activation Functions**:\n",
        "\n",
        "### Sigmoid Function\n",
        "\n",
        "#### Formula:\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "#### Derivative:\n",
        "The derivative of the sigmoid function is:\n",
        "$$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$$\n",
        "\n",
        "#### Key Properties:\n",
        "- **Advantages**:\n",
        "  - Smooth gradient, preventing jumps in output values.\n",
        "  - Output range is (0, 1), suitable for probability estimations.\n",
        "- **Disadvantages**:\n",
        "  - Can cause vanishing gradient problem during backpropagation.\n",
        "  - Outputs are not zero-centered, which can slow down convergence.\n",
        "\n",
        "### Hyperbolic Tangent (Tanh) Function\n",
        "\n",
        "#### Formula:\n",
        "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
        "\n",
        "#### Derivative:\n",
        "The derivative of the tanh function is:\n",
        "$$\\tanh'(x) = 1 - \\tanh^2(x)$$\n",
        "\n",
        "#### Key Properties:\n",
        "- **Advantages**:\n",
        "  - Output range is (-1, 1), making it zero-centered.\n",
        "  - Stronger gradients compared to sigmoid.\n",
        "- **Disadvantages**:\n",
        "  - Can also cause vanishing gradient problem during backpropagation.\n",
        "\n",
        "### Rectified Linear Unit (ReLU)\n",
        "\n",
        "#### Formula:\n",
        "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
        "\n",
        "#### Derivative:\n",
        "The derivative of the ReLU function is:\n",
        "$$\\text{ReLU}'(x) = \\begin{cases}\n",
        "1 & \\text{if } x > 0 \\\\\n",
        "0 & \\text{if } x \\leq 0\n",
        "\\end{cases}$$\n",
        "\n",
        "#### Key Properties:\n",
        "- **Advantages**:\n",
        "  - Computationally efficient, as it involves simple thresholding.\n",
        "  - Reduces the likelihood of vanishing gradient problem.\n",
        "- **Disadvantages**:\n",
        "  - Can cause \"dying ReLU\" problem where neurons can become inactive and only output zero.\n",
        "\n",
        "### Leaky ReLU\n",
        "\n",
        "#### Formula:\n",
        "$$\\text{Leaky ReLU}(x) = \\begin{cases}\n",
        "x & \\text{if } x > 0 \\\\\n",
        "\\alpha x & \\text{if } x \\leq 0\n",
        "\\end{cases}$$\n",
        "where $\\alpha$ is a small constant (e.g., 0.01).\n",
        "\n",
        "#### Derivative:\n",
        "The derivative of the Leaky ReLU function is:\n",
        "$$\\text{Leaky ReLU}'(x) = \\begin{cases}\n",
        "1 & \\text{if } x > 0 \\\\\n",
        "\\alpha & \\text{if } x \\leq 0\n",
        "\\end{cases}$$\n",
        "\n",
        "#### Key Properties:\n",
        "- **Advantages**:\n",
        "  - Mitigates the \"dying ReLU\" problem.\n",
        "  - Allows a small gradient when $x \\leq 0$.\n",
        "- **Disadvantages**:\n",
        "  - Introduces a small slope, which may still lead to slow convergence in some cases.\n",
        "\n",
        "### Softmax Function\n",
        "\n",
        "#### Formula:\n",
        "For a vector $z$ of length $K$:\n",
        "$$\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
        "\n",
        "#### Derivative:\n",
        "The derivative of the Softmax function for a single class $i$ is:\n",
        "$$\\frac{\\partial \\text{Softmax}(z_i)}{\\partial z_j} = \\text{Softmax}(z_i) \\cdot (\\delta_{ij} - \\text{Softmax}(z_j))$$\n",
        "where $\\delta_{ij}$ is the Kronecker delta, which is 1 if $i = j$ and 0 otherwise.\n",
        "\n",
        "#### Key Properties:\n",
        "- **Advantages**:\n",
        "  - Converts logits into probabilities, which are useful for classification tasks.\n",
        "  - Ensures the sum of outputs is 1.\n",
        "- **Disadvantages**:\n",
        "  - Can cause gradient saturation for extreme values, leading to slow learning.\n",
        "\n",
        "## Choosing an Activation Function\n",
        "\n",
        "- The choice of activation function depends on the specific problem and the architecture of the neural network.\n",
        "- For hidden layers, ReLU and its variants are commonly used.\n",
        "- For output layers, sigmoid and softmax are widely used in binary and multiclass classification problems, respectively.\n",
        "\n",
        "## Example: Feedforward Neural Network\n",
        "\n",
        "Consider a simple feedforward neural network with one hidden layer. The forward pass involves:\n",
        "\n",
        "1. Calculating the weighted sum of inputs for the hidden layer neurons.\n",
        "2. Applying an activation function (e.g., ReLU) to the hidden layer outputs.\n",
        "3. Calculating the weighted sum of hidden layer outputs for the output layer neurons.\n",
        "4. Applying an activation function (e.g., softmax) to the output layer to obtain probabilities.\n",
        "\n",
        "Mathematically, for a single hidden layer:\n",
        "\n",
        "$$a^{(1)} = \\text{ReLU}(W^{(1)} x + b^{(1)})$$\n",
        "$$a^{(2)} = \\text{Softmax}(W^{(2)} a^{(1)} + b^{(2)})$$\n",
        "\n",
        "Where:\n",
        "- $x$ is the input vector.\n",
        "- $W^{(1)}, W^{(2)}$ are the weight matrices for the hidden and output layers, respectively.\n",
        "- $b^{(1)}, b^{(2)}$ are the bias vectors for the hidden and output layers, respectively.\n",
        "- $a^{(1)}, a^{(2)}$ are the activations of the hidden and output layers, respectively.\n",
        "\n",
        "Understanding and correctly choosing activation functions is crucial for building effective neural networks. They directly impact how well the network learns from the data and generalizes to unseen data.\n"
      ],
      "metadata": {
        "id": "EUvZin71HIIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfXjd2ZFHHo4"
      },
      "outputs": [],
      "source": []
    }
  ]
}