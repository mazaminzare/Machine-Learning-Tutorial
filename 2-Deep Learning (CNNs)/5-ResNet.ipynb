{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In-depth ResNet Tutorial with Detailed Mathematical Formulations\n",
        "\n",
        "ResNet, short for Residual Network, revolutionized deep learning by enabling the training of much deeper networks through the use of residual blocks. This architecture was introduced by Kaiming He et al. in 2015 and has proven to be highly effective across various tasks. This tutorial provides a detailed mathematical analysis of ResNet's operations, including its characteristic skip connections.\n",
        "\n",
        "## ResNet Architecture Overview\n",
        "\n",
        "ResNet architectures come in various depths, including ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152. Here, we'll focus on the general structure that is common to all variants, which consists of:\n",
        "1. **Input Layer**: This processes the input image.\n",
        "2. **Initial Convolution and Max Pooling Layer**: Prepares the data for the residual blocks.\n",
        "3. **Residual Blocks**: These are the core of ResNet.\n",
        "4. **Global Average Pooling**: Averages feature maps across dimensions to reduce feature dimensionality.\n",
        "5. **Fully Connected Output Layer**: Produces the final classification output.\n",
        "6. **Output Layer**: Softmax for classification.\n",
        "\n",
        "### Residual Blocks\n",
        "\n",
        "A key component of ResNet is the residual block, which includes skip connections that allow activations to be forwarded directly across layers. For simplicity, let's consider a two-layer block used in ResNet-34 as an example.\n",
        "\n",
        "## Detailed Mathematical Operations\n",
        "\n",
        "### Initial Convolution Layer\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $O = \\sigma(W * X + b)$\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial X} = W^T * \\frac{\\partial L}{\\partial O} \\cdot \\sigma'(W * X + b)$\n",
        "  - **Gradient w.r.t. weights**: $\\frac{\\partial L}{\\partial W} = X * \\frac{\\partial L}{\\partial O} \\cdot \\sigma'(W * X + b)$\n",
        "\n",
        "### Residual Block\n",
        "Each residual block has an identity shortcut connection that skips one or more layers:\n",
        "- **Forward Pass**:\n",
        "  - **Input to Block**: $x$\n",
        "  - **First Layer**: $y = \\sigma(W_1 * x + b_1)$\n",
        "  - **Second Layer**: $z = W_2 * y + b_2$\n",
        "  - **Output of Block**: $O = \\sigma(z + x)$\n",
        "- **Backward Pass** (using chain rule and assuming ReLU activations for simplicity):\n",
        "  - **Gradient through Second Layer**:\n",
        "    - $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial O} \\cdot \\sigma'(z + x)$\n",
        "    - $\\frac{\\partial L}{\\partial W_2} = y \\cdot \\frac{\\partial L}{\\partial z}$\n",
        "    - $\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial z}$\n",
        "  - **Gradient through First Layer**:\n",
        "    - $\\frac{\\partial L}{\\partial y} = W_2^T \\cdot \\frac{\\partial L}{\\partial z}$\n",
        "    - $\\frac{\\partial L}{\\partial W_1} = x \\cdot (\\frac{\\partial L}{\\partial y} \\cdot \\sigma'(W_1 * x + b_1))$\n",
        "    - $\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial y} \\cdot \\sigma'(W_1 * x + b_1)$\n",
        "  - **Gradient w.r.t. input of Block**:\n",
        "    - $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial O} \\cdot \\sigma'(z + x) + W_1^T \\cdot (\\frac{\\partial L}{\\partial y} \\cdot \\sigma'(W_1 * x + b_1))$\n",
        "\n",
        "### Global Average Pooling\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $O_k = \\frac{1}{N} \\sum_{i=1}^N x_{ik}$\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial x_{ik}} = \\frac{1}{N} \\frac{\\partial L}{\\partial O_k}$\n",
        "\n",
        "### Fully Connected and Output Layers\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $O = Wx + b$\n",
        "  - **Softmax**: $S_k = \\frac{e^{O_k}}{\\sum_{i} e^{O_i}}$\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. output**: $\\frac{\\partial L}{\\partial O_k} = S_k - y_k$ (where $y_k$ is the target probability for class $k$).\n",
        "  - **Gradient w.r.t. weights and input**: Similar to those in the fully connected layer, applying the chain rule.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This comprehensive mathematical breakdown of ResNet illustrates the power of residual learning and its implications for training deep neural networks. By adding shortcut connections, ResNet allows gradients to flow directly through the network, making it possible to train much deeper networks effectively.\n"
      ],
      "metadata": {
        "id": "XoVvLRSOSanT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet-50 Overview\n",
        "\n",
        "ResNet-50, developed by Kaiming He et al., represents a significant advancement in deep learning architectures through its innovative use of residual learning. It is widely recognized for its ability to train very deep networks effectively, vastly improving performance on a range of computer vision tasks.\n",
        "\n",
        "### Innovations of ResNet-50\n",
        "\n",
        "ResNet-50 introduced several groundbreaking architectural innovations:\n",
        "\n",
        "1. **Residual Blocks**: These blocks help address the vanishing gradient problem by introducing shortcut connections that skip one or more layers.\n",
        "2. **Bottleneck Layers**: These layers reduce the number of parameters and computational complexity by using a 1x1 convolution to reduce and then increase dimensions, sandwiching a 3x3 convolution.\n",
        "3. **Identity Mappings**: These shortcuts are used directly when the input and output dimensions are the same, aiding in the uninterrupted flow of gradients.\n",
        "4. **Global Average Pooling**: This replaces the traditional fully connected layers at the top of the network, significantly reducing the number of parameters.\n",
        "\n",
        "### Detailed Architecture and Parameter Calculation\n",
        "\n",
        "Below is a breakdown of each significant layer of ResNet-50, detailing dimensions, configurations, and parameter calculations:\n",
        "\n",
        "| Layer                 | Input Dimension              | Output Dimension             | Kernel Size/Stride/Pad | Parameters Formula                                                  | Number of Parameters |\n",
        "|-----------------------|------------------------------|------------------------------|------------------------|---------------------------------------------------------------------|----------------------|\n",
        "| **Input**             | $224 \\times 224 \\times 3$    | N/A                          | N/A                    | N/A                                                                 | 0                    |\n",
        "| **Initial Conv**      | $224 \\times 224 \\times 3$    | $112 \\times 112 \\times 64$   | $7 \\times 7$, S=2, P=3 | $(7 \\times 7 \\times 3) \\times 64 + 64$                              | 9,472                |\n",
        "| **Max Pooling**       | $112 \\times 112 \\times 64$   | $56 \\times 56 \\times 64$     | $3 \\times 3$, S=2, P=1 | 0                                                                   | 0                    |\n",
        "| **Bottleneck Block**  | $56 \\times 56 \\times 256$    | $56 \\times 56 \\times 256$    | Mixed                  | Varies by sub-layer                                                 | Varies               |\n",
        "| **Global Avg Pooling**| $7 \\times 7 \\times 2048$     | $1 \\times 1 \\times 2048$     | Global                | 0                                                                   | 0                    |\n",
        "| **Fully Connected**   | $2048$                       | $1000$                       | N/A                    | $2048 \\times 1000 + 1000$                                           | 2,049,000            |\n",
        "\n",
        "### Calculation Formulas\n",
        "\n",
        "- **Parameter Formula for Conv Layers**: For convolution layers, the formula is $(K \\times K \\times C_{\\text{in}}) \\times C_{\\text{out}} + C_{\\text{out}}$, where $K$ is the kernel size, $C_{\\text{in}}$ is the number of input channels, and $C_{\\text{out}}$ is the number of output channels.\n",
        "- **Output Dimension for Conv Layers**: Given by $\\left\\lfloor \\frac{W-K+2P}{S} + 1 \\right\\rfloor \\times \\left\\lfloor \\frac{H-K+2P}{S} + 1 \\right\\rfloor$, where $W$ and $H$ are the width and height of the input, $K$ is the kernel size, $P$ is the padding, and $S$ is the stride.\n",
        "\n",
        "### Advantages of ResNet-50\n",
        "\n",
        "- **Allows Deeper Networks**: Overcomes the vanishing gradient problem, enabling the training of networks that are much deeper than was previously possible.\n",
        "- **Improved Accuracy**: Demonstrates significantly better accuracy on complex tasks due to its deeper and more sophisticated architecture.\n",
        "- **Efficient Use of Parameters**: Uses bottleneck layers and global average pooling to reduce the total number of parameters, making it computationally more efficient.\n",
        "\n",
        "### Disadvantages of ResNet-50\n",
        "\n",
        "- **High Computational Requirement**: Despite its efficient use of parameters, the training and inference are still computationally intensive.\n",
        "- **Complexity**: The architecture's complexity can make it challenging to implement and optimize for specific tasks or hardware.\n",
        "\n",
        "### Key Properties of ResNet\n",
        "\n",
        "- **Skip Connections**: Facilitate training by allowing gradients to flow through the network without hindrance.\n",
        "- **Feature Reusability**: Enhances the network's ability to reuse features, which improves learning efficiency and effectiveness.\n",
        "- **Scalability**: The architecture can be scaled up (e.g., ResNet-101, ResNet-152) effectively to improve performance without substantial modifications.\n",
        "\n",
        "ResNet-50 continues to be one of the most influential models in the field of deep learning, illustrating a significant shift in how deep learning architectures are constructed and understood.\n"
      ],
      "metadata": {
        "id": "KI3pXv7xaj80"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XGzXPU_SBVV"
      },
      "outputs": [],
      "source": []
    }
  ]
}