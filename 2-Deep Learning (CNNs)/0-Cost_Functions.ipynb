{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cost Functions\n",
        "\n",
        "Cost functions are a fundamental concept in machine learning, statistics, and optimization. They measure the difference between the predicted values by a model and the actual values from the data. The goal of many learning algorithms is to minimize the cost function, thus improving the accuracy of the model.\n",
        "\n",
        "## Key Concepts of Cost Functions:\n",
        "\n",
        "1. **Definition**:\n",
        "    - A cost function, also known as a loss function or error function, quantifies the error between predicted outputs and the actual outputs.\n",
        "    - It provides a measure of how well the model is performing.\n",
        "\n",
        "2. **Common Types of Cost Functions**:\n",
        "\n",
        "### Mean Squared Error (MSE)\n",
        "\n",
        "#### Formula:\n",
        "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "#### Derivative:\n",
        "To find the derivative of MSE with respect to the model parameters $\\theta$, we use:\n",
        "\n",
        "$$\\hat{y}_i = \\theta_0 + \\theta_1 x_i$$\n",
        "\n",
        "The MSE cost function becomes:\n",
        "$$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\theta_0 + \\theta_1 x_i))^2$$\n",
        "\n",
        "1. Take the partial derivative with respect to $\\theta_0$:\n",
        "$$\\frac{\\partial J}{\\partial \\theta_0} = \\frac{\\partial}{\\partial \\theta_0} \\left( \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\theta_0 + \\theta_1 x_i))^2 \\right)$$\n",
        "\n",
        "Using the chain rule:\n",
        "$$\\frac{\\partial J}{\\partial \\theta_0} = \\frac{2}{n} \\sum_{i=1}^{n} (y_i - (\\theta_0 + \\theta_1 x_i)) (-1)$$\n",
        "$$\\frac{\\partial J}{\\partial \\theta_0} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)$$\n",
        "\n",
        "2. Take the partial derivative with respect to $\\theta_1$:\n",
        "$$\\frac{\\partial J}{\\partial \\theta_1} = \\frac{\\partial}{\\partial \\theta_1} \\left( \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\theta_0 + \\theta_1 x_i))^2 \\right)$$\n",
        "\n",
        "Using the chain rule:\n",
        "$$\\frac{\\partial J}{\\partial \\theta_1} = \\frac{2}{n} \\sum_{i=1}^{n} (y_i - (\\theta_0 + \\theta_1 x_i)) (-x_i)$$\n",
        "$$\\frac{\\partial J}{\\partial \\theta_1} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i) x_i$$\n",
        "\n",
        "#### Key Properties:\n",
        "- **Advantages**:\n",
        "  - Differentiable and convex, leading to a single global minimum.\n",
        "  - Penalizes larger errors more significantly, which can be beneficial in some contexts.\n",
        "- **Disadvantages**:\n",
        "  - Sensitive to outliers, which can disproportionately influence the model.\n",
        "\n",
        "### Mean Absolute Error (MAE)\n",
        "\n",
        "#### Formula:\n",
        "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
        "\n",
        "#### Derivative:\n",
        "For the MAE cost function, the derivative is not straightforward due to the absolute value function. The gradient depends on the sign of the residual:\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{sign}(y_i - \\hat{y}_i) \\cdot \\frac{\\partial (y_i - \\hat{y}_i)}{\\partial \\theta_j}$$\n",
        "\n",
        "For linear regression:\n",
        "$$\\frac{\\partial (y_i - \\hat{y}_i)}{\\partial \\theta_0} = -1$$\n",
        "$$\\frac{\\partial (y_i - \\hat{y}_i)}{\\partial \\theta_1} = -x_i$$\n",
        "\n",
        "So:\n",
        "$$\\frac{\\partial J}{\\partial \\theta_0} = -\\frac{1}{n} \\sum_{i=1}^{n} \\text{sign}(y_i - \\hat{y}_i)$$\n",
        "$$\\frac{\\partial J}{\\partial \\theta_1} = -\\frac{1}{n} \\sum_{i=1}^{n} \\text{sign}(y_i - \\hat{y}_i) x_i$$\n",
        "\n",
        "#### Key Properties:\n",
        "- **Advantages**:\n",
        "  - Less sensitive to outliers compared to MSE.\n",
        "  - Provides a more robust measure of central tendency.\n",
        "- **Disadvantages**:\n",
        "  - Not differentiable at zero, complicating gradient-based optimization methods.\n",
        "\n",
        "### Cross-Entropy Loss\n",
        "\n",
        "#### Formula:\n",
        "For binary classification:\n",
        "$$\\text{Cross-Entropy Loss} = - \\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$$\n",
        "\n",
        "#### Derivative:\n",
        "The derivative of the Cross-Entropy Loss with respect to the prediction $\\hat{y}_i$:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\hat{y}_i} = -\\left(\\frac{y_i}{\\hat{y}_i} - \\frac{1 - y_i}{1 - \\hat{y}_i}\\right)$$\n",
        "\n",
        "For logistic regression, where $\\hat{y}_i = \\sigma(z_i)$ and $z_i = \\theta^T x_i$:\n",
        "$$\\frac{\\partial \\sigma(z_i)}{\\partial z_i} = \\sigma(z_i)(1 - \\sigma(z_i))$$\n",
        "\n",
        "Combining these:\n",
        "$$\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) x_i$$\n",
        "\n",
        "#### Key Properties:\n",
        "- **Advantages**:\n",
        "  - Highly suitable for binary and multiclass classification.\n",
        "  - Provides probabilistic interpretations.\n",
        "- **Disadvantages**:\n",
        "  - Can suffer from saturation, making it difficult to train models when predictions are very confident but incorrect.\n",
        "\n",
        "### Hinge Loss\n",
        "\n",
        "#### Formula:\n",
        "$$\\text{Hinge Loss} = \\frac{1}{n} \\sum_{i=1}^{n} \\max(0, 1 - y_i \\hat{y}_i)$$\n",
        "\n",
        "#### Derivative:\n",
        "For the hinge loss, the derivative depends on whether $1 - y_i \\hat{y}_i$ is greater than zero:\n",
        "\n",
        "1. If $1 - y_i \\hat{y}_i > 0$:\n",
        "   $$\\frac{\\partial L}{\\partial \\hat{y}_i} = -y_i$$\n",
        "2. Otherwise:\n",
        "   $$\\frac{\\partial L}{\\partial \\hat{y}_i} = 0$$\n",
        "\n",
        "For a linear classifier $\\hat{y}_i = \\theta^T x_i$:\n",
        "$$\\frac{\\partial L}{\\partial \\theta} = \\begin{cases}\n",
        "-y_i x_i & \\text{if } 1 - y_i \\theta^T x_i > 0 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "#### Key Properties:\n",
        "- **Advantages**:\n",
        "  - Particularly effective for training support vector machines.\n",
        "  - Emphasizes correct classification margins.\n",
        "- **Disadvantages**:\n",
        "  - Not suitable for probabilistic interpretations.\n",
        "  - Only useful for classification problems.\n",
        "\n",
        "## Regularization\n",
        "\n",
        "To prevent overfitting, regularization terms are often added to the cost function. Examples include L1 (Lasso) and L2 (Ridge) regularization:\n",
        "\n",
        "$$\\text{Regularized Cost} = \\text{Cost Function} + \\lambda \\left\\| \\theta \\right\\|_p$$\n",
        "\n",
        "where $\\lambda$ is the regularization parameter and $p$ is 1 for L1 and 2 for L2 regularization.\n",
        "\n",
        "## Choosing a Cost Function\n",
        "\n",
        "- Depends on the specific problem and the type of data.\n",
        "- For regression tasks, MSE or MAE are commonly used.\n",
        "- For classification tasks, Cross-Entropy Loss is widely used.\n",
        "\n",
        "## Example: Linear Regression\n",
        "\n",
        "For linear regression, the model predicts $ \\hat{y} = \\theta_0 + \\theta_1 x $. The cost function typically used is the Mean Squared Error (MSE):\n",
        "\n",
        "$$J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2$$\n",
        "\n",
        "where $m$ is the number of training examples, $y_i$ is the actual value, and $\\hat{y}_i$ is the predicted value. The factor $\\frac{1}{2}$ is used for convenience in the differentiation process.\n",
        "\n",
        "## Optimization Process:\n",
        "\n",
        "1. **Initialize** parameters $\\theta_0, \\theta_1$.\n",
        "2. **Compute** the cost function $J(\\theta_0, \\theta_1)$.\n",
        "3. **Update** the parameters to minimize the cost function, typically using Gradient Descent:\n",
        "    $$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_j}$$\n",
        "    where $\\alpha$ is the learning rate.\n",
        "\n",
        "By iteratively updating the parameters, the algorithm converges to the values that minimize the cost function, leading to an optimal model.\n",
        "\n",
        "Understanding and correctly choosing cost functions is crucial for building effective machine learning models. They directly impact how well the model learns from the data and generalizes to unseen data.\n"
      ],
      "metadata": {
        "id": "MlIfbbhnEQko"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO1hL4TaEQDl"
      },
      "outputs": [],
      "source": []
    }
  ]
}