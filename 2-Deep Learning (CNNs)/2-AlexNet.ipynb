{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## AlexNet Overview\n",
        "\n",
        "AlexNet, developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, marked a monumental advancement in the field of deep learning, particularly in computer vision. This convolutional neural network (CNN) won the 2012 ImageNet Large Scale Visual Recognition Challenge by reducing the top-5 error rate from 25.8% to 16.4%, a staggering improvement over traditional image classification models.\n",
        "\n",
        "### Innovations of AlexNet\n",
        "\n",
        "AlexNet introduced several groundbreaking innovations that set new standards for CNN architectures:\n",
        "\n",
        "1. **Deeper Architecture with Convolutional Layers**: AlexNet has five convolutional layers; some are followed by max-pooling layers. This deeper and more complex architecture allows it to capture a wide hierarchy of visual features.\n",
        "2. **ReLU Activation Function**: It was one of the first to utilize the ReLU (Rectified Linear Unit) activation function, which helps to alleviate problems associated with training deep networks such as the vanishing gradient problem.\n",
        "3. **Use of Dropout and Data Augmentation**: To prevent overfitting, AlexNet incorporated dropout layers that randomly ignore some neurons during training, forcing the network to develop more robust features. Additionally, data augmentation techniques such as image translations, horizontal reflections, scaling, and alterations in intensity were used during training to expand the effective size of the training set and introduce more variability, enhancing the model's generalization capabilities.\n",
        "4. **Overlapping Pooling**: Instead of the traditional pooling, AlexNet used overlapping max-pooling, which can help to avoid overfitting by providing an abstracted form of the representation.\n",
        "5. **Normalization Layers**: Local Response Normalization (LRN) layers were used, which apply a form of lateral inhibition inspired by the activity in biological neurons, promoting competition for large activations among neighboring neurons in a kernel map.\n",
        "\n",
        "### Key Properties of AlexNet\n",
        "\n",
        "- **Dual GPU Utilization**: The original architecture was split across two GPUs, which not only provided the necessary computational power but also facilitated model parallelism, significantly speeding up the training process.\n",
        "- **Large Model Size**: With 60 million parameters and 650,000 neurons, AlexNet was an exceptionally large model at the time, necessitating substantial computational resources.\n",
        "- **Input Requirements**: The network accepts an input image size of 227x227 pixels, slightly smaller than the originally reported 224x224 due to the size requirements of certain convolutional and pooling layers.\n",
        "\n",
        "### Advantages of AlexNet\n",
        "\n",
        "- **Historical Impact**: AlexNet is credited with reigniting interest in neural networks, especially in the field of computer vision, due to its performance in the ImageNet competition.\n",
        "- **Technological Advancements**: Techniques introduced by AlexNet such as ReLU, dropout, and data augmentation have become standard in training deep neural networks.\n",
        "- **High Accuracy**: It achieved a substantial reduction in error rates on complex image recognition tasks, setting new benchmarks for accuracy in large-scale visual recognition challenges.\n",
        "\n",
        "### Disadvantages of AlexNet\n",
        "\n",
        "- **High Computational Cost**: The large model size and deep architecture require significant GPU resources, which can be a barrier for those without access to such hardware.\n",
        "- **Complexity in Training and Tuning**: Due to its depth and size, AlexNet requires careful configuration and extensive training time to achieve optimal performance.\n",
        "- **Outdated by Newer Architectures**: Subsequent models, such as VGG, GoogLeNet, and ResNet, have provided improvements over AlexNet in terms of efficiency and accuracy, incorporating more layers and refined training techniques.\n",
        "\n",
        "AlexNet not only transformed the landscape of computer vision but also demonstrated the vast capabilities of deep networks, influencing a myriad of applications in various fields beyond image classification.\n"
      ],
      "metadata": {
        "id": "dG8mEnL3wB_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive AlexNet Tutorial with Detailed Mathematical Computations and Derivatives\n",
        "\n",
        "This tutorial provides a thorough examination of AlexNet, a pioneering convolutional neural network that significantly advanced image recognition technology. It includes detailed formulas for each layer's operations, especially focusing on the normalization and dropout layers, and explicates the derivatives necessary for understanding the backpropagation process.\n",
        "\n",
        "## AlexNet Architecture Overview\n",
        "\n",
        "AlexNet consists of five convolutional layers, some followed by max-pooling layers, and three fully connected layers, interspersed with normalization and dropout layers:\n",
        "1. **Input Layer**: 227x227 RGB image.\n",
        "2. **Conv1**: 96 kernels of size 11x11, stride 4.\n",
        "3. **Normalization 1 (LRN)**: Local Response Normalization.\n",
        "4. **Max Pooling 1**: 3x3, stride 2.\n",
        "5. **Conv2**: 256 kernels of size 5x5, grouped convolution.\n",
        "6. **Normalization 2 (LRN)**: Local Response Normalization.\n",
        "7. **Max Pooling 2**: 3x3, stride 2.\n",
        "8. **Conv3**: 384 kernels of size 3x3.\n",
        "9. **Conv4**: 384 kernels of size 3x3, grouped convolution.\n",
        "10. **Conv5**: 256 kernels of size 3x3, grouped convolution.\n",
        "11. **Max Pooling 3**: 3x3, stride 2.\n",
        "12. **Fully Connected 1 (FC6)**: 4096 neurons, includes dropout.\n",
        "13. **Fully Connected 2 (FC7)**: 4096 neurons, includes dropout.\n",
        "14. **Fully Connected 3 (FC8)**: 1000 neurons (outputs, corresponding to 1000 classes).\n",
        "15. **Output Layer**: Softmax for classification.\n",
        "\n",
        "## Detailed Layer-by-Layer Mathematical Operations\n",
        "\n",
        "### Convolutional Layers (Conv1, Conv2, Conv3, Conv4, Conv5)\n",
        "- **Forward Pass**:\n",
        "  - **Formula for Conv1**: $O_{ij}^l = \\sigma(b^l + \\sum_{p=0}^{10} \\sum_{q=0}^{10} W_{pq}^{l} \\cdot I_{4i+p,4j+q})$\n",
        "  - **Adjust for Conv2 to Conv5**: Adapt convolution size and feature map grouping.\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial I_{i+p,j+q}} = \\sum_{l} \\frac{\\partial L}{\\partial O_{ij}^l} \\cdot W_{pq}^l \\cdot \\sigma'(net_{ij}^l)$\n",
        "  - **Gradient w.r.t. weights**: $\\frac{\\partial L}{\\partial W_{pq}^l} = \\sum_{ij} \\frac{\\partial L}{\\partial O_{ij}^l} \\cdot I_{4i+p,4j+q} \\cdot \\sigma'(net_{ij}^l)$\n",
        "\n",
        "### Local Response Normalization (LRN) Layers\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $O_{ij}^k = I_{ij}^k \\left( k + \\alpha \\sum_{l=\\max(0, k-n/2)}^{\\min(N-1, k+n/2)} (I_{ij}^l)^2 \\right)^{-\\beta}$\n",
        "- **Backward Pass**:\n",
        "  - **Detailed derivative**:\n",
        "    - $\\frac{\\partial O_{ij}^k}{\\partial I_{ij}^k} = \\left( k + \\alpha \\sum_{l=\\max(0, k-n/2)}^{\\min(N-1, k+n/2)} (I_{ij}^l)^2 \\right)^{-\\beta} - \\alpha \\beta I_{ij}^k \\left( k + \\alpha \\sum_{l=\\max(0, k-n/2)}^{\\min(N-1, k+n/2)} (I_{ij}^l)^2 \\right)^{-\\beta - 1} \\cdot 2 I_{ij}^k$\n",
        "\n",
        "### Max Pooling Layers (1, 2, 3)\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $O_{ij} = \\max_{p,q \\in [0,2]} I_{2i+p, 2j+q}$\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial I_{2i+p, 2j+q}} = \\begin{cases} \\frac{\\partial L}{\\partial O_{ij}}, & \\text{if } I_{2i+p, 2j+q} = O_{ij} \\\\ 0, & \\text{otherwise} \\end{cases}$\n",
        "\n",
        "### Fully Connected Layers (FC6, FC7, FC8) with Dropout\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $O_j = \\sigma(b_j + \\sum_i W_{ij} \\cdot I_i)$\n",
        "  - **Dropout**: $O_j = D_j \\cdot O_j$ where $D_j \\sim Bernoulli(p)$\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial I_i} = \\sum_j \\frac{\\partial L}{\\partial O_j} \\cdot W_{ij} \\cdot \\sigma'(net_j)$\n",
        "  - **Gradient w.r.t. weights**: $\\frac{\\partial L}{\\partial W_{ij}} = \\sum_j \\frac{\\partial L}{\\partial O_j} \\cdot I_i \\cdot \\sigma'(net_j)$\n",
        "  - **Adjustments for dropout**: Gradients are only propagated through neurons that were not dropped (i.e., $D_j = 1$).\n",
        "\n",
        "### Output Layer - Softmax\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $O_k = \\frac{e^{z_k}}{\\sum_{k'} e^{z_{k'}}}$\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. output of FC8**: $\\frac{\\partial L}{\\partial z_k} = O_k - y_k$ (where $y_k$ is the target probability for class k).\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This comprehensive analysis includes detailed mathematical operations and derivative calculations for all layers within AlexNet, highlighting the complexity of computations that enable learning in deep convolutional neural networks. Understanding these operations enhances our insight into the complex dynamics of these powerful machine learning models.\n"
      ],
      "metadata": {
        "id": "z8MwlQU71Fhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Properties and Layer Configuration in AlexNet\n",
        "\n",
        "The table below provides detailed calculations and configurations for each layer in AlexNet:\n",
        "\n",
        "| Layer                | Input Dimension           | Output Dimension          | Kernel Size/Stride/Pad | Parameters Formula                                                            | Number of Parameters |\n",
        "|----------------------|---------------------------|---------------------------|------------------------|-------------------------------------------------------------------------------|----------------------|\n",
        "| **Conv1**            | $227 \\times 227 \\times 3$ | $55 \\times 55 \\times 96$  | $11 \\times 11$, S=4, P=0 | $(11 \\times 11 \\times 3) \\times 96 + 96$                                       | 34,944               |\n",
        "| **LRN1**             | $55 \\times 55 \\times 96$  | $55 \\times 55 \\times 96$  | N/A                    | $0$ (non-learnable)                                                           | 0                    |\n",
        "| **Pooling1**         | $55 \\times 55 \\times 96$  | $27 \\times 27 \\times 96$  | $3 \\times 3$, S=2       | $0$                                                                           | 0                    |\n",
        "| **Conv2**            | $27 \\times 27 \\times 96$  | $27 \\times 27 \\times 256$ | $5 \\times 5$, S=1, P=2  | $(5 \\times 5 \\times 48) \\times 256 + 256$ (Note: Split across 2 GPUs)          | 307,456              |\n",
        "| **LRN2**             | $27 \\times 27 \\times 256$ | $27 \\times 27 \\times 256$ | N/A                    | $0$ (non-learnable)                                                           | 0                    |\n",
        "| **Pooling2**         | $27 \\times 27 \\times 256$ | $13 \\times 13 \\times 256$ | $3 \\times 3$, S=2       | $0$                                                                           | 0                    |\n",
        "| **Conv3**            | $13 \\times 13 \\times 256$ | $13 \\times 13 \\times 384$ | $3 \\times 3$, S=1, P=1  | $(3 \\times 3 \\times 256) \\times 384 + 384$                                     | 885,120              |\n",
        "| **Conv4**            | $13 \\times 13 \\times 384$ | $13 \\times 13 \\times 384$ | $3 \\times 3$, S=1, P=1  | $(3 \\times 3 \\times 192) \\times 384 + 384$ (Note: Split across 2 GPUs)         | 664,320              |\n",
        "| **Conv5**            | $13 \\times 13 \\times 384$ | $13 \\times 13 \\times 256$ | $3 \\times 3$, S=1, P=1  | $(3 \\times 3 \\times 192) \\times 256 + 256$ (Note: Split across 2 GPUs)         | 442,624              |\n",
        "| **Pooling3**         | $13 \\times 13 \\times 256$ | $6 \\times 6 \\times 256$   | $3 \\times 3$, S=2       | $0$                                                                           | 0                    |\n",
        "| **Fully Connected 1**| $9216$                     | $4096$                    | N/A                    | $(9216 + 1) \\times 4096$                                                       | 37,752,832           |\n",
        "| **Dropout1**         | $4096$                     | $4096$                    | N/A                    | $0$ (non-learnable)                                                           | 0                    |\n",
        "| **Fully Connected 2**| $4096$                     | $4096$                    | N/A                    | $(4096 + 1) \\times 4096$                                                       | 16,781,312           |\n",
        "| **Dropout2**         | $4096$                     | $4096$                    | N/A                    | $0$ (non-learnable)                                                           | 0                    |\n",
        "| **Fully Connected 3**| $4096$                     | $1000$                    | N/A                    | $(4096 + 1) \\times 1000$                                                       | 4,097,000            |\n",
        "\n",
        "### Calculation Formulas\n",
        "\n",
        "- **Parameter Formula for Conv Layers**: $(K \\times K \\times C_{\\text{in}} + 1) \\times C_{\\text{out}}$, where $K$ is the kernel size, $C_{\\text{in}}$ is the number of input channels, and $C_{\\text{out}}$ is the number of output channels.\n",
        "- **Output Dimension for Conv Layers**: $\\left\\lfloor\\frac{W-K+2P}{S}+1\\right\\rfloor \\times \\left\\lfloor\\frac{H-K+2P}{S}+1\\right\\rfloor$, where $W$ and $H$ are the width and height of the input, $K$ is the kernel size, $P$ is the padding, and $S$ is the stride.\n",
        "- **Output Dimension for Pooling Layers**: Same as above but typically with $P=0$.\n",
        "\n",
        "### Advantages of AlexNet\n",
        "\n",
        "- **Historical Impact**: AlexNet is credited with reigniting interest in neural networks, especially in the field of computer vision, due to its performance in the ImageNet competition.\n",
        "- **Technological Advancements**: Techniques introduced by AlexNet such as ReLU, dropout, and data augmentation have become standard in training deep neural networks.\n",
        "- **High Accuracy**: It achieved a substantial reduction in error rates on complex image recognition tasks, setting new benchmarks for accuracy in large-scale visual recognition challenges.\n",
        "\n",
        "### Disadvantages of AlexNet\n",
        "\n",
        "- **High Computational Cost**: The large model size and deep architecture require significant GPU resources, which can be a barrier for those without access to such hardware.\n",
        "- **Complexity in Training and Tuning**: Due to its depth and size, AlexNet requires careful configuration and extensive training time to achieve optimal performance.\n",
        "- **Outdated by Newer Architectures**: Subsequent models, such as VGG, GoogLeNet, and ResNet, have provided improvements over AlexNet in terms of efficiency and accuracy, incorporating more layers and refined training techniques.\n",
        "\n",
        "AlexNet not only transformed the landscape of computer vision but also demonstrated the vast capabilities of deep networks, influencing a myriad of applications in various fields beyond image classification.\n"
      ],
      "metadata": {
        "id": "jJJHXmc99TH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Local Response Normalization (LRN) Layer\n",
        "\n",
        "Local Response Normalization (LRN) is a type of normalization used in deep learning, specifically in convolutional neural networks (CNNs). It was introduced in the paper \"ImageNet Classification with Deep Convolutional Neural Networks\" by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton (2012).\n",
        "\n",
        "The LRN layer performs \"competition\" normalization over local input regions, which encourages neurons that are strongly activated to suppress neurons at the same spatial location but in neighboring feature maps.\n",
        "\n",
        "#### Mathematics Behind LRN\n",
        "\n",
        "Given a neuron at position $(i, x, y)$ in the feature map, where $i$ is the index of the feature map (channel), $x$ and $y$ are the spatial coordinates, the response after normalization $b_{i, x, y}$ is given by:\n",
        "\n",
        "$$\n",
        "b_{i, x, y} = \\frac{a_{i, x, y}}{\\left( k + \\alpha \\sum_{j=\\max(0, i - \\frac{n}{2})}^{\\min(N-1, i + \\frac{n}{2})} a_{j, x, y}^2 \\right)^\\beta}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $a_{i, x, y}$ is the activity of the neuron before normalization.\n",
        "- $N$ is the total number of feature maps.\n",
        "- $k$, $\\alpha$, and $\\beta$ are hyperparameters.\n",
        "- $n$ is the number of neighboring channels to consider for normalization.\n",
        "\n",
        "#### Advantages of Using LRN\n",
        "\n",
        "1. **Improved Generalization**: By normalizing the activations, LRN can help reduce overfitting by preventing neurons with high activations from dominating the learning process.\n",
        "2. **Enhanced Feature Competition**: It encourages competition among neurons in the same spatial location across different feature maps, which can help in learning more discriminative features.\n",
        "3. **Biologically Inspired**: LRN is loosely inspired by the lateral inhibition observed in biological neurons, where excited neurons reduce the activity of their neighbors.\n",
        "\n",
        "#### Disadvantages of Using LRN\n",
        "\n",
        "1. **Increased Computation**: LRN adds extra computation to the network, which can slow down training and inference.\n",
        "2. **Memory Usage**: The normalization process requires additional memory to store intermediate values, potentially increasing the memory footprint of the model.\n",
        "3. **Diminishing Use**: Modern normalization techniques like Batch Normalization (BN) and Layer Normalization (LN) have largely replaced LRN due to their simpler implementation and better performance in most cases.\n"
      ],
      "metadata": {
        "id": "BnGoaEe1FZuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout Layer\n",
        "\n",
        "Dropout is a regularization technique used in deep learning to prevent overfitting. It was introduced by Geoffrey Hinton et al. in the paper \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\" (2014).\n",
        "\n",
        "In dropout, during the training process, randomly selected neurons are ignored, or \"dropped out\". This means that their contribution to the activation of downstream neurons is temporarily removed on the forward pass, and any weight updates are not applied to the neuron on the backward pass.\n",
        "\n",
        "#### Mathematics Behind Dropout\n",
        "\n",
        "For a given layer, let:\n",
        "- $h_i$ be the output of the $i$-th neuron in that layer.\n",
        "- $p$ be the probability of keeping a neuron (hyperparameter).\n",
        "\n",
        "During training, the dropout layer modifies $h_i$ as follows:\n",
        "\n",
        "$$\n",
        "h_i' = h_i \\cdot z_i\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $z_i \\sim \\text{Bernoulli}(p)$, meaning $z_i$ is a random variable that is 1 with probability $p$ and 0 with probability $1-p$.\n",
        "\n",
        "During testing, the output is scaled by $p$ to account for the neurons that were dropped during training:\n",
        "\n",
        "$$\n",
        "\\text{output} = p \\cdot h_i\n",
        "$$\n",
        "\n",
        "#### Advantages of Using Dropout\n",
        "\n",
        "1. **Reduced Overfitting**: Dropout prevents neurons from co-adapting too much, which can lead to overfitting. By randomly dropping neurons, the network learns more robust features that generalize better to new data.\n",
        "2. **Efficient Regularization**: Dropout is a simple and computationally cheap way to add regularization to a model.\n",
        "\n",
        "#### Disadvantages of Using Dropout\n",
        "\n",
        "1. **Increased Training Time**: Since dropout introduces noise during training, the network may need more epochs to converge.\n",
        "2. **Complexity in Hyperparameter Tuning**: The dropout rate $p$ is an additional hyperparameter that needs to be tuned, which can add complexity to the model optimization process.\n",
        "3. **Not Always Beneficial**: For some models and datasets, dropout may not provide significant benefits and could even hurt performance if not used appropriately.\n"
      ],
      "metadata": {
        "id": "WTai2nD1Jizg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dU_ovVN0JibY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}