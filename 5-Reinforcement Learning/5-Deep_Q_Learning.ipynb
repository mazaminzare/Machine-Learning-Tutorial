{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q-Learning in Reinforcement Learning\n",
        "\n",
        "Deep Q-Learning is a sophisticated reinforcement learning (RL) algorithm that combines Q-Learning with deep neural networks. It is particularly effective for solving complex problems with high-dimensional state spaces, such as playing Atari games and controlling robots. This notebook covers the theoretical foundations of Deep Q-Learning, including mathematical formulations and a practical example.\n",
        "\n",
        "## Key Concepts in Deep Q-Learning\n",
        "\n",
        "### Markov Decision Process (MDP)\n",
        "\n",
        "A Markov Decision Process (MDP) is a mathematical model used for decision-making problems. An MDP is defined as a tuple $(S, A, P, R, \\gamma)$:\n",
        "\n",
        "- **States ($S$)**: The set of all possible states in the environment.\n",
        "- **Actions ($A$)**: The set of all possible actions the agent can take.\n",
        "- **Transition Probability $P(s'|s, a)$**: The probability of transitioning to state $s'$ from state $s$ after taking action $a$.\n",
        "- **Reward Function $R(s, a)$**: The immediate reward received for taking action $a$ in state $s$.\n",
        "- **Discount Factor ($\\gamma$)**: A factor $0 \\leq \\gamma < 1$ that determines the importance of future rewards.\n",
        "\n",
        "### Q-Learning\n",
        "\n",
        "Q-Learning is an off-policy, model-free RL algorithm designed to find the optimal action-selection policy. It aims to learn the Q-value function $Q(s, a)$, which represents the expected future rewards of taking action $a$ in state $s$.\n",
        "\n",
        "#### Bellman Equation for Q-Learning\n",
        "\n",
        "The Bellman equation for Q-Learning updates the Q-values as follows:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $Q(s, a)$ is the current estimate of the Q-value for state $s$ and action $a$.\n",
        "- $\\alpha$ is the learning rate.\n",
        "- $r$ is the reward received after taking action $a$ in state $s$.\n",
        "- $\\gamma$ is the discount factor.\n",
        "- $\\max_{a'} Q(s', a')$ is the maximum Q-value for the next state $s'$ over all actions $a'$.\n",
        "\n",
        "### Deep Q-Learning (DQN)\n",
        "\n",
        "Deep Q-Learning extends Q-Learning by approximating the Q-value function using a deep neural network. This approach enables the handling of high-dimensional state spaces.\n",
        "\n",
        "#### Q-Value Approximation with Neural Networks\n",
        "\n",
        "In Deep Q-Learning, the Q-value function is approximated by a neural network $Q(s, a; \\theta)$, where $\\theta$ represents the network’s parameters. The network outputs Q-values for all possible actions given a state $s$, and the action with the highest Q-value is selected.\n",
        "\n",
        "The goal of Deep Q-Learning is to update the neural network’s parameters to minimize the difference between the predicted Q-values and the target Q-values.\n",
        "\n",
        "The Q-value function is approximated as:\n",
        "\n",
        "$$\n",
        "Q(s, a; \\theta) \\approx Q^*(s, a)\n",
        "$$\n",
        "\n",
        "Where $\\theta$ denotes the parameters of the neural network.\n",
        "\n",
        "#### Target Q-Value Calculation\n",
        "\n",
        "The target Q-value for a given state-action pair is computed as:\n",
        "\n",
        "$$\n",
        "y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)\n",
        "$$\n",
        "\n",
        "Here, $\\theta^-$ are the parameters of the Target Network, which are periodically updated from the main network to stabilize training.\n",
        "\n",
        "#### Loss Function for Training DQN\n",
        "\n",
        "The loss function used to train the DQN is the Mean Squared Error between the predicted Q-values and the target Q-values:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\mathbb{E} \\left[ \\left( r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta) \\right)^2 \\right]\n",
        "$$\n",
        "\n",
        "#### Experience Replay\n",
        "\n",
        "Experience Replay is a technique used to stabilize training by breaking the correlation between consecutive experiences. The agent stores experiences in a replay buffer and samples mini-batches to train the DQN.\n",
        "\n",
        "Each experience is a tuple $(s, a, r, s')$, and the DQN updates are based on a mini-batch sampled from the replay buffer.\n",
        "\n",
        "#### Target Network\n",
        "\n",
        "The Target Network is a copy of the DQN used to compute the target Q-values. It helps to stabilize training by keeping the target Q-values fixed for a number of steps.\n",
        "\n",
        "#### $\\epsilon$-Greedy Policy\n",
        "\n",
        "The $\\epsilon$-greedy policy balances exploration and exploitation:\n",
        "\n",
        "$$\n",
        "a =\n",
        "\\begin{cases}\n",
        "\\text{Random action with probability } \\epsilon \\\\\n",
        "\\text{Action with maximum } Q(s, a; \\theta) \\text{ with probability } 1 - \\epsilon\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "### Deep Q-Learning Algorithm\n",
        "\n",
        "Here’s a high-level overview of the Deep Q-Learning algorithm:\n",
        "\n",
        "1. **Initialize**:\n",
        "   - Initialize the replay buffer.\n",
        "   - Initialize the DQN with random weights $\\theta$.\n",
        "   - Initialize the Target Network with the same weights $\\theta^-$.\n",
        "\n",
        "2. **For each episode**:\n",
        "   - **For each time step**:\n",
        "     - **Select an action $a$** using an $\\epsilon$-greedy policy:\n",
        "       \n",
        "     - **Take action $a$** and observe reward $r$ and next state $s'$.\n",
        "     - **Store the transition** $(s, a, r, s')$ in the replay buffer.\n",
        "     - **Sample a mini-batch** of transitions from the replay buffer.\n",
        "     - **Compute the target Q-values**:\n",
        "       $$\n",
        "       y_i = r_i + \\gamma \\max_{a'} Q(s'_i, a'; \\theta^-)\n",
        "       $$\n",
        "     - **Update the DQN** by minimizing the loss:\n",
        "       $$\n",
        "       L(\\theta) = \\mathbb{E} \\left[ \\left( y_i - Q(s_i, a_i; \\theta) \\right)^2 \\right]\n",
        "       $$\n",
        "     - **Periodically update the Target Network** parameters $\\theta^-$ to match $\\theta$.\n",
        "\n",
        "### Advantages of Deep Q-Learning\n",
        "\n",
        "1. **Handles High-Dimensional State Spaces**: By using neural networks, DQN can effectively handle high-dimensional input spaces, making it suitable for complex tasks like image-based games.\n",
        "2. **Stabilizes Learning**: Techniques like Experience Replay and Target Networks help stabilize the learning process and prevent divergence.\n",
        "3. **Effective in Practice**: DQN has been shown to achieve human-level performance in various Atari games and other challenging domains.\n",
        "\n",
        "### Drawbacks of Deep Q-Learning\n",
        "\n",
        "1. **Sample Inefficiency**: DQN often requires a large number of samples to learn effectively, which can be computationally expensive.\n",
        "2. **Hyperparameter Sensitivity**: The performance of DQN is highly sensitive to the choice of hyperparameters like learning rate, discount factor, and $\\epsilon$.\n",
        "3. **Overestimation Bias**: DQN can suffer from overestimation of Q-values, which can affect the quality of the learned policy.\n",
        "4. **Memory Consumption**: Maintaining a large replay buffer requires significant memory, which can be a limitation for resource-constrained environments.\n",
        "\n",
        "### Mathematical Background\n",
        "\n",
        "#### 1. **Q-Value Update Rule**\n",
        "\n",
        "The Q-value update rule is derived from the Bellman Optimality Equation:\n",
        "\n",
        "$$\n",
        "Q(s, a) = \\mathbb{E} \\left[ r + \\gamma \\max_{a'} Q(s', a') \\mid s, a \\right]\n",
        "$$\n",
        "\n",
        "The update rule adjusts $Q(s, a)$ towards the target $r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$.\n",
        "\n",
        "#### 2. **Loss Function Derivation**\n",
        "\n",
        "The loss function is derived to minimize the difference between the predicted Q-value and the target Q-value:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\mathbb{E} \\left[ \\left( r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta) \\right)^2 \\right]\n",
        "$$\n",
        "\n",
        "Minimizing this loss function helps the neural network learn to approximate the Q-values more accurately.\n",
        "\n",
        "#### 3. **Experience Replay**\n",
        "\n",
        "Experience Replay breaks temporal correlations in experience by storing past experiences and sampling from this buffer. This approach helps improve the efficiency and stability of the learning process.\n",
        "\n",
        "#### 4. **Target Network**\n",
        "\n",
        "The Target Network provides stability by fixing the Q-value targets for a number of steps. This technique mitigates the issue of moving target problems during Q-value updates.\n",
        "\n",
        "#### 5. **$\\epsilon$-Greedy Policy**\n",
        "\n",
        "The $\\epsilon$-greedy policy allows for exploration by selecting random actions with probability $\\epsilon$ and greedy actions with probability $1 - \\epsilon$. This balance between exploration and exploitation is crucial for effective learning.\n",
        "\n",
        "### Numerical Example\n",
        "\n",
        "Let’s illustrate Deep Q-Learning with a simple grid world example.\n",
        "\n",
        "#### Grid World Setup\n",
        "\n",
        "We have a grid world with states $s_1$, $s_2$, $s_3$, $s_4$ and actions \"Right\", \"Down\", \"Up\", \"Left\". The reward is $-1$ for each move, and the discount factor $\\gamma$ is $0.9$.\n",
        "\n",
        "The initial Q-values are:\n",
        "\n",
        "| State | Action | Q-Value |\n",
        "|-------|--------|--------|\n",
        "| $s_1$  | Right  | 0.0    |\n",
        "| $s_1$  | Down   | 0.0    |\n",
        "| $s_2$  | Up     | 0.0    |\n",
        "| $s_2$  | Right  | 0.0    |\n",
        "| $s_3$  | Left   | 0.0    |\n",
        "| $s_3$  | Down   | 0.0    |\n",
        "| $s_4$  | Up     | 0.0    |\n",
        "\n",
        "Assume the following transition:\n",
        "- **State**: $s_1$\n",
        "- **Action**: \"Right\"\n",
        "- **Reward**: $-1$\n",
        "- **Next State**: $s_2$\n",
        "- **Discount Factor**: $\\gamma = 0.9$\n",
        "- **Learning Rate**: $\\alpha = 0.1$\n",
        "\n",
        "The target Q-value is:\n",
        "\n",
        "$$\n",
        "y = r + \\gamma \\max_{a'} Q(s_2, a'; \\theta^-) = -1 + 0.9 \\cdot 0 = -1\n",
        "$$\n",
        "\n",
        "Update the Q-value for state-action pair $(s_1, \\text{Right})$:\n",
        "\n",
        "$$\n",
        "Q(s_1, \\text{Right}; \\theta) \\leftarrow Q(s_1, \\text{Right}; \\theta) + \\alpha \\left[ y - Q(s_1, \\text{Right}; \\theta) \\right]\n",
        "$$\n",
        "\n",
        "Substitute the values:\n",
        "\n",
        "$$\n",
        "Q(s_1, \\text{Right}; \\theta) \\leftarrow 0 + 0.1 \\left[ -1 - 0 \\right] = -0.1\n",
        "$$\n",
        "\n",
        "\n",
        "# Deep Q-Learning (DQL)\n",
        "\n",
        "## Advantages\n",
        "\n",
        "1. **Handling High-Dimensional Input**:\n",
        "   - DQL can process high-dimensional sensory input like images directly, which traditional Q-Learning struggles with. This is achieved by using deep convolutional neural networks.\n",
        "\n",
        "2. **Learning Complex Policies**:\n",
        "   - The use of deep networks enables DQL to learn complex policies that are not easily achievable with shallow or linear models.\n",
        "\n",
        "3. **Scalability**:\n",
        "   - DQL can be scaled to work with large action and state spaces, making it suitable for complex environments such as video games and robotics.\n",
        "\n",
        "4. **Sample Efficiency**:\n",
        "   - Techniques such as experience replay and target networks help improve sample efficiency, allowing the algorithm to learn more effectively from the same set of experiences.\n",
        "\n",
        "## Drawbacks\n",
        "\n",
        "1. **Computationally Intensive**:\n",
        "   - Training deep networks is computationally expensive and time-consuming, requiring significant hardware resources like GPUs.\n",
        "\n",
        "2. **Instability and Divergence**:\n",
        "   - DQL can suffer from instability and divergence issues during training due to the correlation of sequential data and the non-stationarity of the target values.\n",
        "\n",
        "3. **Hyperparameter Sensitivity**:\n",
        "   - The performance of DQL is highly sensitive to the choice of hyperparameters, making it challenging to tune for optimal performance.\n",
        "\n",
        "## Key Innovations\n",
        "\n",
        "1. **Experience Replay**:\n",
        "   - Experience replay involves storing past experiences in a replay buffer and randomly sampling from it during training. This breaks the correlation between consecutive experiences and improves training stability.\n",
        "\n",
        "2. **Target Network**:\n",
        "   - The use of a separate target network helps to stabilize training by providing consistent target values. The target network is updated less frequently than the primary network, reducing the likelihood of oscillations and divergence.\n",
        "\n",
        "3. **Deep Convolutional Networks**:\n",
        "   - Integrating deep convolutional networks allows DQL to handle raw pixel input and extract hierarchical features, which is crucial for tasks involving visual data.\n",
        "\n",
        "4. **End-to-End Learning**:\n",
        "   - DQL facilitates end-to-end learning where both the feature extraction and the policy are learned simultaneously, leading to more cohesive and effective policies.\n"
      ],
      "metadata": {
        "id": "0Bi7tcH6U9Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_9vhVgqcFg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}