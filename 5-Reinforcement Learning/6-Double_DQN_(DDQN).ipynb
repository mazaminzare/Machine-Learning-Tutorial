{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Double DQN (DDQN): A Comprehensive Tutorial\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Double DQN (DDQN) is an enhancement of the standard Deep Q-Network (DQN) algorithm that addresses the overestimation bias in Q-learning methods. By using two separate networks, DDQN aims to provide more accurate value estimates, leading to improved policy performance in reinforcement learning tasks.\n",
        "\n",
        "## Mathematical Background\n",
        "\n",
        "### Markov Decision Process (MDP)\n",
        "\n",
        "An MDP is defined by:\n",
        "- **S**: A set of states.\n",
        "- **A**: A set of actions.\n",
        "- **P**: State transition probabilities, where $P(s' \\mid s, a)$ represents the probability of transitioning to state $s'$ from state $s$ after taking action $a$.\n",
        "- **R**: Reward function, where $R(s, a)$ is the expected reward received after taking action $a$ in state $s$.\n",
        "- **$\\gamma$**: Discount factor, $0 \\leq \\gamma < 1$, which represents the importance of future rewards.\n",
        "\n",
        "### Q-Function\n",
        "\n",
        "The Q-function, or action-value function, $Q(s, a)$, represents the expected cumulative reward of taking action $a$ in state $s$ and then following a policy $\\pi$:\n",
        "\n",
        "$$\n",
        "Q^\\pi(s, a) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\bigg| s_0 = s, a_0 = a, \\pi \\right]\n",
        "$$\n",
        "\n",
        "### Double DQN Update Rule\n",
        "\n",
        "In DDQN, we maintain two separate networks:\n",
        "- **Online Network**: Used to select actions.\n",
        "- **Target Network**: Used to evaluate actions.\n",
        "\n",
        "The update rule is as follows:\n",
        "\n",
        "1. Select action using the online network:\n",
        "   $$ a_t = \\text{argmax}_a Q_{\\text{online}}(s_t, a) $$\n",
        "\n",
        "2. Compute the target Q-value using the target network:\n",
        "   $$ Q_{\\text{target}}(s_t, a_t) = r_t + \\gamma Q_{\\text{target}}(s_{t+1}, \\text{argmax}_a Q_{\\text{online}}(s_{t+1}, a)) $$\n",
        "\n",
        "3. Update the online network using:\n",
        "   $$ Q_{\\text{online}}(s_t, a_t) \\leftarrow Q_{\\text{online}}(s_t, a_t) + \\alpha \\left[ Q_{\\text{target}}(s_t, a_t) - Q_{\\text{online}}(s_t, a_t) \\right] $$\n",
        "\n",
        "### Algorithm Steps\n",
        "\n",
        "1. **Initialize**:\n",
        "   - Initialize the online and target networks with random weights.\n",
        "   - Initialize the replay memory to store experiences.\n",
        "\n",
        "2. **Policy Selection**:\n",
        "   - Choose an action $a_t$ for the current state $s_t$ using an epsilon-greedy policy based on the online network.\n",
        "\n",
        "3. **Experience Replay**:\n",
        "   - Store the experience $(s_t, a_t, r_t, s_{t+1})$ in replay memory.\n",
        "\n",
        "4. **Sample Mini-Batch**:\n",
        "   - Sample a mini-batch of experiences from the replay memory.\n",
        "\n",
        "5. **Target Calculation**:\n",
        "   - Calculate the target using the target network.\n",
        "\n",
        "6. **Network Update**:\n",
        "   - Update the online network based on the calculated targets.\n",
        "\n",
        "7. **Update Target Network**:\n",
        "   - Periodically update the target network weights to match the online network weights.\n",
        "\n",
        "### Pseudocode\n",
        "\n",
        "```python\n",
        "Initialize online and target networks\n",
        "Initialize replay memory\n",
        "Repeat for each episode:\n",
        "    Initialize state s\n",
        "    Choose action a using policy derived from online network (epsilon-greedy)\n",
        "    Repeat for each step of the episode:\n",
        "        Take action a, observe r, s'\n",
        "        Store (s, a, r, s') in replay memory\n",
        "        Sample mini-batch from replay memory\n",
        "        For each (s, a, r, s') in mini-batch:\n",
        "            Calculate target using target network\n",
        "            Update online network\n",
        "        Update target network periodically\n",
        "    until s is terminal\n"
      ],
      "metadata": {
        "id": "Mx37hGVbmyF-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "03SHp52kIxyn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}