{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Overview of Policy Gradient Methods (Past to 2024)\n",
        "\n",
        "Policy gradient methods have evolved significantly since their inception, with numerous algorithms developed to improve their stability, efficiency, and performance. This tutorial provides a chronological overview of key policy gradient methods from the past to 2024.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "1. **Policy ($\\pi_{\\theta}(a|s)$)**: A mapping from states to actions, parameterized by $\\theta$, representing the probability of taking action $a$ given state $s$.\n",
        "2. **Value Function ($V^{\\pi}(s)$)**: The expected return starting from state $s$ and following policy $\\pi$.\n",
        "3. **Action-Value Function ($Q^{\\pi}(s, a)$)**: The expected return starting from state $s$, taking action $a$, and then following policy $\\pi$.\n",
        "4. **Advantage Function ($A^{\\pi}(s, a)$)**: Measures how much better taking action $a$ in state $s$ is compared to the average action. It is defined as $A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)$.\n",
        "\n",
        "## Mathematical Background\n",
        "\n",
        "### Objective Function\n",
        "\n",
        "The objective in reinforcement learning is to maximize the expected return from the start state $s_0$:\n",
        "\n",
        "$$\n",
        "J(\\pi_{\\theta}) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\right]\n",
        "$$\n",
        "\n",
        "where $\\gamma$ is the discount factor and $r(s_t, a_t)$ is the reward received at time step $t$.\n",
        "\n",
        "### Policy Gradient Theorem\n",
        "\n",
        "The policy gradient theorem provides the gradient of the objective function with respect to the policy parameters $\\theta$:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) Q^{\\pi_{\\theta}}(s_t, a_t) \\right]\n",
        "$$\n",
        "\n",
        "### Simplifying the Gradient\n",
        "\n",
        "In practice, we use a sample estimate of the gradient:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J(\\pi_{\\theta}) \\approx \\frac{1}{N} \\sum_{i=1}^N \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t^i | s_t^i) Q^{\\pi_{\\theta}}(s_t^i, a_t^i) \\right]\n",
        "$$\n",
        "\n",
        "where $N$ is the number of samples.\n",
        "\n",
        "### Advantage Function\n",
        "\n",
        "To reduce variance in the gradient estimate, we often use the advantage function $A^{\\pi}(s, a)$:\n",
        "\n",
        "$$\n",
        "A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)\n",
        "$$\n",
        "\n",
        "The policy gradient can then be written as:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) A^{\\pi_{\\theta}}(s_t, a_t) \\right]\n",
        "$$\n",
        "\n",
        "## Evolution of Policy Gradient Methods\n",
        "\n",
        "### REINFORCE (Monte Carlo Policy Gradient)\n",
        "\n",
        "The REINFORCE algorithm (Williams, 1992) is one of the earliest policy gradient methods. It uses the return $G_t$ as an unbiased estimate of $Q^{\\pi}(s_t, a_t)$:\n",
        "\n",
        "1. **Initialize**: Initialize policy parameters $\\theta$.\n",
        "2. **Repeat**:\n",
        "    - Collect trajectory $(s_0, a_0, r_1, s_1, a_1, r_2, \\ldots, s_T, a_T)$ by following policy $\\pi_{\\theta}$.\n",
        "    - Compute return for each time step:\n",
        "      $$\n",
        "      G_t = \\sum_{k=t}^T \\gamma^{k-t} r_k\n",
        "      $$\n",
        "    - Update policy parameters:\n",
        "      $$\n",
        "      \\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) G_t\n",
        "      $$\n",
        "\n",
        "**Advantages**:\n",
        "- Simple to understand and implement.\n",
        "- Unbiased gradient estimates.\n",
        "\n",
        "**Disadvantages**:\n",
        "- High variance in gradient estimates.\n",
        "- Inefficient for long-horizon problems.\n",
        "\n",
        "**Drawbacks**:\n",
        "- Requires full trajectories to compute returns, making it unsuitable for real-time learning.\n",
        "\n",
        "### Baseline\n",
        "\n",
        "A baseline $b(s)$ can be subtracted from the return to reduce variance without changing the expected value of the gradient:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) (G_t - b(s_t)) \\right]\n",
        "$$\n",
        "\n",
        "A common choice for the baseline is the value function $V^{\\pi}(s)$.\n",
        "\n",
        "### Actor-Critic Methods\n",
        "\n",
        "Actor-Critic methods (Konda and Tsitsiklis, 2000) combine policy gradient methods with value function approximation. The actor updates the policy parameters $\\theta$ and the critic updates the value function parameters $\\phi$.\n",
        "\n",
        "1. **Initialize**: Initialize policy parameters $\\theta$ and value function parameters $\\phi$.\n",
        "2. **Repeat**:\n",
        "    - Collect trajectory $(s_t, a_t, r_t, s_{t+1})$ by following policy $\\pi_{\\theta}$.\n",
        "    - Compute TD error:\n",
        "      $$\n",
        "      \\delta_t = r_t + \\gamma V_{\\phi}(s_{t+1}) - V_{\\phi}(s_t)\n",
        "      $$\n",
        "    - Update critic by minimizing the loss function:\n",
        "  $\n",
        "      L(\\phi) = \\mathbb{E}_{(s_t, r_t, s_{t+1}) \\sim \\pi_{\\theta}} \\left[ \\delta_t^2 \\right]\n",
        "  $\n",
        "  \n",
        "      $$\n",
        "      \\phi \\leftarrow \\phi - \\alpha_c \\nabla_{\\phi} L(\\phi)\n",
        "      $$\n",
        "    - Update actor by ascending the policy gradient:\n",
        "      $$\n",
        "      \\theta \\leftarrow \\theta + \\alpha_a \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) \\delta_t\n",
        "      $$\n",
        "\n",
        "**Advantages**:\n",
        "- Lower variance in gradient estimates compared to REINFORCE.\n",
        "- Can handle continuous action spaces.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Requires careful tuning of both actor and critic learning rates.\n",
        "- Can be unstable due to the interplay between actor and critic updates.\n",
        "\n",
        "**Drawbacks**:\n",
        "- Sensitive to hyperparameters.\n",
        "- Critic updates can be biased if the value function is not well-approximated.\n",
        "\n",
        "### Trust Region Policy Optimization (TRPO)\n",
        "\n",
        "TRPO (Schulman et al., 2015) addresses the issue of large policy updates in policy gradient methods by ensuring that updates stay within a trust region. This is achieved by optimizing a surrogate objective function subject to a constraint on the KL divergence between the new and old policies:\n",
        "\n",
        "$$\n",
        "\\max_{\\theta} \\mathbb{E}_{s_t, a_t \\sim \\pi_{\\theta_{\\text{old}}}} \\left[ \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} A^{\\pi_{\\theta_{\\text{old}}}}(s_t, a_t) \\right]\n",
        "$$\n",
        "\n",
        "subject to:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{s_t \\sim \\pi_{\\theta_{\\text{old}}}} \\left[ D_{\\text{KL}} \\left( \\pi_{\\theta_{\\text{old}}}(\\cdot | s_t) \\| \\pi_{\\theta}(\\cdot | s_t) \\right) \\right] \\leq \\delta\n",
        "$$\n",
        "\n",
        "**Advantages**:\n",
        "- More stable updates by ensuring policy changes are not too large.\n",
        "- Effective for complex environments.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Computationally expensive due to the need for second-order optimization techniques.\n",
        "- Requires careful tuning of the trust region size.\n",
        "\n",
        "**Drawbacks**:\n",
        "- Implementation complexity.\n",
        "- Can be slow to converge due to conservative updates.\n",
        "\n",
        "### Proximal Policy Optimization (PPO)\n",
        "\n",
        "PPO (Schulman et al., 2017) simplifies TRPO by using a clipped surrogate objective to limit the size of policy updates, making it more efficient and easier to implement:\n",
        "\n",
        "$$\n",
        "L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_{t} \\left[ \\min \\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t \\right) \\right]\n",
        "$$\n",
        "\n",
        "where $r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}$ and $\\epsilon$ is a hyperparameter that controls the clip range.\n",
        "\n",
        "**Advantages**:\n",
        "- Simpler to implement compared to TRPO.\n",
        "- More stable and robust policy updates.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Still requires significant hyperparameter tuning.\n",
        "- May struggle with highly stochastic environments.\n",
        "\n",
        "**Drawbacks**:\n",
        "- Clipping can introduce bias in gradient estimates.\n",
        "- Performance can be sensitive to the choice of clipping parameter.\n",
        "\n",
        "### Soft Actor-Critic (SAC)\n",
        "\n",
        "SAC (Haarnoja et al., 2018) is an off-policy actor-critic method that aims to maximize both the expected return and the entropy of the policy, promoting exploration:\n",
        "\n",
        "$$\n",
        "J(\\pi_{\\theta}) = \\mathbb{E}_{(s_t, a_t) \\sim \\mathcal{D}} \\left[ \\min_{j=1,2} Q_{\\phi_j}(s_t, a_t) - \\alpha \\log \\pi_{\\theta}(a_t | s_t) \\right]\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is the temperature parameter that determines the relative importance of the entropy term against the reward.\n",
        "\n",
        "**Advantages**:\n",
        "- Encourages exploration through entropy maximization.\n",
        "- Efficient and stable learning process.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Requires tuning of the temperature parameter $\\alpha$.\n",
        "- More computationally intensive due to dual Q-networks and entropy term.\n",
        "\n",
        "**Drawbacks**:\n",
        "- Performance can be sensitive to hyperparameters.\n",
        "- Complexity in balancing exploration and exploitation.\n",
        "\n",
        "### Asynchronous Advantage Actor-Critic (A3C)\n",
        "\n",
        "A3C (Mnih et al., 2016) uses multiple worker agents to explore different parts of the state space in parallel. Each worker updates a global policy and value function using local gradients:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) A_t\n",
        "$$\n",
        "\n",
        "where $A_t$ is the advantage function.\n",
        "\n",
        "**Advantages**:\n",
        "- Faster training through parallelism.\n",
        "- Reduces correlation in experience data, improving stability.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Requires a distributed computing setup.\n",
        "- More complex implementation due to asynchronous updates.\n",
        "\n",
        "**Drawbacks**:\n",
        "- Difficult to debug due to concurrency issues.\n",
        "- Can suffer from high variance in gradient estimates.\n",
        "\n",
        "### Recent Advances (2020-2024)\n",
        "\n",
        "#### Distributed Proximal Policy Optimization (DPPO)\n",
        "\n",
        "DPPO extends PPO to a distributed setting, leveraging multiple workers to collect experiences and update the global policy in parallel, improving sample efficiency and training speed.\n",
        "\n",
        "**Advantages**:\n",
        "- Scalable and efficient for large-scale problems.\n",
        "- Maintains stability and robustness of PPO.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Requires distributed infrastructure.\n",
        "- Increased complexity in implementation.\n",
        "\n",
        "**Drawbacks**:\n",
        "- Communication overhead between workers.\n",
        "- Synchronization issues can arise.\n",
        "\n",
        "#### Importance Weighted Actor-Learner Architectures (IMPALA)\n",
        "\n",
        "IMPALA (Espeholt et al., 2018) introduces a novel off-policy correction mechanism, V-trace, to handle delayed gradient updates in distributed settings:\n",
        "\n",
        "$$\n",
        "V(s_t) = V(x_t) + \\sum_{i=t}^{T-1} \\gamma^{i-t} \\prod_{j=t}^{i-1} c_j \\delta_i\n",
        "$$\n",
        "\n",
        "where $c_j$ is a clipping term to ensure stability.\n",
        "\n",
        "**Advantages**:\n",
        "- Efficient and scalable for large-scale problems.\n",
        "- Corrects for off-policy data, improving sample efficiency.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Requires distributed setup.\n",
        "- Implementation complexity.\n",
        "\n",
        "**Drawbacks**:\n",
        "- Potential instability in large action spaces.\n",
        "- Requires careful tuning of off-policy correction terms.\n",
        "\n",
        "#### Deep Deterministic Policy Gradient (DDPG)\n",
        "\n",
        "DDPG (Lillicrap et al., 2016) is an actor-critic algorithm that combines deterministic policy gradients with deep learning, enabling the learning of policies in high-dimensional continuous action spaces:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{s_t \\sim \\mathcal{D}} \\left[ \\nabla_{\\theta} \\pi_{\\theta}(s_t) \\nabla_{a} Q^{\\pi_{\\theta}}(s_t, a) \\bigg|_{a=\\pi_{\\theta}(s_t)} \\right]\n",
        "$$\n",
        "\n",
        "**Advantages**:\n",
        "- Effective for high-dimensional continuous action spaces.\n",
        "- Uses experience replay and target networks to stabilize training.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Can suffer from overestimation bias.\n",
        "- Sensitive to hyperparameters and noise processes.\n",
        "\n",
        "**Drawbacks**:\n",
        "- Requires careful tuning and regularization.\n",
        "- High variance in gradient estimates.\n",
        "\n",
        "#### Twin Delayed Deep Deterministic Policy Gradient (TD3)\n",
        "\n",
        "TD3 (Fujimoto et al., 2018) improves upon DDPG by addressing overestimation bias in value function estimation and incorporating tricks like clipped double Q-learning and delayed policy updates:\n",
        "\n",
        "$$\n",
        "Q_{\\phi}^{\\text{target}}(s_t, a_t) = r_t + \\gamma \\min_{i=1,2} Q_{\\phi'_i}(s_{t+1}, \\pi_{\\theta'}(s_{t+1}) + \\epsilon)\n",
        "$$\n",
        "\n",
        "where $\\epsilon$ is added noise to improve robustness.\n",
        "\n",
        "**Advantages**:\n",
        "- Reduces overestimation bias.\n",
        "- More stable and reliable performance compared to DDPG.\n",
        "\n",
        "**Disadvantages**:\n",
        "- More complex implementation due to additional networks.\n",
        "- Increased computational cost.\n",
        "\n",
        "**Drawbacks**:\n",
        "- Sensitive to hyperparameters.\n",
        "- Requires careful tuning of noise processes.\n",
        "\n",
        "#### Stochastic Actor-Critic (SAC)\n",
        "\n",
        "Stochastic Actor-Critic (SAC) combines elements of SAC and stochastic policy optimization, aiming to better handle environments with stochastic dynamics and rewards.\n",
        "\n",
        "**Advantages**:\n",
        "- Handles stochastic environments effectively.\n",
        "- Balances exploration and exploitation through entropy regularization.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Increased computational complexity.\n",
        "- Requires careful tuning of entropy-related hyperparameters.\n",
        "\n",
        "**Drawbacks**:\n",
        "- Can be less sample efficient.\n",
        "- Performance sensitive to hyperparameter settings.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Policy gradient methods have seen significant advancements from early algorithms like REINFORCE to state-of-the-art methods like PPO, SAC, and TD3. These methods have improved the stability, efficiency, and performance of policy optimization in reinforcement learning, making them applicable to a wide range of complex tasks.\n",
        "\n",
        "The field continues to evolve, with ongoing research aimed at further enhancing the scalability, sample efficiency, and robustness of policy gradient methods.\n"
      ],
      "metadata": {
        "id": "uxjhhKfBZW9u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOTA93TyUhdt"
      },
      "outputs": [],
      "source": []
    }
  ]
}