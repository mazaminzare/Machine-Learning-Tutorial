{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning Categories: Model-Based and Model-Free\n",
        "\n",
        "Reinforcement Learning (RL) algorithms can be categorized into two main types: Model-Based and Model-Free. Each type has its own approach to learning and interacting with the environment. Here, we provide an in-depth explanation of both categories.\n",
        "\n",
        "## Model-Free Reinforcement Learning\n",
        "\n",
        "Model-Free methods do not use a model of the environment. They learn policies or value functions solely based on interactions with the environment. These methods are divided into Value-Based, Policy-Based, and Actor-Critic methods.\n",
        "\n",
        "### Value-Based Methods\n",
        "\n",
        "Value-Based methods focus on learning a value function that estimates the expected return (reward) for each state or state-action pair. The policy is derived implicitly from the value function by selecting actions that maximize the value.\n",
        "\n",
        "#### Q-Learning (1989)\n",
        "- **Type**: Off-Policy\n",
        "- **Description**: Learns the optimal action-value function $Q(s, a)$ by iteratively updating estimates using the Bellman equation:\n",
        "  $$\n",
        "  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "  $$\n",
        "  where $\\alpha$ is the learning rate and $\\gamma$ is the discount factor.\n",
        "\n",
        "#### SARSA (1996)\n",
        "- **Type**: On-Policy\n",
        "- **Description**: Updates the action-value function $Q(s, a)$ based on the action actually taken by the policy:\n",
        "  $$\n",
        "  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma Q(s', a') - Q(s, a) \\right]\n",
        "  $$\n",
        "  where $a'$ is the action taken in the next state $s'$.\n",
        "\n",
        "#### Deep Q-Networks (DQN) (2013)\n",
        "- **Type**: Off-Policy\n",
        "- **Description**: Extends Q-Learning by using deep neural networks to approximate $Q(s, a)$. Introduces experience replay and target networks for stability.\n",
        "\n",
        "### Policy-Based Methods\n",
        "\n",
        "Policy-Based methods directly learn a policy that maps states to actions without explicitly learning a value function. They optimize the policy by adjusting its parameters to maximize the expected return.\n",
        "\n",
        "#### REINFORCE (1992)\n",
        "- **Type**: On-Policy\n",
        "- **Description**: Uses Monte Carlo methods to update policy parameters $\\theta$ in the direction of the gradient of expected return:\n",
        "  $$\n",
        "  \\nabla J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta (s, a) R \\right]\n",
        "  $$\n",
        "  where $R$ is the total return following state $s$ and action $a$.\n",
        "\n",
        "#### Proximal Policy Optimization (PPO) (2017)\n",
        "- **Type**: On-Policy\n",
        "- **Description**: Improves the stability and reliability of policy gradient methods by using a clipped objective function that restricts large policy updates.\n",
        "\n",
        "### Actor-Critic Methods\n",
        "\n",
        "Actor-Critic methods combine value-based and policy-based approaches. They consist of two components: an actor that updates the policy and a critic that updates the value function, providing feedback to the actor.\n",
        "\n",
        "#### Asynchronous Advantage Actor-Critic (A3C) (2016)\n",
        "- **Type**: On-Policy\n",
        "- **Description**: Uses multiple agents to explore different parts of the state space in parallel, updating a global policy and value function asynchronously.\n",
        "\n",
        "#### Soft Actor-Critic (SAC) (2018)\n",
        "- **Type**: Off-Policy\n",
        "- **Description**: Incorporates a maximum entropy framework to encourage exploration by adding an entropy term to the reward, balancing exploration and exploitation.\n",
        "\n",
        "## Model-Based Reinforcement Learning\n",
        "\n",
        "Model-Based methods use a model of the environment to predict the next state and reward given a current state and action. These methods can simulate future states, enabling the agent to plan its actions more effectively.\n",
        "\n",
        "### Dyna-Q (1991)\n",
        "- **Description**: Combines model-free Q-Learning with a model-based approach. The agent learns a model of the environment and uses it to simulate experiences, updating the Q-values using both real and simulated experiences.\n",
        "\n",
        "### Monte Carlo Tree Search (MCTS) (2006)\n",
        "- **Description**: Uses a tree structure to represent the possible future states of the environment. It performs simulations to evaluate the outcomes of different actions, guiding the agent's decision-making process.\n",
        "\n",
        "### MuZero (2019)\n",
        "- **Description**: A sophisticated model-based approach that learns a model of the environment implicitly through a combination of a learned value function, a policy, and a model that predicts future states and rewards. It balances model-based planning and model-free learning.\n",
        "\n",
        "### World Models (2018)\n",
        "- **Description**: Constructs a generative model of the environment, enabling the agent to simulate and plan future actions. The agent uses this internal model to learn policies efficiently.\n",
        "\n",
        "## Summary\n",
        "\n",
        "### Model-Free vs. Model-Based\n",
        "\n",
        "- **Model-Free**:\n",
        "  - Do not use an explicit model of the environment.\n",
        "  - Learn policies or value functions from direct interaction with the environment.\n",
        "  - Examples: Q-Learning, SARSA, DQN, PPO, A3C, SAC.\n",
        "\n",
        "- **Model-Based**:\n",
        "  - Use a model of the environment to predict future states and rewards.\n",
        "  - Enable planning by simulating potential future experiences.\n",
        "  - Examples: Dyna-Q, MCTS, MuZero, World Models.\n",
        "\n",
        "Understanding these categories and their respective methods is crucial for selecting the appropriate RL algorithm based on the specific problem and computational resources available.\n"
      ],
      "metadata": {
        "id": "MRxvBxLcxrSV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKRY1bACxp7w"
      },
      "outputs": [],
      "source": []
    }
  ]
}