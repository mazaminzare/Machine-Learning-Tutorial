{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Fundamentals of Reinforcement Learning: An In-Depth Tutorial\n",
        "\n",
        "#### Introduction to Reinforcement Learning\n",
        "\n",
        "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. Unlike supervised learning, where the model learns from labeled data, RL is based on learning from the consequences of actions to achieve long-term goals.\n",
        "\n",
        "#### Mathematical Background\n",
        "\n",
        "The RL framework can be formalized using the **Markov Decision Process (MDP)**, which is defined by the tuple $(S, A, P, R, \\gamma)$, where:\n",
        "\n",
        "- **$S$** is a set of states.\n",
        "- **$A$** is a set of actions.\n",
        "- **$P(s'|s, a)$** is the state transition probability, representing the probability of moving from state $s$ to state $s'$ given action $a$.\n",
        "- **$R(s, a)$** is the reward function, providing the immediate reward received after taking action $a$ in state $s$.\n",
        "- **$\\gamma$** is the discount factor, which determines the importance of future rewards (where $0 \\leq \\gamma < 1$).\n",
        "\n",
        "##### Markov Decision Process (MDP)\n",
        "\n",
        "An MDP provides a mathematical model for decision-making where outcomes are partly random and partly under the control of the decision maker. The goal of the RL agent is to find a policy $\\pi$ that maximizes the expected cumulative reward over time.\n",
        "\n",
        "The **policy** $\\pi(a|s)$ is a strategy that specifies the probability of taking action $a$ given state $s$.\n",
        "\n",
        "##### Key Components of MDPs\n",
        "\n",
        "1. **State Space ($S$)**:\n",
        "   The set of all possible situations in which the agent might find itself.\n",
        "\n",
        "2. **Action Space ($A$)**:\n",
        "   The set of all possible actions the agent can take.\n",
        "\n",
        "3. **Transition Probability ($P$)**:\n",
        "   The probability of moving from one state to another given a specific action. Mathematically, $P(s'|s, a)$ denotes the probability of ending up in state $s'$ when action $a$ is taken in state $s$.\n",
        "\n",
        "4. **Reward Function ($R$)**:\n",
        "   A function that provides the immediate reward received after taking action $a$ in state $s$. This can be represented as $R(s, a)$.\n",
        "\n",
        "5. **Discount Factor ($\\gamma$)**:\n",
        "   A factor that quantifies the importance of future rewards versus immediate rewards.\n",
        "\n",
        "##### Value Functions\n",
        "\n",
        "To evaluate the quality of a policy, we use **value functions** which estimate the expected cumulative reward.\n",
        "\n",
        "1. **State Value Function ($V^\\pi(s)$)**:\n",
        "   The expected return (total reward) starting from state $s$ and following policy $\\pi$:\n",
        "\n",
        "   $$\n",
        "   V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s \\right]\n",
        "   $$\n",
        "\n",
        "2. **Action Value Function ($Q^\\pi(s, a)$)**:\n",
        "   The expected return of taking action $a$ in state $s$ and then following policy $\\pi$:\n",
        "\n",
        "   $$\n",
        "   Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s, a_0 = a \\right]\n",
        "   $$\n",
        "\n",
        "##### Bellman Equations\n",
        "\n",
        "The **Bellman Equation** provides a recursive decomposition for the value functions.\n",
        "\n",
        "1. **Bellman Equation for State Value Function**:\n",
        "\n",
        "   $$\n",
        "   V^\\pi(s) = \\mathbb{E}_\\pi \\left[ R(s, a) + \\gamma V^\\pi(s') \\mid s \\right]\n",
        "   $$\n",
        "\n",
        "   where $a \\sim \\pi(\\cdot | s)$ and $s' \\sim P(\\cdot | s, a)$.\n",
        "\n",
        "2. **Bellman Equation for Action Value Function**:\n",
        "\n",
        "   $$\n",
        "   Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^\\pi(s') \\mid s, a \\right]\n",
        "   $$\n",
        "\n",
        "##### Optimal Policies and Value Functions\n",
        "\n",
        "The goal of the RL agent is to find the optimal policy $\\pi^*$ that maximizes the expected cumulative reward.\n",
        "\n",
        "1. **Optimal State Value Function** ($V^*(s)$):\n",
        "\n",
        "   $$\n",
        "   V^*(s) = \\max_\\pi V^\\pi(s)\n",
        "   $$\n",
        "\n",
        "2. **Optimal Action Value Function** ($Q^*(s, a)$):\n",
        "\n",
        "   $$\n",
        "   Q^*(s, a) = \\max_\\pi Q^\\pi(s, a)\n",
        "   $$\n",
        "\n",
        "   The optimal policy can be derived from the optimal action-value function:\n",
        "\n",
        "   $$\n",
        "   \\pi^*(s) = \\text{argmax}_a Q^*(s, a)\n",
        "   $$\n",
        "\n",
        "##### Key Properties of Reinforcement Learning\n",
        "\n",
        "1. **Exploration vs. Exploitation**:\n",
        "   - **Exploration** involves trying new actions to discover their effects.\n",
        "   - **Exploitation** involves using known actions that yield the highest reward.\n",
        "   - Balancing these two aspects is crucial for effective learning.\n",
        "\n",
        "2. **Temporal Difference Learning**:\n",
        "   - Methods like Q-Learning use the Bellman equation to update estimates of the value function based on observed rewards and estimated future values.\n",
        "   - It combines ideas from Monte Carlo methods and Dynamic Programming.\n",
        "\n",
        "3. **Value Iteration and Policy Iteration**:\n",
        "   - **Value Iteration** updates value functions to eventually converge to the optimal policy.\n",
        "   - **Policy Iteration** alternates between policy evaluation and policy improvement to find the optimal policy.\n",
        "\n",
        "4. **Model-Free vs. Model-Based Methods**:\n",
        "   - **Model-Free** methods like Q-Learning learn value functions directly from experience.\n",
        "   - **Model-Based** methods use a model of the environment to plan and make decisions.\n",
        "\n",
        "##### Important Notes on Reinforcement Learning\n",
        "\n",
        "1. **Delayed Rewards**:\n",
        "   - Rewards may be delayed, requiring the agent to learn to associate actions with long-term outcomes.\n",
        "   - Techniques such as eligibility traces can help in addressing the credit assignment problem.\n",
        "\n",
        "2. **Scalability**:\n",
        "   - RL algorithms can struggle with large state and action spaces. Techniques like function approximation and deep learning (Deep Q-Networks) are used to handle complex environments.\n",
        "\n",
        "3. **Sample Efficiency**:\n",
        "   - RL algorithms may require a large number of interactions with the environment. Improving sample efficiency through techniques like experience replay is a key research area.\n",
        "\n",
        "4. **Stability and Convergence**:\n",
        "   - Many RL algorithms can be unstable or have convergence issues. Ensuring stability through techniques like target networks in Q-Learning is essential for successful learning.\n",
        "\n",
        "5. **Multi-Agent Systems**:\n",
        "   - In multi-agent environments, the presence of other agents introduces additional complexities such as non-stationarity.\n",
        "   - Cooperative, competitive, and mixed settings require different strategies and algorithms.\n",
        "\n",
        "##### Numerical Example\n",
        "\n",
        "Let’s go through a simple numerical example of an MDP.\n",
        "\n",
        "Consider a grid world with states $S = \\{s_0, s_1, s_2, s_3\\}$ and actions $A = \\{a_0, a_1\\}$ where $a_0$ moves left and $a_1$ moves right. We define the following rewards and transitions:\n",
        "\n",
        "- If the agent is in $s_0$ and takes action $a_1$, it moves to $s_1$ and receives a reward of $+1$.\n",
        "- If the agent is in $s_1$ and takes action $a_1$, it moves to $s_2$ and receives a reward of $+1$.\n",
        "- If the agent is in $s_2$ and takes action $a_1$, it moves to $s_3$ and receives a reward of $+1$.\n",
        "- If the agent is in $s_3$ and takes any action, it receives a reward of $0$ and stays in $s_3$.\n",
        "\n",
        "Let’s assume the discount factor $\\gamma = 0.9$.\n",
        "\n",
        "We will calculate the optimal state value function using the Bellman Equation.\n",
        "\n",
        "1. **Initialize Value Functions**:\n",
        "\n",
        "   Let’s initialize $V(s_0) = 0$, $V(s_1) = 0$, $V(s_2) = 0$, $V(s_3) = 0$.\n",
        "\n",
        "2. **Update Value Functions**:\n",
        "\n",
        "   We will use the Bellman equation to iteratively update the value functions:\n",
        "\n",
        "   For each state, the value function update is:\n",
        "\n",
        "   $$\n",
        "   V(s) = \\mathbb{E}_\\pi \\left[ R(s, a) + \\gamma V(s') \\mid s \\right]\n",
        "   $$\n",
        "\n",
        "   We perform the updates for all states:\n",
        "\n",
        "   - For $s_2$: $V(s_2) = 0 + \\gamma \\cdot 0 = 0$\n",
        "   - For $s_1$: $V(s_1) = 1 + \\gamma \\cdot 0 = 1$\n",
        "   - For $s_0$: $V(s_0) = 1 + \\gamma \\cdot 1 = 1 + 0.9 \\cdot 1 = 1.9$\n",
        "\n",
        "   After a few iterations, the value functions converge to:\n",
        "\n",
        "   - $V(s_0) = 1.9$\n",
        "   - $V(s_1) = 1$\n",
        "   - $V(s_2) = 0$\n",
        "   - $V(s_3) = 0$\n",
        "\n",
        "This example demonstrates a simple MDP where the agent's actions lead to different rewards, and the goal is to find the optimal value functions and policy.\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "Reinforcement Learning is a powerful framework for learning optimal decision-making strategies through interaction with an environment. By understanding the components of MDPs, value functions, Bellman equations, and key properties of RL algorithms, you can begin to explore more advanced RL algorithms and techniques.\n",
        "\n",
        "This tutorial provides a foundation for studying topics such as Q-Learning, Policy Gradients, and Deep Reinforcement Learning in more depth.\n",
        "\n"
      ],
      "metadata": {
        "id": "OMiHblEpo5sr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbDODh1No5Q1"
      },
      "outputs": [],
      "source": []
    }
  ]
}