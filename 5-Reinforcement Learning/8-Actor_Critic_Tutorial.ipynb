{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Actor-Critic Tutorial\n",
        "\n",
        "The Actor-Critic method is a popular approach in reinforcement learning that combines the benefits of both policy-based and value-based methods. It consists of two main components: the actor, which updates the policy, and the critic, which evaluates the policy by estimating the value function.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "1. **Policy ($\\pi_{\\theta}(a|s)$)**: A mapping from states to actions, parameterized by $\\theta$, representing the probability of taking action $a$ given state $s$.\n",
        "2. **Value Function ($V^{\\pi}(s)$)**: The expected return starting from state $s$ and following policy $\\pi$.\n",
        "3. **Action-Value Function ($Q^{\\pi}(s, a)$)**: The expected return starting from state $s$, taking action $a$, and then following policy $\\pi$.\n",
        "4. **Advantage Function ($A^{\\pi}(s, a)$)**: Measures how much better taking action $a$ in state $s$ is compared to the average action. It is defined as $A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)$.\n",
        "\n",
        "## Mathematical Background\n",
        "\n",
        "### Objective Function\n",
        "\n",
        "The objective in reinforcement learning is to maximize the expected return from the start state $s_0$:\n",
        "\n",
        "$$\n",
        "J(\\pi_{\\theta}) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\right]\n",
        "$$\n",
        "\n",
        "where $\\gamma$ is the discount factor and $r(s_t, a_t)$ is the reward received at time step $t$.\n",
        "\n",
        "### Policy Gradient\n",
        "\n",
        "The policy gradient theorem provides the gradient of the objective function with respect to the policy parameters $\\theta$:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) Q^{\\pi_{\\theta}}(s_t, a_t) \\right]\n",
        "$$\n",
        "\n",
        "In Actor-Critic methods, we use an estimate of the action-value function $Q^{\\pi_{\\theta}}(s_t, a_t)$, typically provided by the critic.\n",
        "\n",
        "### Critic\n",
        "\n",
        "The critic estimates the value function $V^{\\pi_{\\theta}}(s)$, which is used to compute the advantage function. The parameters of the value function, $\\phi$, are updated by minimizing the temporal difference (TD) error:\n",
        "\n",
        "$$\n",
        "\\delta_t = r(s_t, a_t) + \\gamma V_{\\phi}(s_{t+1}) - V_{\\phi}(s_t)\n",
        "$$\n",
        "\n",
        "The loss function for the critic is:\n",
        "\n",
        "$$\n",
        "L(\\phi) = \\mathbb{E}_{(s_t, r_t, s_{t+1}) \\sim \\pi_{\\theta}} \\left[ \\delta_t^2 \\right]\n",
        "$$\n",
        "\n",
        "The gradient of the loss function with respect to $\\phi$ is:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\phi} L(\\phi) = -2 \\mathbb{E}_{(s_t, r_t, s_{t+1}) \\sim \\pi_{\\theta}} \\left[ \\delta_t \\nabla_{\\phi} V_{\\phi}(s_t) \\right]\n",
        "$$\n",
        "\n",
        "### Actor\n",
        "\n",
        "The actor updates the policy parameters $\\theta$ in the direction suggested by the advantage function. The policy gradient with the advantage function is:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) A^{\\pi_{\\theta}}(s_t, a_t) \\right]\n",
        "$$\n",
        "\n",
        "Using the TD error as an estimate of the advantage function:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J(\\pi_{\\theta}) \\approx \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) \\delta_t \\right]\n",
        "$$\n",
        "\n",
        "### Actor-Critic Algorithm\n",
        "\n",
        "The Actor-Critic algorithm iteratively updates the actor and the critic as follows:\n",
        "\n",
        "1. **Initialize**: Initialize policy parameters $\\theta$ and value function parameters $\\phi$.\n",
        "2. **Repeat**:\n",
        "    - Collect trajectory $(s_t, a_t, r_t, s_{t+1})$ by following policy $\\pi_{\\theta}$.\n",
        "    - Compute TD error:\n",
        "      $$\n",
        "      \\delta_t = r(s_t, a_t) + \\gamma V_{\\phi}(s_{t+1}) - V_{\\phi}(s_t)\n",
        "      $$\n",
        "    - Update critic by minimizing the loss function:\n",
        "      $$\n",
        "      \\phi \\leftarrow \\phi - \\alpha_c \\nabla_{\\phi} L(\\phi)\n",
        "      $$\n",
        "    - Update actor by ascending the policy gradient:\n",
        "      $$\n",
        "      \\theta \\leftarrow \\theta + \\alpha_a \\nabla_{\\theta} J(\\pi_{\\theta})\n",
        "      $$\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The Actor-Critic method effectively combines the strengths of policy-based and value-based approaches. The actor updates the policy directly, while the critic provides feedback on the quality of the actions taken, allowing the actor to make more informed updates.\n"
      ],
      "metadata": {
        "id": "oRh2-X1KNTWW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-xagOrEKGbH"
      },
      "outputs": [],
      "source": []
    }
  ]
}