{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning in Reinforcement Learning\n",
        "\n",
        "Q-Learning is a model-free reinforcement learning algorithm used to find the optimal action-selection policy for any given finite Markov Decision Process (MDP). It is an off-policy learner, meaning it learns the value of the optimal policy independently of the agent's actions.\n",
        "\n",
        "## Key Concepts in Q-Learning\n",
        "\n",
        "### Markov Decision Process (MDP)\n",
        "\n",
        "An MDP is defined by a tuple $(S, A, P, R, \\gamma)$, where:\n",
        "- $S$ is a set of states.\n",
        "- $A$ is a set of actions.\n",
        "- $P(s'|s, a)$ is the state transition probability function, representing the probability of moving to state $s'$ from state $s$ after taking action $a$.\n",
        "- $R(s, a)$ is the reward function, which provides the immediate reward received after taking action $a$ in state $s$.\n",
        "- $\\gamma$ is the discount factor, which determines the present value of future rewards.\n",
        "\n",
        "### Q-Learning Algorithm\n",
        "\n",
        "Q-Learning aims to learn the optimal action-value function $Q^*(s, a)$, which gives the maximum expected utility of performing action $a$ in state $s$ and following the optimal policy thereafter.\n",
        "\n",
        "The Q-value update rule is given by:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R(s, a) + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\alpha$ is the learning rate.\n",
        "- $R(s, a)$ is the reward received after taking action $a$ in state $s$.\n",
        "- $\\gamma$ is the discount factor.\n",
        "- $s'$ is the next state after taking action $a$ in state $s$.\n",
        "- $\\max_{a'} Q(s', a')$ is the maximum Q-value for the next state $s'$ over all possible actions $a'$.\n",
        "\n",
        "### Q-Learning Algorithm Steps\n",
        "\n",
        "1. **Initialize Q-values**: Initialize the Q-values arbitrarily for all state-action pairs $(s, a)$, e.g., $Q(s, a) = 0$.\n",
        "\n",
        "2. **Loop**: For each episode:\n",
        "   - Initialize the starting state $s$.\n",
        "   - For each step of the episode:\n",
        "     - Choose an action $a$ based on the current state $s$ using an exploration strategy (e.g., $\\epsilon$-greedy).\n",
        "     - Take the action $a$, observe the reward $r$ and the next state $s'$.\n",
        "     - Update the Q-value using the Q-learning update rule.\n",
        "     - Set the state $s$ to the next state $s'$.\n",
        "\n",
        "3. **Repeat**: Repeat the process until the Q-values converge.\n",
        "\n",
        "## Numerical Example\n",
        "\n",
        "Let's consider a simple MDP with 4 states and 2 actions.\n",
        "\n",
        "- **States**: $S = \\{s_1, s_2, s_3, s_4\\}$\n",
        "- **Actions**: $A = \\{\\text{Left}, \\text{Right}\\}$\n",
        "- **Transition Probabilities**: Deterministic transitions (with 100% probability) based on the action.\n",
        "- **Rewards**: $R(s, a)$ is defined for each state-action pair.\n",
        "- **Discount Factor**: $\\gamma = 0.9$\n",
        "- **Learning Rate**: $\\alpha = 0.1$\n",
        "\n",
        "### MDP Configuration\n",
        "\n",
        "| State  | Action | Next State | Reward |\n",
        "|--------|--------|------------|--------|\n",
        "| $s_1$  | Right  | $s_2$      | 0      |\n",
        "| $s_1$  | Left   | $s_1$      | 0      |\n",
        "| $s_2$  | Right  | $s_3$      | 0      |\n",
        "| $s_2$  | Left   | $s_1$      | 0      |\n",
        "| $s_3$  | Right  | $s_4$      | 1      |\n",
        "| $s_3$  | Left   | $s_2$      | 0      |\n",
        "| $s_4$  | Right  | $s_4$      | 0      |\n",
        "| $s_4$  | Left   | $s_3$      | 0      |\n",
        "\n",
        "### Q-Learning Algorithm Execution\n",
        "\n",
        "1. **Initialize Q-values**:\n",
        "   - $Q(s_1, \\text{Right}) = 0$\n",
        "   - $Q(s_1, \\text{Left}) = 0$\n",
        "   - $Q(s_2, \\text{Right}) = 0$\n",
        "   - $Q(s_2, \\text{Left}) = 0$\n",
        "   - $Q(s_3, \\text{Right}) = 0$\n",
        "   - $Q(s_3, \\text{Left}) = 0$\n",
        "   - $Q(s_4, \\text{Right}) = 0$\n",
        "   - $Q(s_4, \\text{Left}) = 0$\n",
        "\n",
        "2. **Loop**: For each episode, repeat until convergence.\n",
        "\n",
        "   Example: Suppose the agent is in state $s_3$ and chooses the action Right, then:\n",
        "\n",
        "   - $s = s_3$\n",
        "   - $a = \\text{Right}$\n",
        "   - $s' = s_4$\n",
        "   - $r = 1$\n",
        "\n",
        "   Update Q-value:\n",
        "\n",
        "   $$\n",
        "   Q(s_3, \\text{Right}) \\leftarrow Q(s_3, \\text{Right}) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s_4, a') - Q(s_3, \\text{Right}) \\right]\n",
        "   $$\n",
        "\n",
        "   Substitute values:\n",
        "\n",
        "   $$\n",
        "   Q(s_3, \\text{Right}) \\leftarrow 0 + 0.1 \\left[ 1 + 0.9 \\times \\max_{a'} Q(s_4, a') - 0 \\right]\n",
        "   $$\n",
        "\n",
        "   Assuming $\\max_{a'} Q(s_4, a') = 0$ (initially):\n",
        "\n",
        "   $$\n",
        "   Q(s_3, \\text{Right}) \\leftarrow 0.1 \\times (1 + 0 - 0) = 0.1\n",
        "   $$\n",
        "\n",
        "3. **Repeat**: Continue updating Q-values for all state-action pairs until convergence.\n",
        "\n",
        "### Q-Learning Convergence\n",
        "\n",
        "The Q-learning algorithm iteratively updates the Q-values until they converge to the optimal action-value function $Q^*(s, a)$. The optimal policy $\\pi^*$ can then be derived by selecting the action with the highest Q-value in each state:\n",
        "\n",
        "$$\n",
        "\\pi^*(s) = \\text{argmax}_a Q^*(s, a)\n",
        "$$\n",
        "\n",
        "### Example Convergence\n",
        "\n",
        "Suppose after multiple episodes, the Q-values converge to:\n",
        "\n",
        "- $Q(s_1, \\text{Right}) = 0.243$\n",
        "- $Q(s_1, \\text{Left}) = 0$\n",
        "- $Q(s_2, \\text{Right}) = 0.27$\n",
        "- $Q(s_2, \\text{Left}) = 0$\n",
        "- $Q(s_3, \\text{Right}) = 1$\n",
        "- $Q(s_3, \\text{Left}) = 0.243$\n",
        "- $Q(s_4, \\text{Right}) = 0$\n",
        "- $Q(s_4, \\text{Left}) = 0.9$\n",
        "\n",
        "The optimal policy $\\pi^*$ can be derived as:\n",
        "\n",
        "- $\\pi^*(s_1) = \\text{Right}$\n",
        "- $\\pi^*(s_2) = \\text{Right}$\n",
        "- $\\pi^*(s_3) = \\text{Right}$\n",
        "- $\\pi^*(s_4) = \\text{Left}$\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Q-Learning is a powerful model-free reinforcement learning algorithm that can learn the optimal policy for any finite MDP. It does not require a model of the environment and can handle stochastic transitions and rewards. By iteratively updating Q-values based on the agent's experience, Q-Learning converges to the optimal action-value function, enabling the agent to make optimal decisions in the environment.\n",
        "\n",
        "## References\n",
        "\n",
        "- **[Watkins, C.J.C.H., Dayan, P. (1992)]** Q-learning. Machine Learning, 8, 279-292.\n",
        "- **[Sutton, R.S., Barto, A.G. (1998)]** Reinforcement Learning: An Introduction. MIT Press.\n"
      ],
      "metadata": {
        "id": "yigU4tUDkfvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Properties of Q-Learning\n",
        "\n",
        "- **Model-Free**: Q-Learning does not require a model of the environment, making it suitable for environments where the transition probabilities and rewards are unknown.\n",
        "- **Off-Policy**: Q-Learning learns the value of the optimal policy independently of the agent's actions. It can learn from actions that are outside the current policy.\n",
        "- **Exploration vs. Exploitation**: The algorithm balances exploration (trying new actions to discover their effects) and exploitation (using known actions that yield high rewards) through strategies such as $\\epsilon$-greedy.\n",
        "- **Convergence**: Q-Learning is guaranteed to converge to the optimal action-value function $Q^*(s, a)$ under certain conditions, such as sufficiently large exploration and decaying learning rate $\\alpha$.\n",
        "\n",
        "# Important Notes on Using Q-Learning\n",
        "\n",
        "- **Learning Rate $\\alpha$**: The learning rate should decrease over time but remain positive to ensure convergence.\n",
        "- **Discount Factor $\\gamma$**: The discount factor $\\gamma$ should be chosen carefully, as it determines the importance of future rewards.\n",
        "- **Exploration Strategy**: Effective exploration strategies, such as $\\epsilon$-greedy, are crucial to ensure that the agent explores the state-action space sufficiently.\n",
        "- **Stability**: Q-Learning may require a large number of episodes to converge, especially in environments with large state-action spaces.\n",
        "\n",
        "# Advantages of Q-Learning\n",
        "\n",
        "- **Simplicity**: Q-Learning is conceptually simple and easy to implement.\n",
        "- **Model-Free**: It does not require knowledge of the environment's transition probabilities and rewards.\n",
        "- **Off-Policy**: Q-Learning can learn optimal policies while exploring suboptimal actions.\n",
        "- **Flexibility**: It can be applied to a wide range of environments, including stochastic and deterministic settings.\n",
        "- **Online Learning**: The agent learns and improves its policy continuously while interacting with the environment.\n",
        "\n",
        "# Disadvantages of Q-Learning\n",
        "\n",
        "- **Scalability**: Q-Learning can struggle with large state-action spaces due to the need to store and update a Q-value for each state-action pair.\n",
        "- **Convergence Time**: It can take a long time to converge to the optimal policy, especially in complex environments.\n",
        "- **Exploration Challenges**: Balancing exploration and exploitation effectively can be challenging.\n",
        "- **Function Approximation**: In environments with continuous state or action spaces, Q-Learning requires function approximation techniques, which can introduce additional complexity and instability.\n",
        "- **Sensitivity to Hyperparameters**: The performance of Q-Learning can be sensitive to the choice of hyperparameters such as the learning rate and discount factor.\n",
        "\n"
      ],
      "metadata": {
        "id": "ANDv1KDbksNN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZoz7aNNkeMT"
      },
      "outputs": [],
      "source": []
    }
  ]
}