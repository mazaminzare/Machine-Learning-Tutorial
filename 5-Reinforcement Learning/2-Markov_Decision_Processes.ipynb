{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Markov Decision Processes (MDPs) in Reinforcement Learning\n",
        "\n",
        "A Markov Decision Process (MDP) provides a mathematical framework for modeling decision-making in environments where outcomes are partly random and partly under the control of a decision maker. MDPs are used to model the environment in Reinforcement Learning.\n",
        "\n",
        "## Key Concepts in Markov Decision Processes\n",
        "\n",
        "### Definition of an MDP\n",
        "\n",
        "An MDP is defined by a tuple $(S, A, P, R, \\gamma)$, where:\n",
        "- $S$ is a finite set of states.\n",
        "- $A$ is a finite set of actions.\n",
        "- $P(s'|s, a)$ is the state transition probability function, representing the probability of moving to state $s'$ from state $s$ after taking action $a$.\n",
        "- $R(s, a)$ is the reward function, which gives the immediate reward received after taking action $a$ in state $s$.\n",
        "- $\\gamma \\in [0, 1]$ is the discount factor, which determines the present value of future rewards.\n",
        "\n",
        "### State Transition Probability\n",
        "\n",
        "The state transition probability function $P(s'|s, a)$ captures the dynamics of the environment. It is the probability of transitioning to state $s'$ given that the current state is $s$ and the action taken is $a$.\n",
        "\n",
        "### Reward Function\n",
        "\n",
        "The reward function $R(s, a)$ provides the immediate reward received after taking action $a$ in state $s$. The reward signal guides the agent to learn the optimal policy.\n",
        "\n",
        "### Discount Factor\n",
        "\n",
        "The discount factor $\\gamma$ is used to weigh future rewards. It lies between 0 and 1, where a value closer to 0 makes the agent myopic by only considering immediate rewards, while a value closer to 1 makes the agent strive for long-term rewards.\n",
        "\n",
        "### Policy\n",
        "\n",
        "A policy $\\pi(a|s)$ is a mapping from states to probabilities of selecting each possible action. The goal in an MDP is to find an optimal policy $\\pi^*$ that maximizes the expected return from each state.\n",
        "\n",
        "### Value Functions\n",
        "\n",
        "Value functions estimate the expected return (cumulative reward) from a given state or state-action pair under a policy.\n",
        "\n",
        "- **State-Value Function** $V^\\pi(s)$: The expected return starting from state $s$ and following policy $\\pi$.\n",
        "\n",
        "  $$\n",
        "  V^\\pi(s) = \\mathbb{E}^\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s \\right]\n",
        "  $$\n",
        "\n",
        "- **Action-Value Function** $Q^\\pi(s, a)$: The expected return starting from state $s$, taking action $a$, and following policy $\\pi$ thereafter.\n",
        "\n",
        "  $$\n",
        "  Q^\\pi(s, a) = \\mathbb{E}^\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s, a_0 = a \\right]\n",
        "  $$\n",
        "\n",
        "### Bellman Equations\n",
        "\n",
        "The Bellman equations provide a recursive decomposition for the value functions.\n",
        "\n",
        "- **Bellman Expectation Equation for $V^\\pi(s)$**:\n",
        "\n",
        "  $$\n",
        "  V^\\pi(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma V^\\pi(s') \\right]\n",
        "  $$\n",
        "\n",
        "- **Bellman Expectation Equation for $Q^\\pi(s, a)$**:\n",
        "\n",
        "  $$\n",
        "  Q^\\pi(s, a) = \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma \\sum_{a' \\in A} \\pi(a'|s') Q^\\pi(s', a') \\right]\n",
        "  $$\n",
        "\n",
        "### Optimal Value Functions\n",
        "\n",
        "The optimal value functions $V^*(s)$ and $Q^*(s, a)$ provide the maximum expected return achievable from any state or state-action pair, respectively.\n",
        "\n",
        "- **Bellman Optimality Equation for $V^*(s)$**:\n",
        "\n",
        "  $$\n",
        "  V^*(s) = \\max_{a \\in A} \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma V^*(s') \\right]\n",
        "  $$\n",
        "\n",
        "- **Bellman Optimality Equation for $Q^*(s, a)$**:\n",
        "\n",
        "  $$\n",
        "  Q^*(s, a) = \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma \\max_{a' \\in A} Q^*(s', a') \\right]\n",
        "  $$\n",
        "\n",
        "### Finding the Optimal Policy\n",
        "\n",
        "The optimal policy $\\pi^*$ can be derived from the optimal value functions.\n",
        "\n",
        "- From $V^*(s)$:\n",
        "\n",
        "  $$\n",
        "  \\pi^*(s) = \\text{argmax}_{a \\in A} \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma V^*(s') \\right]\n",
        "  $$\n",
        "\n",
        "- From $Q^*(s, a)$:\n",
        "\n",
        "  $$\n",
        "  \\pi^*(s) = \\text{argmax}_{a \\in A} Q^*(s, a)\n",
        "  $$\n",
        "\n",
        "## Numerical Example\n",
        "\n",
        "Consider a simple MDP with 3 states and 2 actions.\n",
        "\n",
        "- **States**: $S = \\{s_1, s_2, s_3\\}$\n",
        "- **Actions**: $A = \\{\\text{a1}, \\text{a2}\\}$\n",
        "- **Transition Probabilities** and **Rewards**:\n",
        "\n",
        "  | State | Action | Next State | Probability | Reward |\n",
        "  |-------|--------|------------|-------------|--------|\n",
        "  | $s_1$ | $a_1$  | $s_2$      | 0.5         | 5      |\n",
        "  | $s_1$ | $a_1$  | $s_3$      | 0.5         | 10     |\n",
        "  | $s_1$ | $a_2$  | $s_2$      | 1.0         | 0      |\n",
        "  | $s_2$ | $a_1$  | $s_1$      | 1.0         | -1     |\n",
        "  | $s_2$ | $a_2$  | $s_3$      | 1.0         | 2      |\n",
        "  | $s_3$ | $a_1$  | $s_1$      | 1.0         | 0      |\n",
        "  | $s_3$ | $a_2$  | $s_3$      | 1.0         | 1      |\n",
        "\n",
        "- **Discount Factor**: $\\gamma = 0.9$\n",
        "\n",
        "### Value Iteration Example\n",
        "\n",
        "1. **Initialize $V(s)$**: Initialize $V(s) = 0$ for all $s \\in S$.\n",
        "\n",
        "2. **Value Iteration Update**:\n",
        "\n",
        "   Update $V(s)$ using the Bellman Optimality Equation for $V^*(s)$ until convergence:\n",
        "\n",
        "   $$\n",
        "   V(s) \\leftarrow \\max_{a \\in A} \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma V(s') \\right]\n",
        "   $$\n",
        "\n",
        "   For state $s_1$:\n",
        "\n",
        "   $$\n",
        "   V(s_1) = \\max \\left\\{ 0.5 \\left[ 5 + 0.9 V(s_2) \\right] + 0.5 \\left[ 10 + 0.9 V(s_3) \\right], \\left[ 0 + 0.9 V(s_2) \\right] \\right\\}\n",
        "   $$\n",
        "\n",
        "   For state $s_2$:\n",
        "\n",
        "   $$\n",
        "   V(s_2) = \\max \\left\\{ -1 + 0.9 V(s_1), 2 + 0.9 V(s_3) \\right\\}\n",
        "   $$\n",
        "\n",
        "   For state $s_3$:\n",
        "\n",
        "   $$\n",
        "   V(s_3) = \\max \\left\\{ 0 + 0.9 V(s_1), 1 + 0.9 V(s_3) \\right\\}\n",
        "   $$\n",
        "\n",
        "   Repeat the updates until $V(s)$ converges.\n",
        "\n",
        "### Optimal Policy Derivation\n",
        "\n",
        "Once $V(s)$ converges to $V^*(s)$, derive the optimal policy $\\pi^*$:\n",
        "\n",
        "For each state $s$, choose the action that maximizes the expected return:\n",
        "\n",
        "$$\n",
        "\\pi^*(s) = \\text{argmax}_{a \\in A} \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma V^*(s') \\right]\n",
        "$$\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "MDPs provide a fundamental framework for modeling decision-making processes in uncertain environments. By defining states, actions, transition probabilities, rewards, and discount factors, MDPs enable the formulation and solution of complex decision problems. Value iteration and policy iteration are key algorithms used to compute optimal policies in MDPs, forming the basis for many reinforcement learning methods.\n",
        "\n",
        "## References\n",
        "\n",
        "- **[Puterman, M.L. (1994)]** Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons.\n",
        "- **[Sutton, R.S., Barto, A.G. (1998)]** Reinforcement Learning: An Introduction. MIT Press.\n"
      ],
      "metadata": {
        "id": "mda9xsablXkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Properties of Markov Decision Processes\n",
        "\n",
        "1. **Markov Property**: The future state depends only on the current state and action, not on the sequence of events that preceded it. This is also known as the memoryless property.\n",
        "   \n",
        "   $$\n",
        "   P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_1, a_1, s_2, a_2, \\ldots, s_t, a_t)\n",
        "   $$\n",
        "\n",
        "2. **Stationarity**: The transition probabilities and rewards are stationary over time. That is, they do not change with time.\n",
        "\n",
        "3. **Finite State and Action Spaces**: MDPs typically assume a finite set of states $S$ and actions $A$, making them suitable for discrete decision-making problems.\n",
        "\n",
        "4. **Policy**: A policy $\\pi$ defines the agent's way of behaving at a given time. It can be deterministic or stochastic.\n",
        "   - **Deterministic Policy**: Maps each state to a specific action.\n",
        "   - **Stochastic Policy**: Maps each state to a probability distribution over actions.\n",
        "\n",
        "## Important Notes on Using MDPs\n",
        "\n",
        "- **Model Requirements**: MDPs require a complete model of the environment, including the state transition probabilities and reward functions.\n",
        "- **Computational Complexity**: Solving MDPs exactly using methods like Value Iteration or Policy Iteration can be computationally expensive, especially for large state or action spaces.\n",
        "- **Bellman Equations**: The Bellman equations are central to understanding and solving MDPs, providing a recursive decomposition for value functions.\n",
        "- **Optimality**: The goal is to find the optimal policy $\\pi^*$ that maximizes the expected cumulative reward from any given state.\n",
        "\n",
        "## Advantages of Using MDPs\n",
        "\n",
        "1. **Framework**: Provides a comprehensive mathematical framework for modeling sequential decision-making problems.\n",
        "2. **Optimal Solutions**: Can compute optimal policies that maximize expected rewards, providing a benchmark for performance.\n",
        "3. **Theoretical Foundation**: Strong theoretical foundation, leveraging concepts from probability theory and dynamic programming.\n",
        "4. **Versatility**: Applicable to a wide range of problems in fields such as robotics, operations research, economics, and artificial intelligence.\n",
        "\n",
        "## Disadvantages of Using MDPs\n",
        "\n",
        "1. **Model Dependency**: Requires a known model of the environment, which may not always be available in practice.\n",
        "2. **Scalability**: Solving MDPs for large state or action spaces can be computationally prohibitive, leading to the \"curse of dimensionality.\"\n",
        "3. **Stationarity Assumption**: Assumes stationary transition probabilities and rewards, which may not hold in dynamic environments.\n",
        "4. **Memoryless Property**: The Markov property may be too restrictive for some real-world problems where history or long-term dependencies matter.\n",
        "\n",
        "MDPs provide a powerful and rigorous framework for tackling decision-making problems under uncertainty. However, their practical application is often limited by the need for a complete model and the computational challenges associated with large-scale problems.\n"
      ],
      "metadata": {
        "id": "2b6BKabnmjp1"
      }
    }
  ]
}