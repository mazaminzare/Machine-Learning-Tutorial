{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Programming in Reinforcement Learning\n",
        "\n",
        "Dynamic Programming (DP) refers to a collection of algorithms for solving complex problems by breaking them down into simpler subproblems. In Reinforcement Learning (RL), DP methods are used to find optimal policies for Markov Decision Processes (MDPs) given a complete model of the environment.\n",
        "\n",
        "## Key Concepts in Dynamic Programming\n",
        "\n",
        "### Markov Decision Process (MDP)\n",
        "\n",
        "An MDP is defined by a tuple $(S, A, P, R, \\gamma)$, where:\n",
        "- $S$ is a set of states.\n",
        "- $A$ is a set of actions.\n",
        "- $P(s'|s, a)$ is the state transition probability function, representing the probability of moving to state $s'$ from state $s$ after taking action $a$.\n",
        "- $R(s, a)$ is the reward function, which provides the immediate reward received after taking action $a$ in state $s$.\n",
        "- $\\gamma$ is the discount factor, which determines the present value of future rewards.\n",
        "\n",
        "### Dynamic Programming Algorithms\n",
        "\n",
        "DP algorithms assume full knowledge of the MDP model and can be used to compute the optimal policy and value functions. The two main DP algorithms for solving MDPs are **Policy Evaluation** and **Policy Improvement**.\n",
        "\n",
        "#### 1. Policy Evaluation\n",
        "\n",
        "Policy Evaluation computes the state-value function $V^\\pi(s)$ for a given policy $\\pi$. It uses the Bellman expectation equation for the value function:\n",
        "\n",
        "$$\n",
        "V^\\pi(s) = \\mathbb{E}^\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s \\right]\n",
        "$$\n",
        "\n",
        "The Bellman Expectation Equation for $V^\\pi(s)$ is:\n",
        "\n",
        "$$\n",
        "V^\\pi(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma V^\\pi(s') \\right]\n",
        "$$\n",
        "\n",
        "#### 2. Policy Improvement\n",
        "\n",
        "Policy Improvement updates the policy $\\pi$ to improve it based on the value function computed in the Policy Evaluation step. The improved policy is defined by:\n",
        "\n",
        "$$\n",
        "\\pi'(s) = \\text{argmax}_{a \\in A} \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma V^\\pi(s') \\right]\n",
        "$$\n",
        "\n",
        "The policy improvement process is repeated until the policy converges to the optimal policy.\n",
        "\n",
        "#### 3. Policy Iteration\n",
        "\n",
        "Policy Iteration is a method that alternates between Policy Evaluation and Policy Improvement until the policy converges to the optimal policy:\n",
        "\n",
        "1. **Policy Evaluation**: Compute $V^\\pi(s)$ for the current policy $\\pi$.\n",
        "2. **Policy Improvement**: Update the policy $\\pi$ using the computed value function.\n",
        "\n",
        "#### 4. Value Iteration\n",
        "\n",
        "Value Iteration is an alternative to Policy Iteration that combines Policy Evaluation and Policy Improvement into a single step:\n",
        "\n",
        "$$\n",
        "V(s) \\leftarrow \\max_{a \\in A} \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma V(s') \\right]\n",
        "$$\n",
        "\n",
        "Here, the value function $V(s)$ is updated directly to approach the optimal value function.\n",
        "\n",
        "## Numerical Example\n",
        "\n",
        "Let's solve a simple MDP using Dynamic Programming. Consider a grid world with the following configuration:\n",
        "\n",
        "- **States**: $S = \\{s_1, s_2, s_3, s_4\\}$\n",
        "- **Actions**: $A = \\{\\text{Up}, \\text{Down}, \\text{Left}, \\text{Right}\\}$\n",
        "- **Transition Probabilities**: Deterministic transitions (with 100% probability) based on the action.\n",
        "- **Rewards**: $R(s, a) = -1$ for each move.\n",
        "- **Discount Factor**: $\\gamma = 0.9$\n",
        "\n",
        "### MDP Configuration\n",
        "\n",
        "| State  | Action | Next State | Reward |\n",
        "|--------|--------|------------|--------|\n",
        "| $s_1$   | Down   | $s_2$       | -1     |\n",
        "| $s_2$   | Up     | $s_1$       | -1     |\n",
        "| $s_2$   | Right  | $s_3$       | -1     |\n",
        "| $s_3$   | Left   | $s_2$       | -1     |\n",
        "| $s_3$   | Down   | $s_4$       | -1     |\n",
        "| $s_4$   | Up     | $s_3$       | -1     |\n",
        "\n",
        "### Policy Iteration Algorithm\n",
        "\n",
        "1. **Initialize Policy**: Let's start with a random policy, e.g., always move Right.\n",
        "\n",
        "2. **Policy Evaluation**:\n",
        "\n",
        "   Compute $V^\\pi(s)$ for the initial policy:\n",
        "\n",
        "   For $s_1$:\n",
        "   $$\n",
        "   V^\\pi(s_1) = -1 + \\gamma V^\\pi(s_2)\n",
        "   $$\n",
        "\n",
        "   For $s_2$:\n",
        "   $$\n",
        "   V^\\pi(s_2) = -1 + \\gamma \\left[ 0.5 V^\\pi(s_1) + 0.5 V^\\pi(s_3) \\right]\n",
        "   $$\n",
        "\n",
        "   For $s_3$:\n",
        "   $$\n",
        "   V^\\pi(s_3) = -1 + \\gamma \\left[ 0.5 V^\\pi(s_2) + 0.5 V^\\pi(s_4) \\right]\n",
        "   $$\n",
        "\n",
        "   For $s_4$:\n",
        "   $$\n",
        "   V^\\pi(s_4) = -1 + \\gamma V^\\pi(s_3)\n",
        "   $$\n",
        "\n",
        "   Solving these equations yields:\n",
        "\n",
        "   $$\n",
        "   V^\\pi(s_1) = -1 + 0.9 V^\\pi(s_2)\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   V^\\pi(s_2) = -1 + 0.9 \\left[ 0.5 V^\\pi(s_1) + 0.5 V^\\pi(s_3) \\right]\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   V^\\pi(s_3) = -1 + 0.9 \\left[ 0.5 V^\\pi(s_2) + 0.5 V^\\pi(s_4) \\right]\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   V^\\pi(s_4) = -1 + 0.9 V^\\pi(s_3)\n",
        "   $$\n",
        "\n",
        "   Solving these equations iteratively will yield approximate values for $V^\\pi(s_1)$, $V^\\pi(s_2)$, $V^\\pi(s_3)$, and $V^\\pi(s_4)$.\n",
        "\n",
        "3. **Policy Improvement**:\n",
        "\n",
        "   Improve the policy by choosing actions that maximize the expected return based on the evaluated $V^\\pi(s)$.\n",
        "\n",
        "   For example, update the policy to always move from $s_1$ to $s_2$, from $s_2$ to $s_3$, and so on.\n",
        "\n",
        "4. **Repeat**:\n",
        "\n",
        "   Repeat the Policy Evaluation and Policy Improvement steps until the policy converges.\n",
        "\n",
        "### Value Iteration Algorithm\n",
        "\n",
        "1. **Initialize Value Function**: Initialize $V(s)$ arbitrarily, e.g., $V(s) = 0$ for all states.\n",
        "\n",
        "2. **Update Value Function**:\n",
        "\n",
        "   Update $V(s)$ using the Bellman Optimality Equation:\n",
        "\n",
        "   $$\n",
        "   V(s) \\leftarrow \\max_{a \\in A} \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma V(s') \\right]\n",
        "   $$\n",
        "\n",
        "   For each state $s$:\n",
        "   - Compute the value for each action.\n",
        "   - Update $V(s)$ to be the maximum value of these actions.\n",
        "\n",
        "3. **Derive Policy**:\n",
        "\n",
        "   Once the value function converges, derive the policy by choosing the action that maximizes the expected return.\n",
        "\n",
        "   $$\n",
        "   \\pi(s) = \\text{argmax}_{a \\in A} \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma V(s') \\right]\n",
        "   $$\n",
        "\n",
        "### Example Numerical Computation\n",
        "\n",
        "Consider the grid world example with a discount factor $\\gamma = 0.9$ and the following rewards and transitions:\n",
        "\n",
        "| State | Action | Next State | Reward |\n",
        "|-------|--------|------------|--------|\n",
        "| $s_1$  | Down   | $s_2$      | -1     |\n",
        "| $s_2$  | Right  | $s_3$      | -1     |\n",
        "| $s_3$  | Down   | $s_4$      | -1     |\n",
        "| $s_4$  | Up     | $s_3$      | -1     |\n",
        "\n",
        "Assuming the policy is to move \"Right\" wherever possible, the value functions can be updated as follows:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - $V(s_1) = 0$\n",
        "   - $V(s_2) = 0$\n",
        "   - $V(s_3) = 0$\n",
        "   - $V(s_4) = 0$\n",
        "\n",
        "2. **Value Iteration**:\n",
        "\n",
        "   - Update $V(s_4)$:\n",
        "     $$\n",
        "     V(s_4) = -1 + 0.9 V(s_3) \\approx -1 + 0.9 \\times 0 = -1\n",
        "     $$\n",
        "\n",
        "   - Update $V(s_3)$:\n",
        "     $$\n",
        "     V(s_3) = -1 + 0.9 V(s_4) \\approx -1 + 0.9 \\times (-1) = -1.9\n",
        "     $$\n",
        "\n",
        "   - Update $V(s_2)$:\n",
        "     $$\n",
        "     V(s_2) = -1 + 0.9 \\left[ V(s_3) \\right] \\approx -1 + 0.9 \\times (-1.9) = -2.71\n",
        "     $$\n",
        "\n",
        "   - Update $V(s_1)$:\n",
        "     $$\n",
        "     V(s_1) = -1 + 0.9 V(s_2) \\approx -1 + 0.9 \\times (-2.71) = -3.959\n",
        "     $$\n",
        "\n",
        "   Continue updating until convergence.\n",
        "\n",
        "3. **Derive Policy**:\n",
        "   - For each state, choose the action that maximizes the expected value.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Dynamic Programming provides a robust framework for solving MDPs by leveraging the complete model of the environment. It involves methods such as Policy Iteration and Value Iteration to compute optimal policies and value functions. While DP methods are powerful, they are generally computationally expensive and require a complete model of the environment, which is often not available in practice.\n",
        "\n",
        "In practice, RL methods such as Q-Learning and Policy Gradient methods are used when a model of the environment is not available, as they can learn optimal policies through interaction with the environment.\n",
        "\n",
        "Understanding DP methods is fundamental for grasping more advanced RL techniques and for theoretical analysis of RL algorithms.\n",
        "\n",
        "## References\n",
        "\n",
        "- **[Sutton, R.S., Barto, A.G. (1998)]** Reinforcement Learning: An Introduction. MIT Press.\n",
        "- **[Bellman, R. (1957)]** Dynamic Programming. Princeton University Press.\n"
      ],
      "metadata": {
        "id": "dTzLKGiG2ZGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Properties, Important Notes, Advantages, and Disadvantages of Dynamic Programming in Reinforcement Learning\n",
        "\n",
        "## Key Properties of Dynamic Programming\n",
        "\n",
        "Dynamic Programming (DP) in Reinforcement Learning has several key properties that make it a powerful approach for solving Markov Decision Processes (MDPs):\n",
        "\n",
        "### 1. **Complete Knowledge of the Environment**\n",
        "\n",
        "DP algorithms require a complete model of the environment, including:\n",
        "- **Transition Probabilities** $P(s'|s, a)$: The probability of transitioning to state $s'$ from state $s$ after taking action $a$.\n",
        "- **Reward Function** $R(s, a)$: The immediate reward received after taking action $a$ in state $s$.\n",
        "\n",
        "### 2. **Optimal Policy and Value Functions**\n",
        "\n",
        "DP methods aim to find the optimal policy $\\pi^*$ that maximizes the expected return. They provide:\n",
        "- **Value Function** $V^\\pi(s)$ for a given policy $\\pi$, and\n",
        "- **Action-Value Function** $Q^\\pi(s, a)$ for a given policy $\\pi$.\n",
        "\n",
        "### 3. **Bellman Equations**\n",
        "\n",
        "DP uses the Bellman equations to express relationships between the value functions:\n",
        "- **Bellman Expectation Equation for Policy Evaluation**:\n",
        "  $$\n",
        "  V^\\pi(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma V^\\pi(s') \\right]\n",
        "  $$\n",
        "\n",
        "- **Bellman Optimality Equation for Value Iteration**:\n",
        "  $$\n",
        "  V(s) = \\max_{a \\in A} \\sum_{s' \\in S} P(s'|s, a) \\left[ R(s, a) + \\gamma V(s') \\right]\n",
        "  $$\n",
        "\n",
        "### 4. **Policy Iteration and Value Iteration**\n",
        "\n",
        "DP algorithms can be broadly categorized into:\n",
        "- **Policy Iteration**: Alternates between Policy Evaluation and Policy Improvement.\n",
        "- **Value Iteration**: Combines Policy Evaluation and Policy Improvement into a single step.\n",
        "\n",
        "## Important Notes on Using Dynamic Programming\n",
        "\n",
        "### 1. **Computational Complexity**\n",
        "\n",
        "DP methods can be computationally expensive due to:\n",
        "- The need to compute and store value functions for all states.\n",
        "- The need to solve large systems of linear equations in Policy Evaluation.\n",
        "\n",
        "### 2. **Assumptions**\n",
        "\n",
        "DP assumes that the model of the environment is known, which includes:\n",
        "- **Transition Dynamics**: Detailed knowledge of $P(s'|s, a)$.\n",
        "- **Rewards**: Detailed knowledge of $R(s, a)$.\n",
        "\n",
        "### 3. **Applicability**\n",
        "\n",
        "DP methods are best suited for problems where the model is available, which is often not the case in real-world scenarios. For real-world applications, model-free methods are more commonly used.\n",
        "\n",
        "## Advantages of Dynamic Programming\n",
        "\n",
        "### 1. **Optimal Solutions**\n",
        "\n",
        "DP methods guarantee finding the optimal policy and value functions given a complete model of the environment.\n",
        "\n",
        "### 2. **Theoretical Foundation**\n",
        "\n",
        "DP provides a strong theoretical foundation for Reinforcement Learning algorithms and is a basis for many advanced methods.\n",
        "\n",
        "### 3. **Structured Algorithms**\n",
        "\n",
        "The algorithms (Policy Iteration, Value Iteration) are well-structured and have clear convergence properties.\n",
        "\n",
        "## Disadvantages of Dynamic Programming\n",
        "\n",
        "### 1. **Model Dependence**\n",
        "\n",
        "DP methods require a complete and accurate model of the environment, which is often not available.\n",
        "\n",
        "### 2. **Scalability Issues**\n",
        "\n",
        "The state and action spaces can grow exponentially, leading to high computational costs for large-scale problems.\n",
        "\n",
        "### 3. **Computational Complexity**\n",
        "\n",
        "Computing the value functions and policies can be computationally intensive, especially for large state and action spaces.\n",
        "\n",
        "## Summary Table\n",
        "\n",
        "| Property/Aspect      | Description                                                                                       |\n",
        "|---------------------|---------------------------------------------------------------------------------------------------|\n",
        "| **Complete Knowledge** | Requires detailed knowledge of transition probabilities and rewards.                             |\n",
        "| **Optimal Solutions** | Guarantees finding the optimal policy and value functions given a complete model.                 |\n",
        "| **Computational Complexity** | Can be computationally expensive due to the need to compute and store value functions for all states. |\n",
        "| **Applicability**   | Best suited for problems with a known model; less practical for real-world applications.        |\n",
        "| **Advantages**      | Optimal solutions, strong theoretical foundation, structured algorithms.                         |\n",
        "| **Disadvantages**   | Model dependence, scalability issues, high computational complexity.                               |\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Dynamic Programming provides a powerful framework for solving MDPs under the assumption that a complete model of the environment is\n"
      ],
      "metadata": {
        "id": "Qn9FJYTahWG9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HChgP9yi2YgY"
      },
      "outputs": [],
      "source": []
    }
  ]
}