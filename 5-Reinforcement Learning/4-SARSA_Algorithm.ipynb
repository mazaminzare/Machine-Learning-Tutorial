{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SARSA Algorithm: A Comprehensive Tutorial\n",
        "\n",
        "## Introduction\n",
        "\n",
        "SARSA (State-Action-Reward-State-Action) is an on-policy reinforcement learning algorithm used to learn the optimal policy in Markov Decision Processes (MDPs). The name SARSA comes from the quintuple of elements that are used to update the Q-values: the current state, the current action, the reward, the next state, and the next action.\n",
        "\n",
        "## Mathematical Background\n",
        "\n",
        "### Markov Decision Process (MDP)\n",
        "\n",
        "An MDP is defined by:\n",
        "- **S**: A set of states.\n",
        "- **A**: A set of actions.\n",
        "- **P**: State transition probabilities, where $P(s' \\mid s, a)$ represents the probability of transitioning to state $s'$ from state $s$ after taking action $a$.\n",
        "- **R**: Reward function, where $R(s, a)$ is the expected reward received after taking action $a$ in state $s$.\n",
        "- **$\\gamma$**: Discount factor, $0 \\leq \\gamma < 1$, which represents the importance of future rewards.\n",
        "\n",
        "### Q-Function\n",
        "\n",
        "The Q-function, or action-value function, $Q(s, a)$, represents the expected cumulative reward of taking action $a$ in state $s$ and then following a policy $\\pi$:\n",
        "\n",
        "$$\n",
        "Q^\\pi(s, a) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\bigg| s_0 = s, a_0 = a, \\pi \\right]\n",
        "$$\n",
        "\n",
        "### SARSA Update Rule\n",
        "\n",
        "SARSA is an on-policy method, meaning it updates the Q-values using the action taken by the policy itself. The update rule is:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma Q(s', a') - Q(s, a) \\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $s$ is the current state.\n",
        "- $a$ is the current action.\n",
        "- $r$ is the reward received after taking action $a$.\n",
        "- $s'$ is the next state.\n",
        "- $a'$ is the next action chosen by the policy.\n",
        "- $\\alpha$ is the learning rate.\n",
        "- $\\gamma$ is the discount factor.\n",
        "\n",
        "This equation is derived from the Bellman equation for Q-values:\n",
        "\n",
        "$$\n",
        "Q(s, a) = \\mathbb{E} \\left[ R(s, a) + \\gamma Q(s', a') \\right]\n",
        "$$\n",
        "\n",
        "### Algorithm Steps\n",
        "\n",
        "1. **Initialize Q-values**:\n",
        "   - Initialize the Q-values arbitrarily for all state-action pairs. Set $Q(s, a)$ to 0 for all state-action pairs $(s, a)$.\n",
        "\n",
        "2. **Policy Selection**:\n",
        "   - Choose an action $a$ for the current state $s$ using an epsilon-greedy policy based on Q-values.\n",
        "\n",
        "3. **Action Execution and Reward Observation**:\n",
        "   - Execute the action $a$, observe the reward $r$, and the next state $s'$.\n",
        "\n",
        "4. **Next Action Selection**:\n",
        "   - Choose the next action $a'$ in the next state $s'$ using the same policy (epsilon-greedy).\n",
        "\n",
        "5. **Q-value Update**:\n",
        "   - Update the Q-value using the SARSA update rule.\n",
        "\n",
        "6. **Transition to the Next State**:\n",
        "   - Set $s \\leftarrow s'$ and $a \\leftarrow a'$.\n",
        "\n",
        "7. **Repeat**:\n",
        "   - Repeat steps 3 to 6 until the terminal state is reached.\n",
        "\n",
        "# Advantages and Drawbacks\n",
        "\n",
        "## Advantages\n",
        "### On-Policy Learning\n",
        "- **Stable Learning**: SARSA is an on-policy algorithm, meaning it evaluates and improves the policy that it follows. This can lead to more stable learning in environments where the optimal policy might involve risk-taking.\n",
        "\n",
        "### Convergence\n",
        "- **Optimal Q-values**: SARSA converges to the optimal Q-values under the right conditions (sufficient exploration, decaying learning rate, and discount factor).\n",
        "\n",
        "### Policy-Aware\n",
        "- **Policy-Based Updates**: Since SARSA updates are based on the actions actually taken by the policy, it can be more suitable for scenarios where following the learned policy is crucial.\n",
        "\n",
        "## Drawbacks\n",
        "### Exploration Dependency\n",
        "- **Performance Dependence**: SARSA’s performance heavily depends on the exploration strategy. An inappropriate exploration policy can lead to suboptimal performance.\n",
        "\n",
        "### Slower Convergence\n",
        "- **Convergence Speed**: SARSA might converge more slowly compared to off-policy methods like Q-Learning because it takes into account the action taken by the current policy, which may not always be the optimal action.\n",
        "\n",
        "### Policy Sensitivity\n",
        "- **Update Sensitivity**: SARSA’s updates are based on the current policy, making it more sensitive to the policy's nature, which can sometimes result in learning suboptimal policies if the policy isn't properly managed.\n",
        "\n",
        "## Key Innovations\n",
        "### On-Policy Nature\n",
        "- **Policy-Based Q-Value Updates**: The key innovation of SARSA is its on-policy nature, which means it updates its Q-values based on the actions actually taken by the policy. This makes SARSA suitable for learning policies that account for the exploration strategy used during training.\n",
        "\n",
        "### Balance Between Exploration and Exploitation\n",
        "- **Epsilon-Greedy Policy**: SARSA inherently balances exploration and exploitation through its epsilon-greedy policy. This helps ensure that the agent explores the environment sufficiently while still exploiting known good actions.\n",
        "\n",
        "### Suitability for Risk-Averse Strategies\n",
        "- **Conservative Strategies**: In certain environments where taking risks might lead to high penalties, SARSA’s tendency to follow the current policy can result in safer, more conservative strategies compared to off-policy methods like Q-Learning.\n",
        "\n",
        "\n",
        "\n",
        "### Pseudocode\n",
        "\n",
        "```python\n",
        "Initialize Q(s, a) arbitrarily for all s, a\n",
        "Repeat for each episode:\n",
        "    Initialize s\n",
        "    Choose a from s using policy derived from Q (epsilon-greedy)\n",
        "    Repeat for each step of the episode:\n",
        "        Take action a, observe r, s'\n",
        "        Choose a' from s' using policy derived from Q (epsilon-greedy)\n",
        "        Q(s, a) = Q(s, a) + α [r + γ Q(s', a') - Q(s, a)]\n",
        "        s = s'\n",
        "        a = a'\n",
        "    until s is terminal\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_aoyWhYNlNhu"
      }
    }
  ]
}