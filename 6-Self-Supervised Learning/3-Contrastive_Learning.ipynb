{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive Learning: In-Depth Mathematical Explanation\n",
        "\n",
        "## Core Concepts of Contrastive Learning\n",
        "\n",
        "Contrastive learning is a self-supervised learning technique that aims to learn useful representations by contrasting positive pairs (similar instances) against negative pairs (dissimilar instances) in the data. Here, we will explore contrastive learning with a detailed mathematical explanation, including the training procedure, derivatives, advantages, and drawbacks.\n",
        "\n",
        "### Contrastive Learning Approach\n",
        "\n",
        "#### Objective\n",
        "\n",
        "The main objective of contrastive learning is to learn representations where similar instances are brought closer (clustered together) in the embedding space, while dissimilar instances are pushed apart.\n",
        "\n",
        "#### Mathematical Explanation\n",
        "\n",
        "Let $\\mathcal{D}$ denote a dataset consisting of pairs $(\\mathbf{x}_i, \\mathbf{x}_j)$ where $\\mathbf{x}_i$ and $\\mathbf{x}_j$ are augmented versions of the same image (positive pairs) or different images (negative pairs).\n",
        "\n",
        "- **Representation Encoder**: We have an encoder network $f_\\theta$ parameterized by $\\theta$ that maps input data $\\mathbf{x}$ to a latent representation $\\mathbf{z} = f_\\theta(\\mathbf{x})$.\n",
        "\n",
        "- **Augmentation**: Each input $\\mathbf{x}$ undergoes data augmentation to produce two versions: $\\mathbf{x}_i$ and $\\mathbf{x}_j$.\n",
        "\n",
        "- **Contrastive Objective**: The contrastive loss encourages the model to pull representations of positive pairs together and push representations of negative pairs apart. A commonly used contrastive loss is the InfoNCE (InfoMax) loss:\n",
        "  \n",
        "  $$ \\mathcal{L}(\\theta) = -\\log \\frac{\\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j) / \\tau)}{\\sum_{k=1}^{2N} \\mathbf{1}_{[k \\neq i]} \\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_k) / \\tau)} $$\n",
        "  \n",
        "  where:\n",
        "  - $\\mathbf{z}_i = f_\\theta(\\mathbf{x}_i)$ and $\\mathbf{z}_j = f_\\theta(\\mathbf{x}_j)$ are the representations of positive pairs,\n",
        "  - $\\mathbf{z}_k = f_\\theta(\\mathbf{x}_k)$ are representations of negative pairs,\n",
        "  - $\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j) = \\frac{\\mathbf{z}_i \\cdot \\mathbf{z}_j}{\\|\\mathbf{z}_i\\| \\|\\mathbf{z}_j\\|}$ is the cosine similarity between $\\mathbf{z}_i$ and $\\mathbf{z}_j$,\n",
        "  - $\\tau$ is a temperature parameter that scales the logits to control the concentration of the probability distribution,\n",
        "  - $\\mathbf{1}_{[k \\neq i]}$ is an indicator function that ensures we do not include the similarity of $\\mathbf{z}_i$ with itself in the denominator.\n",
        "\n",
        "#### Training Procedure\n",
        "\n",
        "1. **Forward Pass**: Compute representations $\\mathbf{z}_i$ and $\\mathbf{z}_j$ for positive pairs and $\\mathbf{z}_k$ for negative pairs.\n",
        "  \n",
        "2. **Compute Similarities**: Calculate cosine similarities $\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j)$ and $\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_k)$.\n",
        "\n",
        "3. **Calculate Loss**: Compute the contrastive loss $\\mathcal{L}(\\theta)$ using the InfoNCE loss formulation.\n",
        "\n",
        "4. **Backpropagation**: Compute gradients of $\\mathcal{L}(\\theta)$ with respect to $\\theta$.\n",
        "\n",
        "5. **Update Parameters**: Update the parameters $\\theta$ of the encoder network using gradient descent:\n",
        "   $$ \\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta) $$\n",
        "\n",
        "#### Derivatives\n",
        "\n",
        "- **Gradient of Contrastive Loss**: The gradient of the contrastive loss with respect to the parameters $\\theta$ is:\n",
        "  $$ \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{1}{\\tau^2} \\sum_{i=1}^{N} \\left( \\frac{\\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j) / \\tau)}{\\sum_{k=1}^{2N} \\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_k) / \\tau)} - 1 \\right) \\cdot \\frac{\\partial \\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j)}{\\partial \\theta} \\cdot (\\mathbf{z}_i - \\mathbf{z}_j) $$\n",
        "  \n",
        "  where $\\frac{\\partial \\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j)}{\\partial \\theta}$ denotes the gradient of the cosine similarity function with respect to $\\theta$.\n",
        "\n",
        "#### Advantages\n",
        "\n",
        "- Learns representations that are invariant to augmentations and robust to variations in input data.\n",
        "- Does not require manual annotation of data, making it scalable to large datasets.\n",
        "\n",
        "#### Drawbacks\n",
        "\n",
        "- Choice of augmentation strategies and hyperparameters (e.g., temperature $\\tau$) can significantly impact performance.\n",
        "- Computationally intensive due to the need to compare each instance with all others in the batch.\n",
        "\n",
        "Contrastive learning has shown promising results in various domains, especially in computer vision and natural language processing, where learning powerful representations without explicit labels is crucial.\n"
      ],
      "metadata": {
        "id": "dUOU8e-ksD1L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu56YUyJr6LA"
      },
      "outputs": [],
      "source": []
    }
  ]
}