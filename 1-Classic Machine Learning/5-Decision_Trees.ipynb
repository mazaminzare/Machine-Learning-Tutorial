{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees: Mathematical Background and Variants\n",
        "\n",
        "### Introduction to Decision Trees\n",
        "\n",
        "Decision Trees are a non-parametric supervised learning method used for both classification and regression tasks. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
        "\n",
        "### How Decision Trees Work\n",
        "\n",
        "1. **Node Splitting**: Each node in the tree represents a single feature in a dataset, which is split into two or more branches based on a certain condition. This process starts at the tree's root and is repeated recursively on each derived subset.\n",
        "\n",
        "2. **Splitting Criteria**:\n",
        "   - **Classification**: The Gini impurity or entropy is commonly used to measure the best split.\n",
        "   - **Regression**: Variance reduction is often used for determining splits.\n",
        "\n",
        "3. **Tree Pruning**: Reducing the size of a decision tree by removing sections of the tree that provide little power in predicting instances. This reduces the complexity and variance of the model.\n",
        "\n",
        "4. **Stopping Criteria**: The tree stops growing when it reaches a predetermined condition, like a maximum depth or a minimum number of samples required to split a node.\n",
        "\n",
        "### Mathematical Formulas\n",
        "\n",
        "- **Gini Impurity**: A measure of how often a randomly chosen element from the set would be incorrectly labeled.\n",
        "  \n",
        "  $$\n",
        "  G = 1 - \\sum_{i=1}^{n} p_i^2\n",
        "  $$\n",
        "\n",
        "- **Entropy**: A measure of the amount of information disorder or uncertainty.\n",
        "  \n",
        "  $$\n",
        "  H = -\\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
        "  $$\n",
        "\n",
        "- **Information Gain**: The change in information entropy from a prior state to a state that takes some information as given.\n",
        "  \n",
        "  $$\n",
        "  IG(T, a) = H(T) - H(T|a)\n",
        "  $$\n",
        "\n",
        "### Decision Tree Variant: Random Forest\n",
        "\n",
        "Random Forest is an ensemble learning method for classification and regression that constructs a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees.\n",
        "\n",
        "#### Benefits of Random Forest:\n",
        "- **Accuracy**: Generally more accurate than a single decision tree.\n",
        "- **Overfitting**: Controls overfitting better than individual decision trees.\n",
        "- **Variable Importance**: Provides a good indicator of the importance of features.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QXI8kA4EWC7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems of Using Gini, Information Gain, and Entropy in Decision Trees\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Decision Trees use criteria like Gini impurity, Information Gain, and Entropy to decide the best splits at each node. Each method has its own advantages and disadvantages, which can impact the performance and characteristics of the decision tree.\n",
        "\n",
        "### Gini Impurity\n",
        "\n",
        "#### Formula\n",
        "Gini impurity is a measure of how often a randomly chosen element would be incorrectly classified:\n",
        "\n",
        "$$\n",
        "G = 1 - \\sum_{i=1}^{n} p_i^2\n",
        "$$\n",
        "\n",
        "Where $p_i$ is the probability of an element being classified as class $i$.\n",
        "\n",
        "#### Advantages\n",
        "1. **Computation**: Gini impurity is computationally efficient as it does not require logarithmic calculations.\n",
        "2. **Performance**: Often leads to faster splits as it tends to create purer child nodes early on.\n",
        "\n",
        "#### Disadvantages\n",
        "1. **Bias towards Multisplits**: Gini impurity can be biased towards attributes with more levels, leading to a preference for features with more categories.\n",
        "2. **Sensitivity to Class Distribution**: It may not perform well if the class distribution is highly imbalanced.\n",
        "\n",
        "### Information Gain\n",
        "\n",
        "#### Formula\n",
        "Information Gain is based on the decrease in entropy after a dataset is split on an attribute:\n",
        "\n",
        "$$\n",
        "IG(T, a) = H(T) - H(T|a)\n",
        "$$\n",
        "\n",
        "Where $H(T)$ is the entropy of the dataset $T$ and $H(T|a)$ is the conditional entropy of $T$ given attribute $a$.\n",
        "\n",
        "#### Advantages\n",
        "1. **Intuitive**: Provides a clear measure of the reduction in uncertainty due to the split.\n",
        "2. **Theoretical Foundation**: Based on information theory, providing a solid theoretical background.\n",
        "\n",
        "#### Disadvantages\n",
        "1. **Bias towards Attributes with Many Values**: Information Gain tends to favor attributes with a large number of distinct values, potentially leading to overfitting.\n",
        "2. **Computational Complexity**: Calculating entropy involves logarithmic functions, which can be computationally intensive.\n",
        "\n",
        "### Entropy\n",
        "\n",
        "#### Formula\n",
        "Entropy is a measure of the amount of uncertainty or impurity in the dataset:\n",
        "\n",
        "$$\n",
        "H = -\\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "Where $p_i$ is the probability of an element being classified as class $i$.\n",
        "\n",
        "#### Advantages\n",
        "1. **Robustness**: Entropy is robust to small changes in the dataset and provides a clear measure of impurity.\n",
        "2. **General Use**: Suitable for both binary and multi-class classification problems.\n",
        "\n",
        "#### Disadvantages\n",
        "1. **Computational Cost**: Similar to Information Gain, the calculation of entropy involves logarithmic computations, which can be expensive.\n",
        "2. **Bias towards High Cardinality**: Like Information Gain, Entropy can also be biased towards attributes with many distinct values.\n",
        "\n",
        "### Other Methods\n",
        "\n",
        "#### Chi-Square\n",
        "- **Advantages**: Useful for categorical data and can handle multi-class classification problems.\n",
        "- **Disadvantages**: May not perform well with small sample sizes and can be sensitive to rare categories.\n",
        "\n",
        "#### Gain Ratio\n",
        "- **Advantages**: Adjusts Information Gain by taking the number and size of branches into account, reducing bias towards attributes with many values.\n",
        "- **Disadvantages**: Can still be complex to compute and may not always outperform simpler criteria like Gini.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Each criterion for splitting in Decision Trees has its own set of advantages and disadvantages. The choice of criterion can significantly affect the performance and interpretability of the model. It is essential to consider the nature of the dataset, the computational resources available, and the specific requirements of the problem at hand when selecting a splitting criterion.\n"
      ],
      "metadata": {
        "id": "zk2BN3qRdF73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Numerical Example of Decision Tree\n",
        "\n",
        "Consider a small dataset about whether or not to play tennis based on the weather conditions. The dataset has three features: Outlook, Temperature, and Humidity. Hereâ€™s a simplified dataset:\n",
        "\n",
        "| Outlook | Temperature | Humidity | PlayTennis |\n",
        "|---------|-------------|----------|------------|\n",
        "| Sunny   | Hot         | High     | No         |\n",
        "| Sunny   | Hot         | High     | No         |\n",
        "| Overcast| Hot         | High     | Yes        |\n",
        "| Rainy   | Mild        | High     | Yes        |\n",
        "| Rainy   | Cool        | Normal   | Yes        |\n",
        "| Rainy   | Cool        | Normal   | No         |\n",
        "| Overcast| Cool        | Normal   | Yes        |\n",
        "| Sunny   | Mild        | High     | No         |\n",
        "| Sunny   | Cool        | Normal   | Yes        |\n",
        "| Rainy   | Mild        | Normal   | Yes        |\n",
        "| Sunny   | Mild        | Normal   | Yes        |\n",
        "| Overcast| Mild        | High     | Yes        |\n",
        "| Overcast| Hot         | Normal   | Yes        |\n",
        "| Rainy   | Mild        | High     | No         |\n",
        "\n",
        "### Step-by-Step Calculation\n",
        "\n",
        "#### 1. Calculate Entropy for the Target\n",
        "\n",
        "Entropy helps us measure the impurity in a dataset. For the target variable PlayTennis:\n",
        "\n",
        "$$\n",
        "H(PlayTennis) = -p(Yes) \\log_2(p(Yes)) - p(No) \\log_2(p(No))\n",
        "$$\n",
        "\n",
        "From the dataset:\n",
        "- Total samples = 14\n",
        "- Yes = 9\n",
        "- No = 5\n",
        "\n",
        "Thus:\n",
        "\n",
        "$$\n",
        "H(PlayTennis) = -\\left(\\frac{9}{14}\\right) \\log_2\\left(\\frac{9}{14}\\right) - \\left(\\frac{5}{14}\\right) \\log_2\\left(\\frac{5}{14}\\right) \\approx 0.940\n",
        "$$\n",
        "\n",
        "#### 2. Calculate Entropy for Each Attribute\n",
        "\n",
        "**Outlook:**\n",
        "\n",
        "1. **Sunny:**\n",
        "   - Total = 5\n",
        "   - Yes = 2, No = 3\n",
        "\n",
        "   $$\n",
        "   H(Sunny) = -\\left(\\frac{2}{5}\\right) \\log_2\\left(\\frac{2}{5}\\right) - \\left(\\frac{3}{5}\\right) \\log_2\\left(\\frac{3}{5}\\right) \\approx 0.971\n",
        "   $$\n",
        "\n",
        "2. **Overcast:**\n",
        "   - Total = 4\n",
        "   - Yes = 4, No = 0\n",
        "\n",
        "   $$\n",
        "   H(Overcast) = -\\left(\\frac{4}{4}\\right) \\log_2\\left(\\frac{4}{4}\\right) - \\left(\\frac{0}{4}\\right) \\log_2\\left(\\frac{0}{4}\\right) = 0\n",
        "   $$\n",
        "\n",
        "3. **Rainy:**\n",
        "   - Total = 5\n",
        "   - Yes = 3, No = 2\n",
        "\n",
        "   $$\n",
        "   H(Rainy) = -\\left(\\frac{3}{5}\\right) \\log_2\\left(\\frac{3}{5}\\right) - \\left(\\frac{2}{5}\\right) \\log_2\\left(\\frac{2}{5}\\right) \\approx 0.971\n",
        "   $$\n",
        "\n",
        "The total entropy for Outlook:\n",
        "\n",
        "$$\n",
        "H(Outlook) = \\frac{5}{14}H(Sunny) + \\frac{4}{14}H(Overcast) + \\frac{5}{14}H(Rainy)\n",
        "$$\n",
        "\n",
        "$$\n",
        "H(Outlook) = \\frac{5}{14} \\cdot 0.971 + \\frac{4}{14} \\cdot 0 + \\frac{5}{14} \\cdot 0.971 \\approx 0.693\n",
        "$$\n",
        "\n",
        "**Temperature:**\n",
        "\n",
        "1. **Hot:**\n",
        "   - Total = 4\n",
        "   - Yes = 2, No = 2\n",
        "\n",
        "   $$\n",
        "   H(Hot) = -\\left(\\frac{2}{4}\\right) \\log_2\\left(\\frac{2}{4}\\right) - \\left(\\frac{2}{4}\\right) \\log_2\\left(\\frac{2}{4}\\right) = 1\n",
        "   $$\n",
        "\n",
        "2. **Mild:**\n",
        "   - Total = 6\n",
        "   - Yes = 4, No = 2\n",
        "\n",
        "   $$\n",
        "   H(Mild) = -\\left(\\frac{4}{6}\\right) \\log_2\\left(\\frac{4}{6}\\right) - \\left(\\frac{2}{6}\\right) \\log_2\\left(\\frac{2}{6}\\right) \\approx 0.918\n",
        "   $$\n",
        "\n",
        "3. **Cool:**\n",
        "   - Total = 4\n",
        "   - Yes = 3, No = 1\n",
        "\n",
        "   $$\n",
        "   H(Cool) = -\\left(\\frac{3}{4}\\right) \\log_2\\left(\\frac{3}{4}\\right) - \\left(\\frac{1}{4}\\right) \\log_2\\left(\\frac{1}{4}\\right) \\approx 0.811\n",
        "   $$\n",
        "\n",
        "The total entropy for Temperature:\n",
        "\n",
        "$$\n",
        "H(Temperature) = \\frac{4}{14}H(Hot) + \\frac{6}{14}H(Mild) + \\frac{4}{14}H(Cool)\n",
        "$$\n",
        "\n",
        "$$\n",
        "H(Temperature) = \\frac{4}{14} \\cdot 1 + \\frac{6}{14} \\cdot 0.918 + \\frac{4}{14} \\cdot 0.811 \\approx 0.889\n",
        "$$\n",
        "\n",
        "**Humidity:**\n",
        "\n",
        "1. **High:**\n",
        "   - Total = 7\n",
        "   - Yes = 3, No = 4\n",
        "\n",
        "   $$\n",
        "   H(High) = -\\left(\\frac{3}{7}\\right) \\log_2\\left(\\frac{3}{7}\\right) - \\left(\\frac{4}{7}\\right) \\log_2\\left(\\frac{4}{7}\\right) \\approx 0.985\n",
        "   $$\n",
        "\n",
        "2. **Normal:**\n",
        "   - Total = 7\n",
        "   - Yes = 6, No = 1\n",
        "\n",
        "   $$\n",
        "   H(Normal) = -\\left(\\frac{6}{7}\\right) \\log_2\\left(\\frac{6}{7}\\right) - \\left(\\frac{1}{7}\\right) \\log_2\\left(\\frac{1}{7}\\right) \\approx 0.592\n",
        "   $$\n",
        "\n",
        "The total entropy for Humidity:\n",
        "\n",
        "$$\n",
        "H(Humidity) = \\frac{7}{14}H(High) + \\frac{7}{14}H(Normal)\n",
        "$$\n",
        "\n",
        "$$\n",
        "H(Humidity) = \\frac{7}{14} \\cdot 0.985 + \\frac{7}{14} \\cdot 0.592 \\approx 0.789\n",
        "$$\n",
        "\n",
        "#### 3. Calculate Information Gain for Each Attribute\n",
        "\n",
        "**Information Gain for Outlook:**\n",
        "\n",
        "$$\n",
        "IG(Outlook) = H(PlayTennis) - H(Outlook)\n",
        "$$\n",
        "\n",
        "$$\n",
        "IG(Outlook) = 0.940 - 0.693 = 0.247\n",
        "$$\n",
        "\n",
        "**Information Gain for Temperature:**\n",
        "\n",
        "$$\n",
        "IG(Temperature) = H(PlayTennis) - H(Temperature)\n",
        "$$\n",
        "\n",
        "$$\n",
        "IG(Temperature) = 0.940 - 0.889 = 0.051\n",
        "$$\n",
        "\n",
        "**Information Gain for Humidity:**\n",
        "\n",
        "$$\n",
        "IG(Humidity) = H(PlayTennis) - H(Humidity)\n",
        "$$\n",
        "\n",
        "$$\n",
        "IG(Humidity) = 0.940 - 0.789 = 0.151\n",
        "$$\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The attribute with the highest Information Gain is chosen as the root node for the decision tree. In this example, the attribute with the highest Information Gain is **Outlook**. This process is then repeated for each branch until the stopping criteria are met.\n",
        "\n",
        "This simplified example illustrates the calculation of entropy and information gain, which are fundamental to constructing a decision tree.\n"
      ],
      "metadata": {
        "id": "-k0ujXkNaQNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees: Assumptions, Bias-Variance Trade-off, Advantages, and Disadvantages\n",
        "\n",
        "### Assumptions Behind Decision Trees\n",
        "\n",
        "1. **Assumption of Sufficient Data**:\n",
        "   - Decision Trees assume there is enough data to make multiple splits and ensure that each leaf node has a significant number of data points to reduce overfitting.\n",
        "\n",
        "2. **Assumption of Independent Features**:\n",
        "   - Although Decision Trees do not explicitly assume independence between features, they can sometimes overfit to particular combinations of features if the dataset is small or highly correlated.\n",
        "\n",
        "3. **Homogeneity of Splits**:\n",
        "   - Decision Trees assume that data can be partitioned into homogeneous subsets that maximize purity or information gain at each split.\n",
        "\n",
        "4. **Feature Relevance**:\n",
        "   - Decision Trees implicitly assume that all features are relevant and contribute to the final decision. Irrelevant or noisy features can impact the tree's structure and performance.\n",
        "\n",
        "### Bias-Variance Trade-off\n",
        "\n",
        "#### Bias\n",
        "- **High Bias**:\n",
        "  - Simplistic models like shallow trees that do not capture the underlying data distribution well, leading to underfitting.\n",
        "  \n",
        "#### Variance\n",
        "- **High Variance**:\n",
        "  - Complex models like deep trees that capture noise in the training data, leading to overfitting.\n",
        "  \n",
        "#### Trade-off\n",
        "- **Balance**:\n",
        "  - The goal is to find the optimal depth of the tree that balances bias and variance to ensure good performance on both training and unseen test data.\n",
        "  \n",
        "### Advantages of Decision Trees\n",
        "\n",
        "1. **Easy to Understand and Interpret**:\n",
        "   - Decision Trees can be easily visualized and understood by humans. The rules are clear and can be easily explained.\n",
        "\n",
        "2. **Requires Little Data Preparation**:\n",
        "   - No need for normalization or scaling of data. Can handle both numerical and categorical features.\n",
        "\n",
        "3. **Handles Nonlinear Relationships**:\n",
        "   - Decision Trees can model complex and nonlinear relationships between features and the target variable.\n",
        "\n",
        "4. **Versatile**:\n",
        "   - Applicable for both classification and regression tasks.\n",
        "\n",
        "5. **Feature Selection**:\n",
        "   - Decision Trees perform implicit feature selection as they split the data based on the most important features first.\n",
        "\n",
        "### Disadvantages and Drawbacks of Decision Trees\n",
        "\n",
        "1. **Overfitting**:\n",
        "   - Decision Trees can easily overfit, especially if they are deep. They tend to capture noise in the data and perform poorly on unseen data.\n",
        "\n",
        "2. **Unstable**:\n",
        "   - Small changes in the data can lead to a completely different tree structure, making them highly sensitive to data variations.\n",
        "\n",
        "3. **Biased to Dominant Classes**:\n",
        "   - If some classes dominate, the decision tree can become biased towards these classes.\n",
        "\n",
        "4. **Complexity**:\n",
        "   - As the number of features increases, the complexity of the tree increases, making it computationally expensive.\n",
        "\n",
        "5. **Lack of Smoothness**:\n",
        "   - Decision Trees can create a decision boundary that is very fragmented and lacks smoothness, especially in regression tasks.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Decision Trees are powerful tools for both classification and regression tasks. They are easy to understand and interpret, require little data preparation, and can handle nonlinear relationships. However, they are prone to overfitting, can be unstable, and may become complex with increasing features. Balancing the depth of the tree is crucial to managing the bias-variance trade-off effectively.\n"
      ],
      "metadata": {
        "id": "eHVo_bVncvJf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RUhQU9Ghcuxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yal0A_xnUVwD",
        "outputId": "ca55bce0-714f-4e33-d92b-d82291b456da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Decision Tree model: 1.0\n",
            "|--- petal length (cm) <= 2.45\n",
            "|   |--- class: 0\n",
            "|--- petal length (cm) >  2.45\n",
            "|   |--- petal length (cm) <= 4.75\n",
            "|   |   |--- petal width (cm) <= 1.65\n",
            "|   |   |   |--- class: 1\n",
            "|   |   |--- petal width (cm) >  1.65\n",
            "|   |   |   |--- class: 2\n",
            "|   |--- petal length (cm) >  4.75\n",
            "|   |   |--- petal width (cm) <= 1.75\n",
            "|   |   |   |--- class: 1\n",
            "|   |   |--- petal width (cm) >  1.75\n",
            "|   |   |   |--- class: 2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "tree_classifier = DecisionTreeClassifier(max_depth=3)\n",
        "tree_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of test data\n",
        "y_pred = tree_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of the Decision Tree model:\", accuracy)\n",
        "\n",
        "# Visualize the tree\n",
        "from sklearn.tree import export_text\n",
        "tree_rules = export_text(tree_classifier, feature_names=iris['feature_names'])\n",
        "print(tree_rules)"
      ]
    }
  ]
}