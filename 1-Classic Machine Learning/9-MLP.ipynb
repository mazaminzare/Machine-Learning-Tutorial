{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Multi-Layer Perceptrons (MLP)\n",
        "\n",
        "## Introduction to MLP\n",
        "\n",
        "Multi-Layer Perceptrons (MLP) are a type of artificial neural network known for their ability to model complex relationships in data through multiple layers of neurons. An MLP consists of at least three layers: an input layer, one or more hidden layers, and an output layer. Each neuron in a layer connects to every neuron in the subsequent layer, typically with an associated weight and bias.\n",
        "\n",
        "## Mathematical Background\n",
        "\n",
        "The mathematical operations within an MLP can be broken down into two major phases: the forward pass and the backward pass (used for training via backpropagation).\n",
        "\n",
        "### Forward Pass\n",
        "\n",
        "The forward pass involves computing the output of the neural network for a given input. This is achieved through the following steps:\n",
        "\n",
        "1. **Input Layer**: Receives the input features.\n",
        "2. **Hidden Layers**:\n",
        "   - Each neuron in a hidden layer computes the weighted sum of its inputs, which is then passed through a nonlinear activation function. The output of each neuron can be mathematically represented as follows:\n",
        "     $$ a_j^{(l)} = \\sigma\\left(\\sum_{i} w_{ij}^{(l)} x_i + b_j^{(l)} \\right) $$\n",
        "   - Where:\n",
        "     - $a_j^{(l)}$ is the activation of the $j$-th neuron in the $l$-th layer,\n",
        "     - $w_{ij}^{(l)}$ are the weights connecting the $i$-th neuron in the $(l-1)$-th layer to the $j$-th neuron in the $l$-th layer,\n",
        "     - $b_j^{(l)}$ is the bias of the $j$-th neuron in the $l$-th layer,\n",
        "     - $\\sigma$ is the activation function (e.g., Sigmoid, ReLU),\n",
        "     - $x_i$ are the inputs from the previous layer or the input layer.\n",
        "\n",
        "3. **Output Layer**:\n",
        "   - The final layer's output calculation is similar to that of the hidden layers, but its function may differ depending on the task (e.g., softmax for classification).\n",
        "\n",
        "### Backward Pass (Backpropagation)\n",
        "\n",
        "Backpropagation is used to update the weights and biases of the network based on the error in output. The process includes:\n",
        "\n",
        "1. **Error Calculation**:\n",
        "   - The error between the actual output and the predicted output is calculated, often using a loss function like mean squared error (MSE):\n",
        "     $$ E = \\frac{1}{2} \\sum (y - \\hat{y})^2 $$\n",
        "   - Where $y$ is the true value and $\\hat{y}$ is the predicted value.\n",
        "\n",
        "2. **Gradient Descent**:\n",
        "   - The gradients of the error with respect to each weight and bias are calculated to update the parameters:\n",
        "     $$ w_{ij}^{(l)} = w_{ij}^{(l)} - \\eta \\frac{\\partial E}{\\partial w_{ij}^{(l)}} $$\n",
        "   - Where $\\eta$ is the learning rate.\n",
        "\n",
        "3. **Propagation of Error**:\n",
        "   - The error is propagated back through the network, updating each weight and bias according to its contribution to the output error.\n",
        "\n",
        "### Numerical Example\n",
        "\n",
        "Consider a simple network with one input neuron, one hidden neuron, and one output neuron, with a single data point (x=1, y=2):\n",
        "\n",
        "1. **Forward Pass**:\n",
        "   - Input: x = 1\n",
        "   - Hidden Layer Weight: w_1 = 0.5, Bias: b_1 = 0.1\n",
        "   - Output Layer Weight: w_2 = 1.5, Bias: b_2 = -0.3\n",
        "   - Activation Function: Sigmoid\n",
        "   - Output Calculation:\n",
        "     - Hidden Layer Activation: $ a_1 = \\sigma(0.5 \\times 1 + 0.1) $\n",
        "     - Output: $ \\hat{y} = \\sigma(1.5 \\times a_1 - 0.3) $\n",
        "\n",
        "2. **Backward Pass**:\n",
        "   - Loss: $ E = \\frac{1}{2} (2 - \\hat{y})^2 $\n",
        "   - Update Weights using gradient descent.\n",
        "\n",
        "By understanding and implementing these computations, one can effectively use MLPs to tackle various predictive modeling tasks.\n"
      ],
      "metadata": {
        "id": "fVo8zI3RgDW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Multi-Layer Perceptrons (MLP)\n",
        "\n",
        "## Introduction to MLP\n",
        "\n",
        "Multi-Layer Perceptrons (MLP) are a type of artificial neural network known for their ability to model complex relationships in data through multiple layers of neurons. An MLP consists of at least three layers: an input layer, one or more hidden layers, and an output layer. Each neuron in a layer connects to every neuron in the subsequent layer, typically with an associated weight and bias.\n",
        "\n",
        "## Mathematical Background\n",
        "\n",
        "The mathematical operations within an MLP can be broken down into two major phases: the forward pass and the backward pass (used for training via backpropagation).\n",
        "\n",
        "### Forward Pass\n",
        "\n",
        "The forward pass involves computing the output of the neural network for a given input. This is achieved through the following steps:\n",
        "\n",
        "1. **Input Layer**: Receives the input features.\n",
        "2. **Hidden Layers**:\n",
        "   - Each neuron in a hidden layer computes the weighted sum of its inputs, which is then passed through a nonlinear activation function. The output of each neuron can be mathematically represented as follows:\n",
        "     $$ a_j^{(l)} = \\sigma\\left(\\sum_{i} w_{ij}^{(l)} x_i + b_j^{(l)} \\right) $$\n",
        "   - Where:\n",
        "     - $a_j^{(l)}$ is the activation of the $j$-th neuron in the $l$-th layer,\n",
        "     - $w_{ij}^{(l)}$ are the weights connecting the $i$-th neuron in the $(l-1)$-th layer to the $j$-th neuron in the $l$-th layer,\n",
        "     - $b_j^{(l)}$ is the bias of the $j$-th neuron in the $l$-th layer,\n",
        "     - $\\sigma$ is the activation function (e.g., Sigmoid, ReLU),\n",
        "     - $x_i$ are the inputs from the previous layer or the input layer.\n",
        "\n",
        "3. **Output Layer**:\n",
        "   - The final layer's output calculation is similar to that of the hidden layers, but its function may differ depending on the task (e.g., softmax for classification).\n",
        "\n",
        "### Backward Pass (Backpropagation)\n",
        "\n",
        "Backpropagation is used to update the weights and biases of the network based on the error in output. The process includes:\n",
        "\n",
        "1. **Error Calculation**:\n",
        "   - The error between the actual output and the predicted output is calculated, often using a loss function like mean squared error (MSE):\n",
        "     $$ E = \\frac{1}{2} \\sum (y - \\hat{y})^2 $$\n",
        "   - Where $y$ is the true value and $\\hat{y}$ is the predicted value.\n",
        "\n",
        "2. **Gradient Descent**:\n",
        "   - The gradients of the error with respect to each weight and bias are calculated to update the parameters:\n",
        "     $$ w_{ij}^{(l)} = w_{ij}^{(l)} - \\eta \\frac{\\partial E}{\\partial w_{ij}^{(l)}} $$\n",
        "   - Where $\\eta$ is the learning rate.\n",
        "\n",
        "3. **Propagation of Error**:\n",
        "   - The error is propagated back through the network, updating each weight and bias according to its contribution to the output error.\n",
        "\n",
        "### Derivatives Calculation\n",
        "\n",
        "To update the weights and biases, we calculate the derivatives as follows:\n",
        "\n",
        "- For the output layer:\n",
        "  $$ \\frac{\\partial E}{\\partial w_{kj}} = \\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_k} \\cdot \\frac{\\partial z_k}{\\partial w_{kj}} $$\n",
        "  Where:\n",
        "  - $\\frac{\\partial E}{\\partial \\hat{y}} = \\hat{y} - y$ (for MSE),\n",
        "  - $\\frac{\\partial \\hat{y}}{\\partial z_k} = \\sigma'(z_k)$ (derivative of the activation function),\n",
        "  - $\\frac{\\partial z_k}{\\partial w_{kj}} = a_j$ (activation from the previous layer).\n",
        "\n",
        "- For hidden layers, similarly:\n",
        "  $$ \\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial a_j} \\cdot \\frac{\\partial a_j}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial w_{ij}} $$\n",
        "  Where:\n",
        "  - $\\frac{\\partial E}{\\partial a_j}$ is propagated error from subsequent layers,\n",
        "  - $\\frac{\\partial a_j}{\\partial z_i} = \\sigma'(z_i)$,\n",
        "  - $\\frac{\\partial z_i}{\\partial w_{ij}} = x_i$ (input to the neuron).\n",
        "\n",
        "### Numerical Example\n",
        "\n",
        "Consider a simple network with one input neuron, one hidden neuron, and one output neuron, with a single data point (x=1, y=2):\n",
        "\n",
        "1. **Forward Pass**:\n",
        "   - Input: x = 1\n",
        "   - Hidden Layer Weight: w_1 = 0.5, Bias: b_1 = 0.1\n",
        "   - Output Layer Weight: w_2 = 1.5, Bias: b_2 = -0.3\n",
        "   - Activation Function: Sigmoid\n",
        "   - Output Calculation:\n",
        "     - Hidden Layer Activation: $ a_1 = \\sigma(0.5 \\times 1 + 0.1) $\n",
        "     - Output: $ \\hat{y} = \\sigma(1.5 \\times a_1 - 0.3) $\n",
        "\n",
        "2. **Backward Pass**:\n",
        "   - Loss: $ E = \\frac{1}{2} (2 - \\hat{y})^2 $\n",
        "   - Update Weights using gradient descent.\n",
        "\n",
        "By understanding and implementing these computations, one can effectively use MLPs to tackle various predictive modeling tasks.\n"
      ],
      "metadata": {
        "id": "1dT57LBSh2U1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Multi-Layer Perceptrons (MLP)\n",
        "\n",
        "## Introduction to MLP\n",
        "\n",
        "Multi-Layer Perceptrons (MLP) are a foundational type of artificial neural network designed for modeling complex patterns in data. Comprising multiple layers including an input layer, one or more hidden layers, and an output layer, MLPs facilitate learning through the deep structure of fully connected neurons.\n",
        "\n",
        "## Mathematical Background\n",
        "\n",
        "### Forward Pass\n",
        "\n",
        "During the forward pass, the MLP computes outputs by processing the input data layer-by-layer from the input to the output, applying weights, biases, and activation functions at each neuron.\n",
        "\n",
        "- **Input Layer**: Receives the input features.\n",
        "- **Hidden Layers**:\n",
        "  - Each neuron in a hidden layer computes the weighted sum of its inputs and applies a nonlinear activation function:\n",
        "    $$ a_j^{(l)} = \\sigma\\left(\\sum_{i} w_{ij}^{(l)} x_i + b_j^{(l)} \\right) $$\n",
        "  - Where:\n",
        "    - $a_j^{(l)}$ is the activation of the $j$-th neuron in the $l$-th layer,\n",
        "    - $w_{ij}^{(l)}$ are the weights from the $i$-th neuron in the $(l-1)$-th layer to the $j$-th neuron,\n",
        "    - $b_j^{(l)}$ is the bias of the $j$-th neuron,\n",
        "    - $\\sigma$ is the activation function (e.g., Sigmoid, ReLU),\n",
        "    - $x_i$ are the inputs from the previous layer (or external inputs for the first hidden layer).\n",
        "\n",
        "- **Output Layer**:\n",
        "  - Often differs in function based on the specific application, such as using softmax for classification or a linear function for regression.\n",
        "\n",
        "### Backward Pass (Backpropagation)\n",
        "\n",
        "Backpropagation is used to update the weights and biases of the network based on the error in output. The process includes:\n",
        "\n",
        "1. **Error Calculation**:\n",
        "   - The error between the actual output and the predicted output is calculated, often using a loss function like mean squared error (MSE):\n",
        "     $$ E = \\frac{1}{2} \\sum (y - \\hat{y})^2 $$\n",
        "   - Where $y$ is the true value and $\\hat{y}$ is the predicted value.\n",
        "\n",
        "2. **Gradient Descent**:\n",
        "   - The gradients of the error with respect to each weight and bias are calculated to update the parameters:\n",
        "     $$ w_{ij}^{(l)} = w_{ij}^{(l)} - \\eta \\frac{\\partial E}{\\partial w_{ij}^{(l)}} $$\n",
        "   - Where $\\eta$ is the learning rate.\n",
        "\n",
        "3. **Propagation of Error**:\n",
        "   - The error is propagated back through the network, updating each weight and bias according to its contribution to the output error.\n",
        "\n",
        "#### Derivatives Calculation:\n",
        "\n",
        "1. **Output Layer Error Gradient**:\n",
        "\n",
        "    - For the output layer:\n",
        "  $$ \\frac{\\partial E}{\\partial w_{kj}} = \\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_k} \\cdot \\frac{\\partial z_k}{\\partial w_{kj}} $$\n",
        "  Where:\n",
        "  - $\\frac{\\partial E}{\\partial \\hat{y}} = \\hat{y} - y$ (for MSE),\n",
        "  - $\\frac{\\partial \\hat{y}}{\\partial z_k} = \\sigma'(z_k)$ (derivative of the activation function),\n",
        "  - $\\frac{\\partial z_k}{\\partial w_{kj}} = a_j$ (activation from the previous layer).\n",
        "   - Calculate the error gradient for each output neuron:\n",
        "     $$ \\delta_k = (\\hat{y}_k - y_k) \\sigma'(z_k) $$\n",
        "   - Where $z_k$ is the total input to the k-th output neuron, and $\\sigma'(z_k)$ is the derivative of the activation function at $z_k$.\n",
        "\n",
        "\n",
        "\n",
        "2. **Hidden Layer Error Gradient**:\n",
        "   - Error gradients for hidden layer neurons are calculated by propagating the output layer gradients backward:\n",
        "     $$ \\delta_j = \\left(\\sum_{k} w_{jk} \\delta_k\\right) \\sigma'(z_j) $$\n",
        "   - Where $w_{jk}$ are the weights from the j-th neuron to the k-th neuron in the next layer.\n",
        "\n",
        "   - For hidden layers, similarly:\n",
        "  $$ \\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial a_j} \\cdot \\frac{\\partial a_j}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial w_{ij}} $$\n",
        "  Where:\n",
        "  - $\\frac{\\partial E}{\\partial a_j}$ is propagated error from subsequent layers,\n",
        "  - $\\frac{\\partial a_j}{\\partial z_i} = \\sigma'(z_i)$,\n",
        "  - $\\frac{\\partial z_i}{\\partial w_{ij}} = x_i$ (input to the neuron).\n",
        "\n",
        "\n",
        "\n",
        "4. **Update Rules**:\n",
        "   - Weights are updated by subtracting a portion of the gradient:\n",
        "     $$ w_{ij} = w_{ij} - \\eta \\cdot \\delta_j \\cdot a_i $$\n",
        "   - Where $\\eta$ is the learning rate.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Numerical Example\n",
        "\n",
        "Consider a simple network with one input (x=3), two hidden neurons, and one output neuron, trained on a single data point with target output y=1, using the sigmoid activation function and mean squared error (MSE) loss.\n",
        "\n",
        "#### Forward Pass Calculation:\n",
        "\n",
        "1. **Input to Hidden Layer**:\n",
        "   - Weights: $w_{11} = 0.1, w_{21} = -0.2$\n",
        "   - Biases: $b_1 = 0, b_2 = 0.1$\n",
        "   - Outputs:\n",
        "     - $ a_1 = \\sigma(0.1 \\times 3 + 0) = \\sigma(0.3) $\n",
        "     - $ a_2 = \\sigma(-0.2 \\times 3 + 0.1) = \\sigma(-0.5) $\n",
        "\n",
        "2. **Hidden to Output Layer**:\n",
        "   - Weight: $w_{12} = 0.3$\n",
        "   - Bias: $b_3 = -0.1$\n",
        "   - Output: $ \\hat{y} = \\sigma(0.3 \\times a_1 + 0.3 \\times a_2 - 0.1) $\n",
        "\n",
        "#### Backward Pass Calculation:\n",
        "\n",
        "1. **Output Error**:\n",
        "   - Error term: $ \\delta_3 = (\\hat{y} - 1) \\sigma'(\\text{input to output neuron}) $\n",
        "\n",
        "2. **Hidden Layer Errors**:\n",
        "   - Error terms:\n",
        "     - $ \\delta_1 = (w_{12} \\cdot \\delta_3) \\sigma'(\\text{input to neuron 1}) $\n",
        "     - $ \\delta_2 = (w_{12} \\cdot \\delta_3) \\sigma'(\\text{input to neuron 2}) $\n",
        "\n",
        "3. **Update Weights**:\n",
        "   - $ w_{11} = w_{11} - \\eta \\cdot \\delta_1 \\cdot x $\n",
        "   - $ w_{21} = w_{21} - \\eta \\cdot \\delta_2 \\cdot x $\n",
        "   - $ w_{12} = w_{12} - \\eta \\cdot \\delta_3 \\cdot a_1 $\n",
        "\n",
        "This tutorial aims to demystify the training of an MLP by detailing each computational step involved, from data input through to the adjustments made based on the output error.\n"
      ],
      "metadata": {
        "id": "cMqW_dijjO49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Multi-Layer Perceptrons (MLP)\n",
        "\n",
        "## Assumptions Behind MLP\n",
        "\n",
        "MLPs are based on several key assumptions:\n",
        "- **Universal Approximation**: MLPs assume they can approximate any continuous function, given a sufficient number of neurons in a hidden layer and appropriate activation functions.\n",
        "- **Data Scaling**: Input features should be normalized or standardized to ensure efficient convergence during training.\n",
        "- **Independence and Identical Distribution (i.i.d.)**: The data samples are assumed to be drawn independently from the same distribution, which is crucial for the generalizability of the model.\n",
        "- **Differentiability**: The function to be learned must be differentiable almost everywhere, which is necessary for the backpropagation algorithm to work.\n",
        "\n",
        "## Bias-Variance Trade-off\n",
        "\n",
        "In MLPs:\n",
        "- **Bias**: High bias occurs if the network is too simple to capture the underlying patterns in the data.\n",
        "- **Variance**: High variance can cause overfitting, where the model learns noise from the training data rather than the actual trends.\n",
        "\n",
        "Balancing the bias and variance is crucial to building effective MLP models. Techniques like regularization and dropout are often used to manage this balance.\n",
        "\n",
        "## Advantages of MLPs\n",
        "\n",
        "1. **Non-Linear Modeling**: MLPs are capable of modeling complex non-linear relationships due to their layered structure and activation functions.\n",
        "2. **Flexibility**: They can be used across various tasks including classification, regression, and feature learning.\n",
        "3. **Scalability**: Given adequate data and computational resources, MLPs can be scaled to improve performance.\n",
        "\n",
        "## Disadvantages of MLPs\n",
        "\n",
        "1. **Overfitting Risk**: Without proper regularization, MLPs can easily overfit, especially in cases with noisy training data or when the model is too complex.\n",
        "2. **Computationally Intensive**: Training MLPs can be resource-intensive, requiring substantial computing power and time, particularly as the network size increases.\n",
        "3. **Local Minima**: The non-convex nature of neural network training can lead to suboptimal solutions if the optimization gets stuck in local minima.\n",
        "4. **Require Large Datasets**: MLPs generally require large amounts of data to perform well without overfitting and to generalize effectively to new data.\n",
        "5. **Black Box Nature**: Neural networks, including MLPs, are often considered \"black boxes\" because it can be difficult to interpret how they are making predictions, which can be a drawback in applications where transparency is important.\n",
        "\n"
      ],
      "metadata": {
        "id": "VuXcT0RE8kX_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HliocDHbfPTs",
        "outputId": "0c50a85c-8acf-4bf6-8e45-bcf4d9a435f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss 0.2879688550631573\n",
            "Epoch 1000, Loss 0.07409680852610133\n",
            "Epoch 2000, Loss 0.003439088578271777\n",
            "Epoch 3000, Loss 0.0015734654780032672\n",
            "Epoch 4000, Loss 0.0010027637903330922\n",
            "Epoch 5000, Loss 0.0007309150731999721\n",
            "Epoch 6000, Loss 0.0005730270921955388\n",
            "Epoch 7000, Loss 0.0004702562527598223\n",
            "Epoch 8000, Loss 0.00039820172101227787\n",
            "Epoch 9000, Loss 0.0003449676921624329\n",
            "Input: [0 0] - Predicted: [[0.01915486]]\n",
            "Input: [0 1] - Predicted: [[0.98339885]]\n",
            "Input: [1 0] - Predicted: [[0.98339169]]\n",
            "Input: [1 1] - Predicted: [[0.0172618]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Create a class for the MLP\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
        "        self.bias_hidden = np.zeros((1, hidden_size))\n",
        "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
        "        self.bias_output = np.zeros((1, output_size))\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        # Forward pass\n",
        "        self.hidden = sigmoid(np.dot(X, self.weights_input_hidden) + self.bias_hidden)\n",
        "        self.output = sigmoid(np.dot(self.hidden, self.weights_hidden_output) + self.bias_output)\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, X, y, learning_rate):\n",
        "        # Error in output\n",
        "        output_error = y - self.output\n",
        "        output_delta = output_error * sigmoid_derivative(self.output)\n",
        "\n",
        "        # Error in hidden layer\n",
        "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden)\n",
        "\n",
        "        # Update parameters\n",
        "        self.weights_hidden_output += np.dot(self.hidden.T, output_delta) * learning_rate\n",
        "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
        "        self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate\n",
        "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, learning_rate, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.feedforward(X)\n",
        "            self.backpropagation(X, y, learning_rate)\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean((y - output) ** 2)\n",
        "                print(f\"Epoch {epoch}, Loss {loss}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Input data (e.g., XOR problem)\n",
        "    X = np.array([[0, 0],\n",
        "                  [0, 1],\n",
        "                  [1, 0],\n",
        "                  [1, 1]])\n",
        "    # Labels\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    # Create MLP object\n",
        "    mlp = MLP(input_size=2, hidden_size=2, output_size=1)\n",
        "    mlp.train(X, y, learning_rate=0.5, epochs=10000)\n",
        "\n",
        "    # Test\n",
        "    for inputs in X:\n",
        "        print(f\"Input: {inputs} - Predicted: {mlp.feedforward(inputs)}\")\n"
      ]
    }
  ]
}