{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Is All You Need: A Comprehensive Tutorial with Mathematical Background\n",
        "\n",
        "**Attention Is All You Need** is a seminal paper by Vaswani et al., published in 2017. This paper introduces the Transformer model, which has since become the foundation of many state-of-the-art models in natural language processing (NLP), including GPT-3 and BERT. The key innovation in the Transformer model is the use of self-attention mechanisms to handle the dependencies between input and output sequences, rather than relying on recurrent or convolutional layers.\n",
        "\n",
        "## 1. Background and Motivation\n",
        "\n",
        "Prior to the Transformer model, Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) were popular choices for sequence modeling tasks. However, these models suffer from several limitations:\n",
        "- Difficulty in parallelization due to their sequential nature.\n",
        "- Vanishing gradient problem, making it hard to capture long-range dependencies.\n",
        "\n",
        "The Transformer model addresses these issues by using self-attention mechanisms, which allow for the capture of dependencies regardless of their distance in the sequence, and by enabling parallelization.\n",
        "\n",
        "## 2. Transformer Architecture Overview\n",
        "\n",
        "The Transformer model consists of an encoder-decoder architecture:\n",
        "\n",
        "- **Encoder:** Processes the input sequence to produce a context-aware representation.\n",
        "- **Decoder:** Generates the output sequence using the encoder’s representations.\n",
        "\n",
        "Both the encoder and decoder are composed of multiple layers, each consisting of:\n",
        "1. Self-attention mechanism.\n",
        "2. Feed-forward neural network.\n",
        "\n",
        "Each layer also employs residual connections and layer normalization.\n",
        "\n",
        "## 3. Self-Attention Mechanism\n",
        "\n",
        "Self-attention is the core component of the Transformer model. It allows the model to focus on different parts of the input sequence when producing a specific part of the output sequence.\n",
        "\n",
        "The steps involved in the self-attention mechanism are:\n",
        "\n",
        "1. **Input Representations:** The input consists of a sequence of vectors (e.g., word embeddings).\n",
        "\n",
        "2. **Query, Key, and Value Vectors:** For each input vector, we generate three vectors: query ($Q$), key ($K$), and value ($V$) using learned weight matrices $W_Q$, $W_K$, and $W_V$.\n",
        "\n",
        "   $$\n",
        "   Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
        "   $$\n",
        "\n",
        "   Where $X$ is the input sequence matrix, and $W_Q$, $W_K$, $W_V$ are learned parameter matrices.\n",
        "\n",
        "3. **Scaled Dot-Product Attention:** Compute the attention scores by taking the dot product of the query and key vectors, scaling by the square root of the dimension of the key vectors, and applying a softmax function to obtain the attention weights.\n",
        "\n",
        "   $$\n",
        "   \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
        "   $$\n",
        "\n",
        "   Here, $d_k$ is the dimension of the key vectors. The scaling factor $\\sqrt{d_k}$ helps in stabilizing the gradients.\n",
        "\n",
        "4. **Multi-Head Attention:** Instead of computing a single attention function, the model uses multiple attention heads. Each head performs the above steps independently and their outputs are concatenated and linearly transformed to produce the final output.\n",
        "\n",
        "   $$\n",
        "   \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W_O\n",
        "   $$\n",
        "\n",
        "   Where each head is defined as:\n",
        "\n",
        "   $$\n",
        "   \\text{head}_i = \\text{Attention}(QW_{Q_i}, KW_{K_i}, VW_{V_i})\n",
        "   $$\n",
        "\n",
        "   And $W_O$ is an output projection matrix.\n",
        "\n",
        "## 4. Positional Encoding\n",
        "\n",
        "Since the self-attention mechanism does not inherently capture the order of the sequence, positional encodings are added to the input embeddings to provide information about the position of each token. Positional encodings can be sinusoidal or learned. For sinusoidal positional encoding, the function is defined as:\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
        "$$\n",
        "$$\n",
        "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
        "$$\n",
        "\n",
        "Where $pos$ is the position and $i$ is the dimension.\n",
        "\n",
        "## 5. Encoder and Decoder Structures\n",
        "\n",
        "- **Encoder:**\n",
        "  - Consists of a stack of $N$ identical layers.\n",
        "  - Each layer has two sub-layers: multi-head self-attention mechanism and feed-forward neural network.\n",
        "  - Residual connections are added around each sub-layer, followed by layer normalization.\n",
        "\n",
        "   $$\n",
        "   \\text{LayerNorm}(X + \\text{SelfAttention}(X))\n",
        "   $$\n",
        "   $$\n",
        "   \\text{LayerNorm}(X + \\text{FFN}(X))\n",
        "   $$\n",
        "\n",
        "- **Decoder:**\n",
        "  - Also consists of a stack of $N$ identical layers.\n",
        "  - Each layer has three sub-layers: multi-head self-attention mechanism, multi-head attention mechanism over the encoder’s output, and feed-forward neural network.\n",
        "  - Residual connections and layer normalization are applied similarly to the encoder.\n",
        "\n",
        "   $$\n",
        "   \\text{LayerNorm}(X + \\text{SelfAttention}(X))\n",
        "   $$\n",
        "   $$\n",
        "   \\text{LayerNorm}(X + \\text{EncoderDecoderAttention}(X, E))\n",
        "   $$\n",
        "   $$\n",
        "   \\text{LayerNorm}(X + \\text{FFN}(X))\n",
        "   $$\n",
        "\n",
        "   Where $E$ is the encoder's output.\n",
        "\n",
        "## 6. Feed-Forward Neural Network (FFN)\n",
        "\n",
        "Each position-wise feed-forward network consists of two linear transformations with a ReLU activation in between:\n",
        "\n",
        "$$\n",
        "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "$$\n",
        "\n",
        "## 7. Training and Optimization\n",
        "\n",
        "The model is trained using a sequence-to-sequence task with teacher forcing. The objective function is typically the cross-entropy loss between the predicted and true sequences. Optimization is performed using the Adam optimizer with a learning rate schedule that increases linearly during a warm-up period and then decreases proportionally to the inverse square root of the step number.\n",
        "\n",
        "## 8. Benefits and Applications\n",
        "\n",
        "- **Parallelization:** Transformers enable parallelization, making training more efficient on modern hardware.\n",
        "- **Long-Range Dependencies:** They capture long-range dependencies more effectively than RNNs/LSTMs.\n",
        "- **Scalability:** Transformers can be scaled up with more layers and attention heads, leading to powerful models like GPT and BERT.\n",
        "\n",
        "## 9. Conclusion\n",
        "\n",
        "The Transformer model, introduced by the \"Attention Is All You Need\" paper, revolutionized the field of NLP by providing a more efficient and scalable architecture for sequence modeling tasks. Understanding the core concepts of self-attention and the Transformer architecture is crucial for anyone interested in modern NLP research and applications. The mathematical formulation of self-attention and the overall architecture enables the model to learn complex patterns and dependencies, leading to significant advancements in the field.\n"
      ],
      "metadata": {
        "id": "4T7Zf1jAgmk6"
      },
      "id": "4T7Zf1jAgmk6"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}