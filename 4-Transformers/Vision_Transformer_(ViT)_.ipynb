{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Vision Transformer (ViT) Tutorial with Detailed Mathematical Computations\n",
        "\n",
        "The Vision Transformer (ViT) is a state-of-the-art model for image recognition that leverages the transformer architecture, originally developed for natural language processing. This tutorial covers the ViT's operations, including forward and backward pass computations for each layer.\n",
        "\n",
        "## Vision Transformer Architecture Overview\n",
        "\n",
        "The Vision Transformer consists of several key components:\n",
        "1. **Input Layer**: Tokenizes input images into patches.\n",
        "2. **Patch Embedding**: Projects flattened patches into a higher-dimensional space.\n",
        "3. **Positional Encoding**: Adds positional information to the token embeddings.\n",
        "4. **Transformer Encoder Layers**: Consist of multi-head self-attention and feedforward neural networks.\n",
        "5. **Classification Head**: Applies a linear layer to the class token for classification.\n",
        "\n",
        "### Components and their Details:\n",
        "1. **Input Layer**: Split an image of size $H \\times W$ into patches of size $P \\times P$.\n",
        "2. **Patch Embedding**: Each patch is flattened and mapped to a vector of size $D$ using a linear projection.\n",
        "3. **Positional Encoding**: Adds a positional embedding to each patch embedding.\n",
        "4. **Transformer Encoder**: Stack of $L$ identical layers, each consisting of:\n",
        "   - **Multi-Head Self-Attention (MHSA)**\n",
        "   - **Feedforward Neural Network (FFN)**\n",
        "5. **Classification Head**: A simple fully connected layer that outputs class probabilities.\n",
        "\n",
        "## Detailed Layer-by-Layer Mathematical Operations\n",
        "\n",
        "### Input Layer - Patching\n",
        "- **Forward Pass**:\n",
        "  - Split the image into $N$ patches where $N = \\frac{H \\times W}{P^2}$.\n",
        "  - **Formula**: $I_{\\text{patch}}^n = I[:, i*P:(i+1)*P, j*P:(j+1)*P]$ where $i$ and $j$ index the patch position.\n",
        "\n",
        "### Patch Embedding\n",
        "- **Forward Pass**:\n",
        "  - Flatten each patch and project it to a higher-dimensional space.\n",
        "  - **Formula**: $E_{\\text{patch}}^n = W_e \\cdot \\text{flatten}(I_{\\text{patch}}^n) + b_e$\n",
        "  - Where $W_e$ is the weight matrix and $b_e$ is the bias vector.\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial \\text{flatten}(I_{\\text{patch}}^n)} = W_e^T \\cdot \\frac{\\partial L}{\\partial E_{\\text{patch}}^n}$\n",
        "  - **Gradient w.r.t. weights**: $\\frac{\\partial L}{\\partial W_e} = \\sum_n \\frac{\\partial L}{\\partial E_{\\text{patch}}^n} \\cdot \\text{flatten}(I_{\\text{patch}}^n)^T$\n",
        "  - **Gradient w.r.t. bias**: $\\frac{\\partial L}{\\partial b_e} = \\sum_n \\frac{\\partial L}{\\partial E_{\\text{patch}}^n}$\n",
        "\n",
        "### Positional Encoding\n",
        "- **Forward Pass**:\n",
        "  - Add positional information to each patch embedding.\n",
        "  - **Formula**: $E_{\\text{pos}}^n = E_{\\text{patch}}^n + P^n$\n",
        "  - Where $P^n$ is the positional encoding.\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial E_{\\text{patch}}^n} = \\frac{\\partial L}{\\partial E_{\\text{pos}}^n}$\n",
        "  - **Gradient w.r.t. positional encoding**: $\\frac{\\partial L}{\\partial P^n} = \\frac{\\partial L}{\\partial E_{\\text{pos}}^n}$\n",
        "\n",
        "### Transformer Encoder Layers\n",
        "\n",
        "#### Multi-Head Self-Attention (MHSA)\n",
        "- **Forward Pass**:\n",
        "  - Compute Query, Key, and Value matrices.\n",
        "  - **Formula**: $Q = W_Q \\cdot X$, $K = W_K \\cdot X$, $V = W_V \\cdot X$\n",
        "  - Compute attention scores: $A = \\text{softmax}\\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}\\right)$\n",
        "  - Compute output: $O = A \\cdot V$\n",
        "  - Concatenate heads and project: $O' = W_O \\cdot \\text{concat}(O_1, O_2, ..., O_h)$\n",
        "  - Where $W_Q, W_K, W_V, W_O$ are the weight matrices, and $d_k$ is the dimensionality of the keys.\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial X} = W_Q^T \\cdot \\frac{\\partial L}{\\partial Q} + W_K^T \\cdot \\frac{\\partial L}{\\partial K} + W_V^T \\cdot \\frac{\\partial L}{\\partial V}$\n",
        "  - **Gradient w.r.t. weights**: $\\frac{\\partial L}{\\partial W_Q} = \\frac{\\partial L}{\\partial Q} \\cdot X^T$, $\\frac{\\partial L}{\\partial W_K} = \\frac{\\partial L}{\\partial K} \\cdot X^T$, $\\frac{\\partial L}{\\partial W_V} = \\frac{\\partial L}{\\partial V} \\cdot X^T$\n",
        "\n",
        "#### Feedforward Neural Network (FFN)\n",
        "- **Forward Pass**:\n",
        "  - Two linear transformations with a ReLU activation in between.\n",
        "  - **Formula**: $FFN(X) = W_2 \\cdot \\text{ReLU}(W_1 \\cdot X + b_1) + b_2$\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial X} = W_1^T \\cdot \\left(\\frac{\\partial L}{\\partial H} \\cdot \\text{ReLU}'(W_1 \\cdot X + b_1)\\right) + W_2^T \\cdot \\frac{\\partial L}{\\partial O}$\n",
        "  - **Gradient w.r.t. weights**: $\\frac{\\partial L}{\\partial W_1} = \\left(\\frac{\\partial L}{\\partial H} \\cdot \\text{ReLU}'(W_1 \\cdot X + b_1)\\right) \\cdot X^T$, $\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial O} \\cdot H^T$\n",
        "  - **Gradient w.r.t. biases**: $\\frac{\\partial L}{\\partial b_1} = \\sum \\left(\\frac{\\partial L}{\\partial H} \\cdot \\text{ReLU}'(W_1 \\cdot X + b_1)\\right)$, $\\frac{\\partial L}{\\partial b_2} = \\sum \\frac{\\partial L}{\\partial O}$\n",
        "\n",
        "### Classification Head\n",
        "- **Forward Pass**:\n",
        "  - **Formula**: $O_{\\text{class}} = \\text{softmax}(W_{\\text{class}} \\cdot X_{\\text{class}} + b_{\\text{class}})$\n",
        "  - Where $X_{\\text{class}}$ is the class token.\n",
        "- **Backward Pass**:\n",
        "  - **Gradient w.r.t. input**: $\\frac{\\partial L}{\\partial X_{\\text{class}}} = W_{\\text{class}}^T \\cdot \\frac{\\partial L}{\\partial O_{\\text{class}}}$\n",
        "  - **Gradient w.r.t. weights**: $\\frac{\\partial L}{\\partial W_{\\text{class}}} = \\frac{\\partial L}{\\partial O_{\\text{class}}} \\cdot X_{\\text{class}}^T$\n",
        "  - **Gradient w.r.t. biases**: $\\frac{\\partial L}{\\partial b_{\\text{class}}} = \\frac{\\partial L}{\\partial O_{\\text{class}}}$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T_-_mOl4ExYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Transformer (ViT) Overview\n",
        "\n",
        "Vision Transformer (ViT), introduced by researchers at Google Brain in 2020, marks a significant shift in the approach to image classification tasks traditionally dominated by convolutional neural networks (CNNs). ViT applies the principles of the Transformer architecture, commonly used in natural language processing, to vision tasks by treating images as sequences of patches, which allows it to capture global dependencies within an image.\n",
        "\n",
        "### Key Innovations of ViT\n",
        "\n",
        "Vision Transformer introduced several groundbreaking ideas to the field of computer vision:\n",
        "\n",
        "1. **Transformer Architecture**: ViT adapts the Transformer model, primarily used in NLP, to process images by treating image patches as equivalent to words in a sentence.\n",
        "2. **Attention Mechanism**: Utilizes the self-attention mechanism to weigh the importance of different patches of an image relative to each other.\n",
        "3. **Patch Encoding**: Images are split into fixed-size patches, embedded into tokens, and processed through multiple layers of Transformer blocks.\n",
        "\n",
        "### Architecture of ViT\n",
        "\n",
        "ViT's architecture is simpler in terms of its reliance on well-understood Transformer blocks rather than specialized convolutions. Hereâ€™s a simplified overview:\n",
        "\n",
        "| Layer Type            | Input Dimension              | Output Dimension             | Details                    | Parameters Formula                                           | Number of Parameters |\n",
        "|-----------------------|------------------------------|------------------------------|----------------------------|--------------------------------------------------------------|----------------------|\n",
        "| **Input Image**       | $224 \\times 224 \\times 3$    | N/A                          | N/A                        | N/A                                                          | 0                    |\n",
        "| **Patch Embedding**   | $224 \\times 224 \\times 3$    | $N \\times (P^2 \\cdot C)$     | $16 \\times 16$, flatten   | $(P^2 \\cdot C) \\times D$                                     | Varies               |\n",
        "| **Positional Encoding**| $N \\times D$                | $N \\times D$                 | Add to embedding          | $N \\times D$                                                 | Varies               |\n",
        "| **Transformer Blocks**| $N \\times D$                | $N \\times D$                 | Multi-head attention, MLP | Varies per block                                             | Varies               |\n",
        "| **Classifier Head**   | $D$                          | Number of classes            | Linear                     | $D \\times \\text{Number of classes}$                          | Varies               |\n",
        "\n",
        "- **$N$** is the number of patches.\n",
        "- **$P$** is the size of each patch (e.g., 16x16 pixels).\n",
        "- **$C$** is the number of channels in the image (usually 3 for RGB).\n",
        "- **$D$** is the dimensionality of the patch embeddings.\n",
        "\n",
        "### Advantages of Vision Transformer\n",
        "\n",
        "- **Scalability**: ViT scales effectively with the number of parameters, often surpassing state-of-the-art CNNs as model size increases.\n",
        "- **Efficient Transfer Learning**: Demonstrates strong performance on smaller datasets when pretrained on larger datasets.\n",
        "- **Global Context**: Capable of capturing relationships between distant parts of the image, which is a challenge for CNNs that rely on local receptive fields.\n",
        "\n",
        "### Disadvantages of Vision Transformer\n",
        "\n",
        "- **Data Hungry**: Requires a large amount of data to train effectively from scratch, making it less practical for tasks with limited data.\n",
        "- **Computational Requirements**: High computational cost due to the self-attention mechanism, especially as the number of patches increases.\n",
        "- **Lack of Inductive Biases**: Unlike CNNs, ViT lacks certain inductive biases such as translation invariance, which can sometimes hinder its performance on smaller or less diverse datasets.\n",
        "\n",
        "### Key Properties of ViT\n",
        "\n",
        "- **Patch-based Processing**: Treats image patches as the fundamental component, akin to tokens in NLP, enabling it to leverage the Transformer's capabilities.\n",
        "- **Self-Attention Across Patches**: Allows the model to focus on the most informative parts of the image regardless of their spatial location.\n",
        "- **Flexibility**: Can be easily adapted and extended for various vision tasks beyond classification, such as object detection and segmentation.\n",
        "\n",
        "Vision Transformer continues to inspire innovations in the field of machine learning, challenging conventional approaches and encouraging further exploration of Transformer-based models in vision tasks.\n"
      ],
      "metadata": {
        "id": "mNo7LD4SH0jD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaQ6aSjxEwnN"
      },
      "outputs": [],
      "source": []
    }
  ]
}