{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Contractive Autoencoder (CAE) Tutorial\n",
        "\n",
        "## Introduction\n",
        "\n",
        "A Contractive Autoencoder (CAE) is a type of autoencoder that includes a regularization term to ensure that the learned features are robust to small variations in the input data. This is achieved by adding a penalty to the loss function that penalizes the Jacobian matrix of the encoder's activations with respect to the input data.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "A CAE consists of two main parts:\n",
        "1. **Encoder**: Compresses the input into a latent-space representation.\n",
        "2. **Decoder**: Reconstructs the input from the latent-space representation.\n",
        "\n",
        "### Encoder\n",
        "\n",
        "The encoder function, $h = f(x)$, maps the input $x$ to a hidden representation $h$:\n",
        "\n",
        "$$\n",
        "h = f(x) = \\sigma(Wx + b)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $W$ is a weight matrix\n",
        "- $b$ is a bias vector\n",
        "- $\\sigma$ is an activation function (e.g., ReLU, sigmoid)\n",
        "\n",
        "### Decoder\n",
        "\n",
        "The decoder function, $\\hat{x} = g(h)$, maps the hidden representation $h$ back to the original input space:\n",
        "\n",
        "$$\n",
        "\\hat{x} = g(h) = \\sigma(W'h + b')\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $W'$ is a weight matrix\n",
        "- $b'$ is a bias vector\n",
        "- $\\sigma$ is an activation function\n",
        "\n",
        "### Loss Function\n",
        "\n",
        "The loss function in a Contractive Autoencoder consists of three parts:\n",
        "1. **Reconstruction Loss**: Measures how well the decoder reconstructs the input.\n",
        "2. **Contractive Loss**: Penalizes the sensitivity of the encoder activations to changes in the input.\n",
        "\n",
        "The total loss is:\n",
        "\n",
        "$$\n",
        "L = \\text{Reconstruction Loss} + \\lambda \\text{Contractive Loss}\n",
        "$$\n",
        "\n",
        "where $\\lambda$ is a regularization parameter that controls the weight of the contractive loss.\n",
        "\n",
        "#### Reconstruction Loss\n",
        "\n",
        "The reconstruction loss is typically the mean squared error (MSE):\n",
        "\n",
        "$$\n",
        "\\text{Reconstruction Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2\n",
        "$$\n",
        "\n",
        "#### Contractive Loss\n",
        "\n",
        "The contractive loss penalizes the Frobenius norm of the Jacobian matrix of the encoder's activations with respect to the input:\n",
        "\n",
        "$$\n",
        "\\text{Contractive Loss} = \\text{Tr}(J^T J)\n",
        "$$\n",
        "\n",
        "where $J$ is the Jacobian matrix of the encoder's activations:\n",
        "\n",
        "$$\n",
        "J_{ij} = \\frac{\\partial h_i}{\\partial x_j}\n",
        "$$\n",
        "\n",
        "## Training the Contractive Autoencoder\n",
        "\n",
        "Training a CAE involves minimizing the total loss function, which includes both the reconstruction loss and the contractive loss. This is typically done using gradient descent.\n",
        "\n",
        "### Derivatives\n",
        "\n",
        "Let's derive the gradients for the encoder weights $W$.\n",
        "\n",
        "#### Gradient of the Reconstruction Loss\n",
        "\n",
        "The gradient of the reconstruction loss with respect to the encoder weights is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\hat{x}} \\cdot \\frac{\\partial \\hat{x}}{\\partial h} \\cdot \\frac{\\partial h}{\\partial W}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\hat{x}}{\\partial h} = W'\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial h}{\\partial W} = x\n",
        "$$\n",
        "\n",
        "So:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = (x - \\hat{x}) \\cdot W' \\cdot x^T\n",
        "$$\n",
        "\n",
        "#### Gradient of the Contractive Loss\n",
        "\n",
        "The gradient of the contractive loss with respect to the encoder weights is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial (\\text{Contractive Loss})}{\\partial W} = \\frac{\\partial (\\text{Tr}(J^T J))}{\\partial W}\n",
        "$$\n",
        "\n",
        "Using the chain rule and matrix calculus:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial (\\text{Tr}(J^T J))}{\\partial W} = 2 \\cdot \\left( \\frac{\\partial h}{\\partial W} \\cdot \\frac{\\partial h}{\\partial W}^T \\cdot J^T J \\right)\n",
        "$$\n",
        "\n",
        "Since:\n",
        "\n",
        "$$\n",
        "J = \\frac{\\partial h}{\\partial x}\n",
        "$$\n",
        "\n",
        "Thus:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial (\\text{Contractive Loss})}{\\partial W} = 2 \\cdot x \\cdot (J^T J) \\cdot x^T\n",
        "$$\n",
        "\n",
        "### Gradient Descent Update\n",
        "\n",
        "The weights and biases are updated using the gradients:\n",
        "\n",
        "$$\n",
        "W \\leftarrow W - \\eta \\left(\\frac{\\partial \\text{Reconstruction Loss}}{\\partial W} + \\lambda \\frac{\\partial (\\text{Contractive Loss})}{\\partial W}\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "b \\leftarrow b - \\eta \\frac{\\partial \\text{Reconstruction Loss}}{\\partial b}\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate.\n",
        "\n",
        "# Advantages and Drawbacks\n",
        "\n",
        "## Advantages\n",
        "- **Robust Feature Learning**: CAEs learn features that are more invariant to small perturbations in the input data, making them more robust.\n",
        "- **Better Generalization**: The contractive penalty encourages the model to generalize better to new data by learning smooth and stable features.\n",
        "- **Reduced Overfitting**: The regularization term helps to reduce overfitting by penalizing complex mappings.\n",
        "\n",
        "## Drawbacks\n",
        "- **Computational Cost**: Calculating the Jacobian and the contractive penalty adds computational complexity, making training slower.\n",
        "- **Hyperparameter Tuning**: Choosing the right hyperparameters (e.g., regularization strength) can be challenging and time-consuming.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FC6507IQtWMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H-CQX5JLzzKT"
      }
    }
  ]
}