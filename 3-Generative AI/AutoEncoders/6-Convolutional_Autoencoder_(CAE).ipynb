{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4k0h7tGvfWL"
      },
      "source": [
        "# Convolutional Autoencoder (CAE) Tutorial\n",
        "\n",
        "## Introduction\n",
        "\n",
        "A Convolutional Autoencoder (CAE) is a type of autoencoder that uses convolutional layers to encode and decode the input data. CAEs are particularly well-suited for image data, as convolutional layers can capture spatial hierarchies in images more effectively than fully connected layers.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "A CAE consists of two main parts:\n",
        "1. **Encoder**: Uses convolutional layers to map the input to a latent-space representation.\n",
        "2. **Decoder**: Uses transposed convolutional layers to reconstruct the input from the latent space representation.\n",
        "\n",
        "### Encoder\n",
        "\n",
        "The encoder function uses convolutional layers to reduce the spatial dimensions of the input while increasing the depth (number of filters).\n",
        "\n",
        "Mathematically, a convolutional layer can be described as:\n",
        "\n",
        "$$\n",
        "h = \\sigma(W * x + b)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $*$ denotes the convolution operation\n",
        "- $W$ is a set of learnable filters\n",
        "- $b$ is a bias term\n",
        "- $\\sigma$ is an activation function (e.g., ReLU, sigmoid)\n",
        "- $x$ is the input image\n",
        "\n",
        "### Decoder\n",
        "\n",
        "The decoder function uses transposed convolutional layers (also known as deconvolutional layers) to reconstruct the input from the latent space representation.\n",
        "\n",
        "Mathematically, a transposed convolutional layer can be described as:\n",
        "\n",
        "$$\n",
        "\\hat{x} = \\sigma(W^T * h + b')\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $W^T$ denotes the transposed convolution operation\n",
        "- $b'$ is a bias term\n",
        "- $\\sigma$ is an activation function\n",
        "- $h$ is the latent space representation\n",
        "\n",
        "### Loss Function\n",
        "\n",
        "The loss function for a CAE is typically the mean squared error (MSE) between the input and the reconstructed output:\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2\n",
        "$$\n",
        "\n",
        "## Training Process\n",
        "\n",
        "Training a CAE involves minimizing the loss function with respect to the weights and biases of the convolutional and transposed convolutional layers. This is typically done using gradient descent.\n",
        "\n",
        "### Derivatives\n",
        "\n",
        "Let's derive the gradients for the weights and biases of the convolutional and transposed convolutional layers.\n",
        "\n",
        "#### Convolutional Layer Gradients\n",
        "\n",
        "For the convolutional layer, the gradient of the loss function with respect to the weights $W$ is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial h} \\cdot \\frac{\\partial h}{\\partial W}\n",
        "$$\n",
        "\n",
        "Since $h = \\sigma(W * x + b)$, we have:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial h}{\\partial W} = x \\cdot \\sigma'(W * x + b)\n",
        "$$\n",
        "\n",
        "Thus,\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = (x - \\hat{x}) \\cdot \\sigma'(W^T * h + b') \\cdot x^T \\cdot \\sigma'(W * x + b)\n",
        "$$\n",
        "\n",
        "#### Transposed Convolutional Layer Gradients\n",
        "\n",
        "For the transposed convolutional layer, the gradient of the loss function with respect to the weights $W^T$ is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W^T} = \\frac{\\partial L}{\\partial \\hat{x}} \\cdot \\frac{\\partial \\hat{x}}{\\partial W^T}\n",
        "$$\n",
        "\n",
        "Since $\\hat{x} = \\sigma(W^T * h + b')$, we have:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\hat{x}}{\\partial W^T} = h \\cdot \\sigma'(W^T * h + b')\n",
        "$$\n",
        "\n",
        "Thus,\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W^T} = (x - \\hat{x}) \\cdot \\sigma'(W^T * h + b') \\cdot h^T\n",
        "$$\n",
        "\n",
        "### Gradient Descent Update\n",
        "\n",
        "The weights and biases are updated using the gradients:\n",
        "\n",
        "$$\n",
        "W \\leftarrow W - \\eta \\frac{\\partial L}{\\partial W}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b \\leftarrow b - \\eta \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate.\n",
        "\n",
        "# Advantages and Drawbacks\n",
        "\n",
        "## Advantages\n",
        "- **Spatial Hierarchies**: CAEs can capture spatial hierarchies in the data, making them particularly well-suited for image data.\n",
        "- **Parameter Efficiency**: Convolutional layers have fewer parameters than fully connected layers, reducing the risk of overfitting.\n",
        "- **Local Receptive Fields**: Convolutional layers focus on local regions of the input, allowing the model to learn localized features.\n",
        "\n",
        "## Drawbacks\n",
        "- **Computational Cost**: Convolutional layers can be computationally expensive, especially with large input sizes and deep networks.\n",
        "- **Hyperparameter Tuning**: Choosing the right architecture and hyperparameters (e.g., number of filters, kernel size, pooling size) can be challenging and time-consuming.\n",
        "- **Reconstruction Quality**: While CAEs can capture spatial features effectively, they may not always achieve high-quality reconstructions compared to other types of autoencoders (e.g., Variational Autoencoders).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErBuSWW7vFQ2",
        "outputId": "62c4bfbe-3a21-4160-eaee-a05d8bc49307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Epoch 1/50\n",
            "235/235 [==============================] - 70s 286ms/step - loss: 0.2823 - val_loss: 0.1802\n",
            "Epoch 2/50\n",
            "235/235 [==============================] - 70s 298ms/step - loss: 0.1681 - val_loss: 0.1566\n",
            "Epoch 3/50\n",
            "235/235 [==============================] - 69s 293ms/step - loss: 0.1492 - val_loss: 0.1409\n",
            "Epoch 4/50\n",
            "235/235 [==============================] - 69s 294ms/step - loss: 0.1363 - val_loss: 0.1306\n",
            "Epoch 5/50\n",
            "235/235 [==============================] - 68s 291ms/step - loss: 0.1282 - val_loss: 0.1239\n",
            "Epoch 6/50\n",
            "235/235 [==============================] - 70s 297ms/step - loss: 0.1226 - val_loss: 0.1191\n",
            "Epoch 7/50\n",
            "235/235 [==============================] - 69s 293ms/step - loss: 0.1189 - val_loss: 0.1161\n",
            "Epoch 8/50\n",
            "235/235 [==============================] - 71s 301ms/step - loss: 0.1162 - val_loss: 0.1136\n",
            "Epoch 9/50\n",
            "235/235 [==============================] - 62s 263ms/step - loss: 0.1140 - val_loss: 0.1116\n",
            "Epoch 10/50\n",
            "235/235 [==============================] - 63s 268ms/step - loss: 0.1121 - val_loss: 0.1098\n",
            "Epoch 11/50\n",
            "235/235 [==============================] - 64s 272ms/step - loss: 0.1106 - val_loss: 0.1084\n",
            "Epoch 12/50\n",
            "235/235 [==============================] - 59s 253ms/step - loss: 0.1093 - val_loss: 0.1077\n",
            "Epoch 13/50\n",
            "235/235 [==============================] - 61s 260ms/step - loss: 0.1079 - val_loss: 0.1060\n",
            "Epoch 14/50\n",
            "235/235 [==============================] - 61s 261ms/step - loss: 0.1069 - val_loss: 0.1051\n",
            "Epoch 15/50\n",
            "235/235 [==============================] - 63s 266ms/step - loss: 0.1060 - val_loss: 0.1043\n",
            "Epoch 16/50\n",
            "235/235 [==============================] - 59s 252ms/step - loss: 0.1053 - val_loss: 0.1036\n",
            "Epoch 17/50\n",
            "235/235 [==============================] - 61s 259ms/step - loss: 0.1046 - val_loss: 0.1030\n",
            "Epoch 18/50\n",
            "235/235 [==============================] - 61s 259ms/step - loss: 0.1039 - val_loss: 0.1029\n",
            "Epoch 19/50\n",
            "235/235 [==============================] - 63s 267ms/step - loss: 0.1033 - val_loss: 0.1017\n",
            "Epoch 20/50\n",
            "235/235 [==============================] - 59s 253ms/step - loss: 0.1028 - val_loss: 0.1014\n",
            "Epoch 21/50\n",
            "235/235 [==============================] - 62s 266ms/step - loss: 0.1024 - val_loss: 0.1008\n",
            "Epoch 22/50\n",
            "235/235 [==============================] - 59s 253ms/step - loss: 0.1019 - val_loss: 0.1005\n",
            "Epoch 23/50\n",
            "235/235 [==============================] - 62s 265ms/step - loss: 0.1016 - val_loss: 0.1002\n",
            "Epoch 24/50\n",
            "235/235 [==============================] - 59s 253ms/step - loss: 0.1012 - val_loss: 0.1000\n",
            "Epoch 25/50\n",
            "235/235 [==============================] - 62s 265ms/step - loss: 0.1008 - val_loss: 0.0996\n",
            "Epoch 26/50\n",
            "235/235 [==============================] - 61s 259ms/step - loss: 0.1005 - val_loss: 0.0991\n",
            "Epoch 27/50\n",
            "235/235 [==============================] - 60s 257ms/step - loss: 0.1002 - val_loss: 0.0993\n",
            "Epoch 28/50\n",
            "235/235 [==============================] - 60s 254ms/step - loss: 0.0998 - val_loss: 0.0994\n",
            "Epoch 29/50\n",
            " 12/235 [>.............................] - ETA: 1:06 - loss: 0.1000"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "# Define the CAE architecture\n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "\n",
        "# Encoder\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# Decoder\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "# Create CAE model\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
        "\n",
        "# Train the CAE\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n",
        "\n",
        "# Encode and decode some digits\n",
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "# Display original and reconstructed images\n",
        "n = 10  # Number of digits to display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}