{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Tutorial on CycleGANs\n",
        "\n",
        "Cycle-Consistent Generative Adversarial Networks (CycleGANs) are an extension of GANs designed for unpaired image-to-image translation. Introduced by Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros in 2017, CycleGANs enable the transformation of images from one domain to another without requiring paired examples.\n",
        "\n",
        "## Mathematical Foundations\n",
        "\n",
        "CycleGANs consist of two sets of generators and discriminators:\n",
        "\n",
        "1. **Generators**:\n",
        "   - $G: X \\rightarrow Y$: Transforms images from domain $X$ to domain $Y$.\n",
        "   - $F: Y \\rightarrow X$: Transforms images from domain $Y$ to domain $X$.\n",
        "\n",
        "2. **Discriminators**:\n",
        "   - $D_Y$: Distinguishes between real images in domain $Y$ and generated images $G(X)$.\n",
        "   - $D_X$: Distinguishes between real images in domain $X$ and generated images $F(Y)$.\n",
        "\n",
        "### Adversarial Loss\n",
        "\n",
        "The adversarial loss for generator $G$ and discriminator $D_Y$ is:\n",
        "$$\n",
        "L_{\\text{GAN}}(G, D_Y, X, Y) = \\mathbb{E}_{y \\sim p_{\\text{data}}(y)}[\\log D_Y(y)] + \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log (1 - D_Y(G(x)))]\n",
        "$$\n",
        "\n",
        "Similarly, the adversarial loss for generator $F$ and discriminator $D_X$ is:\n",
        "$$\n",
        "L_{\\text{GAN}}(F, D_X, Y, X) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D_X(x)] + \\mathbb{E}_{y \\sim p_{\\text{data}}(y)}[\\log (1 - D_X(F(y)))]\n",
        "$$\n",
        "\n",
        "### Cycle Consistency Loss\n",
        "\n",
        "Cycle consistency ensures that an image translated to the other domain and back results in the original image. The cycle consistency loss is:\n",
        "$$\n",
        "L_{\\text{cyc}}(G, F) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\|F(G(x)) - x\\|_1] + \\mathbb{E}_{y \\sim p_{\\text{data}}(y)}[\\|G(F(y)) - y\\|_1]\n",
        "$$\n",
        "\n",
        "### Total Loss\n",
        "\n",
        "The total loss for CycleGAN combines the adversarial loss and the cycle consistency loss:\n",
        "$$\n",
        "L(G, F, D_X, D_Y) = L_{\\text{GAN}}(G, D_Y, X, Y) + L_{\\text{GAN}}(F, D_X, Y, X) + \\lambda L_{\\text{cyc}}(G, F)\n",
        "$$\n",
        "\n",
        "where $\\lambda$ controls the relative importance of the cycle consistency loss.\n",
        "\n",
        "## Training Procedure\n",
        "\n",
        "The training of CycleGANs involves the following steps:\n",
        "\n",
        "1. **Sample real data** $(x \\sim p_{\\text{data}}(x))$ from domain $X$ and $(y \\sim p_{\\text{data}}(y))$ from domain $Y$.\n",
        "2. **Update Discriminator $D_Y$**:\n",
        "   - Compute discriminator loss for $D_Y$: $\n",
        "     L_{D_Y} = -\\left(\\mathbb{E}_{y \\sim p_{\\text{data}}(y)}[\\log D_Y(y)] + \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log (1 - D_Y(G(x)))]\\right)\n",
        "  $\n",
        "   - Perform a gradient descent step on $L_{D_Y}$ to update $\\theta_{D_Y}$.\n",
        "3. **Update Discriminator $D_X$**:\n",
        "   - Compute discriminator loss for $D_X$: $\n",
        "     L_{D_X} = -\\left(\\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D_X(x)] + \\mathbb{E}_{y \\sim p_{\\text{data}}(y)}[\\log (1 - D_X(F(y)))]\\right)\n",
        "  $\n",
        "   - Perform a gradient descent step on $L_{D_X}$ to update $\\theta_{D_X}$.\n",
        "4. **Update Generators $G$ and $F$**:\n",
        "   - Compute generator loss for $G$ and $F$: $\n",
        "     L_G = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log (1 - D_Y(G(x)))] + \\mathbb{E}_{y \\sim p_{\\text{data}}(y)}[\\|F(G(x)) - x\\|_1]\n",
        "  $\n",
        "  \n",
        "  $\n",
        "     L_F = \\mathbb{E}_{y \\sim p_{\\text{data}}(y)}[\\log (1 - D_X(F(y)))] + \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\|G(F(y)) - y\\|_1]\n",
        "  $\n",
        "   - Perform a gradient descent step on $L_G$ to update $\\theta_G$.\n",
        "   - Perform a gradient descent step on $L_F$ to update $\\theta_F$.\n",
        "\n",
        "## Mathematical Derivatives of the CycleGAN Training Process\n",
        "\n",
        "To understand the training process of CycleGANs, we need to examine the mathematical derivatives guiding the optimization of the generators and discriminators.\n",
        "\n",
        "### Discriminator $D_Y$ Training\n",
        "\n",
        "The discriminator $D_Y$ aims to maximize the probability of correctly classifying real and generated samples from domain $Y$. The loss function for $D_Y$ is:\n",
        "$$\n",
        "L_{D_Y} = -\\left( \\mathbb{E}_{y \\sim p_{\\text{data}}(y)}[\\log D_Y(y)] + \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log (1 - D_Y(G(x)))] \\right)\n",
        "$$\n",
        "\n",
        "To update $D_Y$, we compute the gradient of $L_{D_Y}$ with respect to the discriminator's parameters $\\theta_{D_Y}$:\n",
        "$$\n",
        "\\nabla_{\\theta_{D_Y}} L_{D_Y} = -\\mathbb{E}_{y \\sim p_{\\text{data}}(y)} \\left[ \\frac{1}{D_Y(y)} \\nabla_{\\theta_{D_Y}} D_Y(y) \\right] - \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\left[ \\frac{1}{1 - D_Y(G(x))} \\nabla_{\\theta_{D_Y}} D_Y(G(x)) \\right]\n",
        "$$\n",
        "\n",
        "### Discriminator $D_X$ Training\n",
        "\n",
        "Similarly, the discriminator $D_X$ aims to maximize the probability of correctly classifying real and generated samples from domain $X$. The loss function for $D_X$ is:\n",
        "$$\n",
        "L_{D_X} = -\\left( \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D_X(x)] + \\mathbb{E}_{y \\sim p_{\\text{data}}(y)}[\\log (1 - D_X(F(y)))] \\right)\n",
        "$$\n",
        "\n",
        "To update $D_X$, we compute the gradient of $L_{D_X}$ with respect to the discriminator's parameters $\\theta_{D_X}$:\n",
        "$$\n",
        "\\nabla_{\\theta_{D_X}} L_{D_X} = -\\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\left[ \\frac{1}{D_X(x)} \\nabla_{\\theta_{D_X}} D_X(x) \\right] - \\mathbb{E}_{y \\sim p_{\\text{data}}(y)} \\left[ \\frac{1}{1 - D_X(F(y))} \\nabla_{\\theta_{D_X}} D_X(F(y)) \\right]\n",
        "$$\n",
        "\n",
        "### Generator $G$ Training\n",
        "\n",
        "The generator $G$ aims to fool the discriminator $D_Y$ while maintaining cycle consistency. The loss function for $G$ is:\n",
        "$$\n",
        "L_G = -\\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D_Y(G(x))] + \\lambda \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\|F(G(x)) - x\\|_1]\n",
        "$$\n",
        "\n",
        "To update $G$, we compute the gradient of $L_G$ with respect to the generator's parameters $\\theta_G$:\n",
        "$$\n",
        "\\nabla_{\\theta_G} L_G = -\\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\left[ \\frac{1}{D_Y(G(x))} \\nabla_{\\theta_G} D_Y(G(x)) \\right] + \\lambda \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\left[ \\nabla_{\\theta_G} \\|F(G(x)) - x\\|_1 \\right]\n",
        "$$\n",
        "\n",
        "### Generator $F$ Training\n",
        "\n",
        "Similarly, the generator $F$ aims to fool the discriminator $D_X$ while maintaining cycle consistency. The loss function for $F$ is:\n",
        "$$\n",
        "L_F = -\\mathbb{E}_{y \\sim p_{\\text{data}}(y)}[\\log D_X(F(y))] + \\lambda \\mathbb{E}_{y \\sim p_{\\text{data}}(y)}[\\|G(F(y)) - y\\|_1]\n",
        "$$\n",
        "\n",
        "To update $F$, we compute the gradient of $L_F$ with respect to the generator's parameters $\\theta_F$:\n",
        "$$\n",
        "\\nabla_{\\theta_F} L_F = -\\mathbb{E}_{y \\sim p_{\\text{data}}(y)} \\left[ \\frac{1}{D_X(F(y))} \\nabla_{\\theta_F} D_X(F(y)) \\right] + \\lambda \\mathbb{E}_{y \\sim p_{\\text{data}}(y)} \\left[ \\nabla_{\\theta_F} \\|G(F(y)) - y\\|_1 \\right]\n",
        "$$\n",
        "\n",
        "## Key Innovations\n",
        "\n",
        "1. **Cycle Consistency Loss**: Enforces that translating to the target domain and back results in the original image, ensuring meaningful transformations.\n",
        "2. **Unpaired Training**: CycleGANs do not require paired training data, making them applicable to a wide range of real-world scenarios where such data is unavailable.\n",
        "3. **Dual GAN Structure**: Utilizes two sets of generators and discriminators, enabling bidirectional transformations between domains.\n",
        "\n",
        "## Advantages of CycleGANs\n",
        "\n",
        "1. **Unsupervised Learning**: Can learn to translate between domains without paired examples.\n",
        "2. **Versatile Applications**: Applicable to various tasks such as style transfer, image enhancement, and domain adaptation.\n",
        "3. **Cycle Consistency**: Ensures that the translations are coherent and preserve important structures of the original images.\n",
        "\n",
        "## Drawbacks of CycleGANs\n",
        "\n",
        "1. **Training Instability**: Training CycleGANs can be challenging due to the adversarial loss and cycle consistency loss.\n",
        "2. **Mode Collapse**: Similar to standard GANs, CycleGANs can suffer from mode collapse where the generators produce limited varieties of samples.\n",
        "3. **Resource Intensive**: Training CycleGANs requires significant computational resources and time.\n",
        "4. **Hyperparameter Sensitivity**: The performance of CycleGANs is highly dependent on the choice of hyperparameters and network architectures.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "CycleGANs have significantly advanced the field of image-to-image translation by enabling unpaired training and introducing cycle consistency loss. Understanding the mathematical foundations and training dynamics of CycleGANs, including the derivatives of the training process and the combined loss functions, is crucial for leveraging their potential in various applications and addressing their limitations.\n"
      ],
      "metadata": {
        "id": "CK6FcFIUq5xz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2KqrgjPqoWl"
      },
      "outputs": [],
      "source": []
    }
  ]
}