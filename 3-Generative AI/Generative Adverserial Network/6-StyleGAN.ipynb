{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Tutorial on StyleGAN\n",
        "\n",
        "StyleGAN, introduced by NVIDIA in 2018, is a significant advancement in the field of GANs. It incorporates a style-based generator architecture that allows for intuitive and scalable control over the generated image's style at different levels of detail.\n",
        "\n",
        "## Key Innovations of StyleGAN\n",
        "\n",
        "1. **Style-Based Generator**: Instead of feeding the latent code directly into the generator, StyleGAN uses a mapping network to transform the latent code into intermediate styles.\n",
        "2. **Adaptive Instance Normalization (AdaIN)**: These styles are then used to control the AdaIN operations, which adjust the mean and variance of the feature maps.\n",
        "3. **Progressive Growing**: Similar to Progressive GANs, StyleGAN employs progressive growing of the generator and discriminator, starting from low resolutions and gradually increasing to higher resolutions.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "### Mapping Network\n",
        "\n",
        "The mapping network $f$ transforms the input latent vector $\\mathbf{z} \\in \\mathcal{Z}$ to an intermediate latent vector $\\mathbf{w} \\in \\mathcal{W}$:\n",
        "$$\n",
        "\\mathbf{w} = f(\\mathbf{z})\n",
        "$$\n",
        "\n",
        "### Synthesis Network\n",
        "\n",
        "The synthesis network $g$ generates an image $\\mathbf{x}$ from the intermediate latent vector $\\mathbf{w}$:\n",
        "$$\n",
        "\\mathbf{x} = g(\\mathbf{w})\n",
        "$$\n",
        "\n",
        "In StyleGAN, the intermediate latent vector $\\mathbf{w}$ controls the style at each layer of the synthesis network through AdaIN. The AdaIN operation is defined as:\n",
        "$$\n",
        "\\text{AdaIN}(\\mathbf{x}, \\mathbf{y}) = \\mathbf{y}_{s} \\left( \\frac{\\mathbf{x} - \\mu(\\mathbf{x})}{\\sigma(\\mathbf{x})} \\right) + \\mathbf{y}_{b}\n",
        "$$\n",
        "where $\\mu(\\mathbf{x})$ and $\\sigma(\\mathbf{x})$ are the mean and standard deviation of the input feature map $\\mathbf{x}$, and $\\mathbf{y}_{s}$ and $\\mathbf{y}_{b}$ are the scale and bias parameters derived from the style vector $\\mathbf{y}$.\n",
        "\n",
        "## Training Procedure\n",
        "\n",
        "### Discriminator Loss\n",
        "\n",
        "The discriminator $D$ aims to distinguish between real and generated images. The loss function for the discriminator is:\n",
        "$$\n",
        "L_D = -\\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}}[\\log D(\\mathbf{x})] - \\mathbb{E}_{\\hat{\\mathbf{x}} \\sim p_G}[\\log (1 - D(\\hat{\\mathbf{x}}))]\n",
        "$$\n",
        "where $\\hat{\\mathbf{x}} = G(\\mathbf{z})$ is the generated image.\n",
        "\n",
        "### Generator Loss\n",
        "\n",
        "The generator $G$ aims to fool the discriminator by generating realistic images. The non-saturating loss for the generator is:\n",
        "$$\n",
        "L_G = -\\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}}[\\log D(G(\\mathbf{z}))]\n",
        "$$\n",
        "\n",
        "### Gradient Penalty\n",
        "\n",
        "StyleGAN uses a gradient penalty for improved training stability:\n",
        "$$\n",
        "L_{\\text{GP}} = \\mathbb{E}_{\\hat{\\mathbf{x}} \\sim p_G}[(\\|\\nabla_{\\hat{\\mathbf{x}}} D(\\hat{\\mathbf{x}})\\|_2 - 1)^2]\n",
        "$$\n",
        "\n",
        "### Total Loss Functions\n",
        "\n",
        "The total loss for the discriminator and generator are:\n",
        "$$\n",
        "L_D^{\\text{total}} = L_D + \\lambda L_{\\text{GP}}\n",
        "$$\n",
        "$$\n",
        "L_G^{\\text{total}} = L_G\n",
        "$$\n",
        "\n",
        "## Mathematical Derivatives of the Training Process\n",
        "\n",
        "### Discriminator Training\n",
        "\n",
        "To update the discriminator, we compute the gradient of $L_D^{\\text{total}}$ with respect to the discriminator's parameters $\\theta_D$:\n",
        "$$\n",
        "\\nabla_{\\theta_D} L_D^{\\text{total}} = -\\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\frac{1}{D(\\mathbf{x})} \\nabla_{\\theta_D} D(\\mathbf{x}) \\right] - \\mathbb{E}_{\\hat{\\mathbf{x}} \\sim p_G} \\left[ \\frac{1}{1 - D(\\hat{\\mathbf{x}})} \\nabla_{\\theta_D} D(\\hat{\\mathbf{x}}) \\right] + \\lambda \\nabla_{\\theta_D} L_{\\text{GP}}\n",
        "$$\n",
        "\n",
        "### Generator Training\n",
        "\n",
        "To update the generator, we compute the gradient of $L_G^{\\text{total}}$ with respect to the generator's parameters $\\theta_G$:\n",
        "$$\n",
        "\\nabla_{\\theta_G} L_G^{\\text{total}} = -\\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} \\left[ \\frac{1}{D(G(\\mathbf{z}))} \\nabla_{\\theta_G} D(G(\\mathbf{z})) \\right]\n",
        "$$\n",
        "\n",
        "## Training Procedure with Gradients\n",
        "\n",
        "1. **Discriminator Update**:\n",
        "    - Sample real data $(\\mathbf{x} \\sim p_{\\text{data}})$.\n",
        "    - Sample noise $(\\mathbf{z} \\sim p_{\\mathbf{z}})$ and generate fake data $(\\hat{\\mathbf{x}} = G(\\mathbf{z}))$.\n",
        "    - Compute the discriminator loss:\n",
        "      $$\n",
        "      L_D = -\\left( \\log D(\\mathbf{x}) + \\log (1 - D(\\hat{\\mathbf{x}})) \\right)\n",
        "      $$\n",
        "    - Compute the gradient penalty:\n",
        "  $L_{\\text{GP}} = \\mathbb{E}_{\\hat{\\mathbf{x}} \\sim p_G}[(\\|\\nabla_{\\hat{\\mathbf{x}}} D(\\hat{\\mathbf{x}})\\|_2 - 1)^2]\n",
        "  $\n",
        "    - Compute the total discriminator loss:\n",
        "      $$\n",
        "      L_D^{\\text{total}} = L_D + \\lambda L_{\\text{GP}}\n",
        "      $$\n",
        "    - Compute gradients:\n",
        "      $$\n",
        "      \\nabla_{\\theta_D} L_D^{\\text{total}} = -\\left( \\frac{\\nabla_{\\theta_D} D(\\mathbf{x})}{D(\\mathbf{x})} + \\frac{\\nabla_{\\theta_D} D(\\hat{\\mathbf{x}})}{1 - D(\\hat{\\mathbf{x}})} \\right) + \\lambda \\nabla_{\\theta_D} L_{\\text{GP}}\n",
        "      $$\n",
        "    - Update $\\theta_D$ using gradient descent.\n",
        "\n",
        "2. **Generator Update**:\n",
        "    - Sample noise $(\\mathbf{z} \\sim p_{\\mathbf{z}})$.\n",
        "    - Generate fake data $(\\hat{\\mathbf{x}} = G(\\mathbf{z}))$.\n",
        "    - Compute the generator loss:\n",
        "      $$\n",
        "      L_G = -\\log D(\\hat{\\mathbf{x}})\n",
        "      $$\n",
        "    - Compute the total generator loss:\n",
        "      $$\n",
        "      L_G^{\\text{total}} = L_G\n",
        "      $$\n",
        "    - Compute gradients:\n",
        "      $$\n",
        "      \\nabla_{\\theta_G} L_G^{\\text{total}} = -\\frac{\\nabla_{\\theta_G} D(\\hat{\\mathbf{x}})}{D(\\hat{\\mathbf{x}})}\n",
        "      $$\n",
        "    - Update $\\theta_G$ using gradient descent.\n",
        "\n",
        "## Advantages of StyleGAN\n",
        "\n",
        "1. **Fine-Grained Control**: StyleGAN provides fine-grained control over the image synthesis process, allowing for changes in style at different levels of detail.\n",
        "2. **High-Quality Images**: StyleGAN can generate high-resolution, realistic images with intricate details.\n",
        "3. **Improved Training Stability**: The architecture and techniques like progressive growing and gradient penalty contribute to more stable training.\n",
        "\n",
        "## Drawbacks of StyleGAN\n",
        "\n",
        "1. **Computationally Intensive**: Training StyleGAN requires significant computational resources, including powerful GPUs.\n",
        "2. **Complex Architecture**: The style-based architecture is more complex and harder to implement compared to basic GANs.\n",
        "3. **Mode Collapse**: Although less frequent than in basic GANs, mode collapse can still occur in StyleGAN.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "StyleGAN represents a major advancement in the field of generative models, offering fine-grained control over image synthesis and producing high-quality images. Understanding the mathematical foundations and training dynamics of StyleGAN, including the derivatives of the training process and improved loss functions, is crucial for leveraging its full potential and addressing its limitations. Despite its challenges, StyleGAN's innovations continue to drive progress in various applications, from art creation to data augmentation.\n"
      ],
      "metadata": {
        "id": "OUF1WM4eRM0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ycngaz9qm_F"
      },
      "outputs": [],
      "source": []
    }
  ]
}