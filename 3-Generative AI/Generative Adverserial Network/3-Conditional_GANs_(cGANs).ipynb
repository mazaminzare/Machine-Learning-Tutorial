{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Tutorial on Conditional GANs (cGANs)\n",
        "\n",
        "Conditional Generative Adversarial Networks (cGANs) extend the original GAN framework by conditioning the model on additional information. This additional information can be class labels, data from other modalities, or any auxiliary information that can help control the output of the generator.\n",
        "\n",
        "## Mathematical Foundations\n",
        "\n",
        "In cGANs, both the generator and discriminator are conditioned on some extra information, denoted by $c$. This conditioning could be class labels, text descriptions, or other context-specific information.\n",
        "\n",
        "1. **Generator (G)**: The generator takes a random noise vector $(\\mathbf{z})$ and conditioning information $(c)$, and maps it to the data space $(G(\\mathbf{z}, c; \\theta_G))$. The objective is to generate data conditioned on $c$.\n",
        "\n",
        "2. **Discriminator (D)**: The discriminator takes a data sample and the conditioning information $(c)$, and outputs a scalar $(D(\\mathbf{x}, c; \\theta_D))$ representing the probability that the sample is real. The discriminator's objective is to correctly classify real and generated samples based on $c$.\n",
        "\n",
        "The objective function for cGANs is:\n",
        "$$\n",
        "\\min_G \\max_D V(D, G) = \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}}[\\log D(\\mathbf{x}, c)] + \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}}[\\log (1 - D(G(\\mathbf{z}, c)))]\n",
        "$$\n",
        "\n",
        "## Training Procedure\n",
        "\n",
        "The training procedure for cGANs is similar to that of GANs but includes the conditioning information $c$:\n",
        "\n",
        "1. **Sample real data** $(\\mathbf{x} \\sim p_{\\text{data}})$ and corresponding conditioning information $c$.\n",
        "2. **Sample noise** $(\\mathbf{z} \\sim p_{\\mathbf{z}})$ and generate fake data $(\\hat{\\mathbf{x}} = G(\\mathbf{z}, c))$.\n",
        "3. **Update Discriminator**:\n",
        "   - Compute discriminator loss: $\n",
        "     L_D = -\\left(\\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}}[\\log D(\\mathbf{x}, c)] + \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}}[\\log (1 - D(G(\\mathbf{z}, c)))]\\right)\n",
        "  $\n",
        "   - Perform a gradient descent step on $L_D$ to update $\\theta_D$.\n",
        "4. **Update Generator**:\n",
        "   - Compute generator loss using the non-saturating loss: $\n",
        "     L_G' = -\\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}}[\\log D(G(\\mathbf{z}, c))]\n",
        "  $\n",
        "   - Perform a gradient descent step on $L_G'$ to update $\\theta_G$.\n",
        "\n",
        "## Mathematical Derivatives of the cGAN Training Process\n",
        "\n",
        "To delve deeper into the training process of cGANs, we examine the mathematical derivatives that guide the optimization of both the generator and the discriminator.\n",
        "\n",
        "### Discriminator Training\n",
        "\n",
        "The discriminator aims to maximize the probability of correctly classifying real and generated samples conditioned on $c$. The loss function for the discriminator is:\n",
        "$$\n",
        "L_D = -\\left( \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}}[\\log D(\\mathbf{x}, c)] + \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}}[\\log (1 - D(G(\\mathbf{z}, c)))] \\right)\n",
        "$$\n",
        "\n",
        "To update the discriminator, we compute the gradient of $L_D$ with respect to the discriminator's parameters $\\theta_D$:\n",
        "$$\n",
        "\\nabla_{\\theta_D} L_D = -\\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\frac{1}{D(\\mathbf{x}, c)} \\nabla_{\\theta_D} D(\\mathbf{x}, c) \\right] - \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} \\left[ \\frac{1}{1 - D(G(\\mathbf{z}, c))} \\nabla_{\\theta_D} D(G(\\mathbf{z}, c)) \\right]\n",
        "$$\n",
        "\n",
        "### Generator Training\n",
        "\n",
        "The generator aims to fool the discriminator, which can be framed as maximizing the following objective:\n",
        "$$\n",
        "L_G' = \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}}[\\log D(G(\\mathbf{z}, c))]\n",
        "$$\n",
        "\n",
        "To update the generator, we compute the gradient of $L_G'$ with respect to the generator's parameters $\\theta_G$:\n",
        "$$\n",
        "\\nabla_{\\theta_G} L_G' = \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} \\left[ \\frac{1}{D(G(\\mathbf{z}, c))} \\nabla_{\\theta_G} D(G(\\mathbf{z}, c)) \\right]\n",
        "$$\n",
        "\n",
        "### Training Procedure with Gradients\n",
        "\n",
        "The training procedure of cGANs with the detailed gradient steps is as follows:\n",
        "\n",
        "1. **Discriminator Update**:\n",
        "    - Sample real data $(\\mathbf{x} \\sim p_{\\text{data}})$ and corresponding conditioning information $c$.\n",
        "    - Sample noise $(\\mathbf{z} \\sim p_{\\mathbf{z}})$ and generate fake data $(\\hat{\\mathbf{x}} = G(\\mathbf{z}, c))$.\n",
        "    - Compute the discriminator loss:\n",
        "      $$\n",
        "      L_D = -\\left( \\log D(\\mathbf{x}, c) + \\log (1 - D(\\hat{\\mathbf{x}}, c)) \\right)\n",
        "      $$\n",
        "    - Compute gradients:\n",
        "      $$\n",
        "      \\nabla_{\\theta_D} L_D = -\\left( \\frac{\\nabla_{\\theta_D} D(\\mathbf{x}, c)}{D(\\mathbf{x}, c)} + \\frac{\\nabla_{\\theta_D} D(\\hat{\\mathbf{x}}, c)}{1 - D(\\hat{\\mathbf{x}}, c)} \\right)\n",
        "      $$\n",
        "    - Update $\\theta_D$ using gradient descent.\n",
        "\n",
        "2. **Generator Update**:\n",
        "    - Sample noise $(\\mathbf{z} \\sim p_{\\mathbf{z}})$ and conditioning information $c$.\n",
        "    - Generate fake data $(\\hat{\\mathbf{x}} = G(\\mathbf{z}, c))$.\n",
        "    - Compute the generator loss using the non-saturating loss:\n",
        "      $$\n",
        "      L_G' = -\\log D(\\hat{\\mathbf{x}}, c)\n",
        "      $$\n",
        "    - Compute gradients:\n",
        "      $$\n",
        "      \\nabla_{\\theta_G} L_G' = -\\frac{\\nabla_{\\theta_G} D(\\hat{\\mathbf{x}}, c)}{D(\\hat{\\mathbf{x}}, c)}\n",
        "      $$\n",
        "    - Update $\\theta_G$ using gradient descent.\n",
        "\n",
        "## Key Innovations\n",
        "\n",
        "1. **Conditioning Information**: cGANs leverage additional information to guide the generation process, making it possible to control the output of the generator.\n",
        "2. **Improved Control**: By conditioning on auxiliary information, cGANs provide finer control over the generated samples compared to standard GANs.\n",
        "3. **Enhanced Flexibility**: cGANs can be applied to a wide range of tasks where context or auxiliary information is available.\n",
        "\n",
        "## Advantages of cGANs\n",
        "\n",
        "1. **Controlled Generation**: cGANs allow for the generation of data that is consistent with the given conditioning information, providing greater control over the output.\n",
        "2. **Versatile Applications**: cGANs can be used in various applications such as image-to-image translation, super-resolution, and text-to-image synthesis.\n",
        "3. **Improved Feature Learning**: cGANs can learn more robust features by incorporating additional context or auxiliary information during training.\n",
        "\n",
        "## Drawbacks of cGANs\n",
        "\n",
        "1. **Training Complexity**: Incorporating conditioning information adds complexity to the training process, requiring careful handling of the auxiliary information.\n",
        "2. **Mode Collapse**: Like standard GANs, cGANs can suffer from mode collapse, where the generator produces limited varieties of samples.\n",
        "3. **Sensitive Hyperparameters**: The performance of cGANs is highly dependent on the choice of hyperparameters and the quality of the conditioning information.\n",
        "4. **Computationally Intensive**: Training cGANs can be computationally demanding, especially when dealing with high-dimensional conditioning information.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Conditional GANs (cGANs) extend the original GAN framework by incorporating additional information to guide the generation process. This conditioning allows for more controlled and context-aware data generation, enhancing the flexibility and applicability of GANs. Understanding the mathematical foundations, training dynamics, and key innovations of cGANs is crucial for effectively leveraging their potential and addressing their limitations.\n"
      ],
      "metadata": {
        "id": "OpgfVM_zo28L"
      }
    }
  ]
}