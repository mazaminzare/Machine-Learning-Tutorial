{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Tutorial on Least Squares GAN (LSGAN)\n",
        "\n",
        "Least Squares GAN (LSGAN) is a variant of the original GAN proposed to stabilize training and provide more meaningful gradients to the generator. It achieves this by using a least squares loss function instead of the cross-entropy loss used in the original GAN.\n",
        "\n",
        "## Mathematical Foundations\n",
        "\n",
        "1. **Generator (G)**: This network takes a random noise vector $(\\mathbf{z})$ from a prior distribution $(p_{\\mathbf{z}})$ (often a Gaussian or uniform distribution) and maps it to the data space $(G(\\mathbf{z}; \\theta_G))$. The generator's objective is to generate data that resembles the true data distribution $(p_{\\text{data}})$.\n",
        "\n",
        "2. **Discriminator (D)**: This network takes a data sample (either real or generated) and outputs a single scalar $(D(\\mathbf{x}; \\theta_D))$. However, in LSGAN, this output is not a probability but rather a real-valued score.\n",
        "\n",
        "The key difference in LSGAN is the loss function. Instead of using the binary cross-entropy loss, LSGAN uses the least squares loss. The objective functions for the discriminator and generator are defined as follows:\n",
        "\n",
        "### Discriminator Loss\n",
        "\n",
        "For real data:\n",
        "$$\n",
        "L_D^{\\text{real}} = \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} [(D(\\mathbf{x}) - b)^2]\n",
        "$$\n",
        "\n",
        "For generated data:\n",
        "$$\n",
        "L_D^{\\text{fake}} = \\frac{1}{2} \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} [(D(G(\\mathbf{z})) - a)^2]\n",
        "$$\n",
        "\n",
        "The total discriminator loss is:\n",
        "$$\n",
        "L_D = L_D^{\\text{real}} + L_D^{\\text{fake}} = \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} [(D(\\mathbf{x}) - b)^2] + \\frac{1}{2} \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} [(D(G(\\mathbf{z})) - a)^2]\n",
        "$$\n",
        "\n",
        "### Generator Loss\n",
        "\n",
        "The generator aims to produce data that the discriminator classifies as the real data's score. The generator loss is:\n",
        "$$\n",
        "L_G = \\frac{1}{2} \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} [(D(G(\\mathbf{z})) - c)^2]\n",
        "$$\n",
        "\n",
        "Typically, $a = 0$, $b = 1$, and $c = 1$.\n",
        "\n",
        "## Training Procedure\n",
        "\n",
        "The training of LSGANs involves the following steps, typically repeated iteratively:\n",
        "\n",
        "1. **Sample real data** $(\\mathbf{x} \\sim p_{\\text{data}})$.\n",
        "2. **Sample noise** $(\\mathbf{z} \\sim p_{\\mathbf{z}})$ and generate fake data $(\\hat{\\mathbf{x}} = G(\\mathbf{z}))$.\n",
        "3. **Update Discriminator**:\n",
        "   - Compute discriminator loss:\n",
        "  $\n",
        "     L_D = \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} [(D(\\mathbf{x}) - 1)^2] + \\frac{1}{2} \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} [(D(G(\\mathbf{z})) - 0)^2]\n",
        "  $\n",
        "   - Perform a gradient descent step on $L_D$ to update $\\theta_D$.\n",
        "4. **Update Generator**:\n",
        "   - Compute generator loss:\n",
        "  $\n",
        "     L_G = \\frac{1}{2} \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} [(D(G(\\mathbf{z})) - 1)^2]\n",
        "  $\n",
        "   - Perform a gradient descent step on $L_G$ to update $\\theta_G$.\n",
        "\n",
        "## Mathematical Derivatives of the LSGAN Training Process\n",
        "\n",
        "### Discriminator Training\n",
        "\n",
        "The discriminator aims to minimize the following loss:\n",
        "$$\n",
        "L_D = \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} [(D(\\mathbf{x}) - 1)^2] + \\frac{1}{2} \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} [(D(G(\\mathbf{z})) - 0)^2]\n",
        "$$\n",
        "\n",
        "To update the discriminator, we compute the gradient of $L_D$ with respect to the discriminator's parameters $\\theta_D$:\n",
        "$$\n",
        "\\nabla_{\\theta_D} L_D = \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} [(D(\\mathbf{x}) - 1) \\nabla_{\\theta_D} D(\\mathbf{x})] + \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} [(D(G(\\mathbf{z})) - 0) \\nabla_{\\theta_D} D(G(\\mathbf{z}))]\n",
        "$$\n",
        "\n",
        "### Generator Training\n",
        "\n",
        "The generator aims to minimize the following loss:\n",
        "$$\n",
        "L_G = \\frac{1}{2} \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} [(D(G(\\mathbf{z})) - 1)^2]\n",
        "$$\n",
        "\n",
        "To update the generator, we compute the gradient of $L_G$ with respect to the generator's parameters $\\theta_G$:\n",
        "$$\n",
        "\\nabla_{\\theta_G} L_G = \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} [(D(G(\\mathbf{z})) - 1) \\nabla_{\\theta_G} D(G(\\mathbf{z}))]\n",
        "$$\n",
        "\n",
        "### Training Procedure with Gradients\n",
        "\n",
        "The training procedure of LSGANs with the detailed gradient steps is as follows:\n",
        "\n",
        "1. **Discriminator Update**:\n",
        "    - Sample real data $(\\mathbf{x} \\sim p_{\\text{data}})$.\n",
        "    - Sample noise $(\\mathbf{z} \\sim p_{\\mathbf{z}})$ and generate fake data $(\\hat{\\mathbf{x}} = G(\\mathbf{z}))$.\n",
        "    - Compute the discriminator loss:\n",
        "      $$\n",
        "      L_D = \\frac{1}{2} [(D(\\mathbf{x}) - 1)^2 + (D(\\hat{\\mathbf{x}}) - 0)^2]\n",
        "      $$\n",
        "    - Compute gradients:\n",
        "      $$\n",
        "      \\nabla_{\\theta_D} L_D = (D(\\mathbf{x}) - 1) \\nabla_{\\theta_D} D(\\mathbf{x}) + (D(\\hat{\\mathbf{x}}) - 0) \\nabla_{\\theta_D} D(\\hat{\\mathbf{x}})\n",
        "      $$\n",
        "    - Update $\\theta_D$ using gradient descent.\n",
        "\n",
        "2. **Generator Update**:\n",
        "    - Sample noise $(\\mathbf{z} \\sim p_{\\mathbf{z}})$.\n",
        "    - Generate fake data $(\\hat{\\mathbf{x}} = G(\\mathbf{z}))$.\n",
        "    - Compute the generator loss:\n",
        "      $$\n",
        "      L_G = \\frac{1}{2} (D(\\hat{\\mathbf{x}}) - 1)^2\n",
        "      $$\n",
        "    - Compute gradients:\n",
        "      $$\n",
        "      \\nabla_{\\theta_G} L_G = (D(\\hat{\\mathbf{x}}) - 1) \\nabla_{\\theta_G} D(\\hat{\\mathbf{x}})\n",
        "      $$\n",
        "    - Update $\\theta_G$ using gradient descent.\n",
        "\n",
        "## Key Innovations\n",
        "\n",
        "1. **Least Squares Loss**: The least squares loss function helps stabilize GAN training and provides more meaningful gradients, reducing the chances of vanishing gradients.\n",
        "2. **Improved Training Dynamics**: The smoother loss landscape of LSGAN improves the overall training dynamics and helps mitigate issues like mode collapse.\n",
        "\n",
        "## Advantages of LSGANs\n",
        "\n",
        "1. **Stabilized Training**: The least squares loss helps in stabilizing the training process compared to the standard GAN loss.\n",
        "2. **Reduced Vanishing Gradients**: The use of least squares loss provides stronger gradients, reducing the problem of vanishing gradients for the generator.\n",
        "3. **Better Quality of Generated Data**: LSGANs often produce higher quality data due to more stable and consistent training.\n",
        "\n",
        "## Drawbacks of LSGANs\n",
        "\n",
        "1. **Complexity**: Implementing and understanding the least squares loss may add complexity compared to the original GAN formulation.\n",
        "2. **Computational Overhead**: The least squares loss might introduce additional computational overhead due to the squared error terms.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Least Squares GANs (LSGANs) address some of the critical issues in the original GAN framework by using a least squares loss function. This modification stabilizes training and provides more meaningful gradients, leading to higher quality generated data. Understanding the mathematical foundations and training dynamics of LSGANs, including the derivatives of the training process and the impact of the least squares loss, is crucial for leveraging their full potential and addressing their limitations.\n"
      ],
      "metadata": {
        "id": "ICNKr7UFaAkp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beZrnNh5ZXtG"
      },
      "outputs": [],
      "source": []
    }
  ]
}